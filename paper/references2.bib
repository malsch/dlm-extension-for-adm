
@misc{stan_development_team_rstan_2021,
	title = {{RStan}: the {R} interface to {Stan}},
	url = {https://mc-stan.org/},
	author = {{Stan Development Team}},
	year = {2021},
}

@techreport{stan_development_team_stan_2022,
	title = {Stan {Modeling} {Language} {Users} {Guide} and {Reference} {Manual}},
	url = {https://mc-stan.org},
	author = {Stan Development Team},
	year = {2022},
}

@article{ge_use_2018,
	title = {Use and {Reliability} of {Exposure} {Assessment} {Methods} in {Occupational} {Case}–{Control} {Studies} in the {General} {Population}: {Past}, {Present}, and {Future}},
	volume = {62},
	issn = {2398-7308},
	shorttitle = {Use and {Reliability} of {Exposure} {Assessment} {Methods} in {Occupational} {Case}–{Control} {Studies} in the {General} {Population}},
	url = {https://doi.org/10.1093/annweh/wxy080},
	doi = {10.1093/annweh/wxy080},
	abstract = {Retrospective occupational exposure assessment has been challenging in case–control studies in the general population. We aimed to review (i) trends of different assessment methods used in the last 40 years and (ii) evidence of reliability for various assessment methods.Two separate literature reviews were conducted. We first reviewed all general population cancer case–control studies published from 1975 to 2016 to summarize the exposure assessment approach used. For the second review, we systematically reviewed evidence of reliability for all methods observed in the first review.Among the 299 studies included in the first review, the most frequently used assessment methods were self-report/assessment (n = 143 studies), case-by-case expert assessment (n = 139), and job-exposure matrices (JEMs; n = 82). Usage trends for these methods remained relatively stable throughout the last four decades. Other approaches, such as the application of algorithms linking questionnaire responses to expert-assigned exposure estimates and modelling of exposure with historical measurement data, appeared in 21 studies that were published after 2000. The second review retrieved 34 comparison studies examining methodological reliability. Overall, we observed slightly higher median kappa agreement between exposure estimates from different expert assessors ({\textasciitilde}0.6) than between expert estimates and exposure estimates from self-reports ({\textasciitilde}0.5) or JEMs ({\textasciitilde}0.4). However, reported reliability measures were highly variable for different methods and agents. Limited evidence also indicates newer methods, such as assessment using algorithms and measurement-calibrated quantitative JEMs, may be as reliable as traditional methods.The majority of current research assesses exposures in the population with similar methods as studies did decades ago. Though there is evidence for the development of newer approaches, more concerted effort is needed to better adopt exposure assessment methods with more transparency, reliability, and efficiency.},
	number = {9},
	urldate = {2022-02-12},
	journal = {Annals of Work Exposures and Health},
	author = {Ge, Calvin B and Friesen, Melissa C and Kromhout, Hans and Peters, Susan and Rothman, Nathaniel and Lan, Qing and Vermeulen, Roel},
	month = nov,
	year = {2018},
	pages = {1047--1063},
}

@misc{noauthor_online_nodate,
	title = {Online {Causal} {Inference} {Seminar}},
	url = {https://sites.google.com/view/ocis/},
	abstract = {Online Causal Inference Seminar},
	language = {de},
	urldate = {2022-02-10},
}

@article{krishna_disagreement_2022,
	title = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}: {A} {Practitioner}'s {Perspective}},
	shorttitle = {The {Disagreement} {Problem} in {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2202.01602},
	abstract = {As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.},
	urldate = {2022-02-08},
	journal = {arXiv:2202.01602 [cs]},
	author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.01602},
}

@book{mcelreath_statistical_2020,
	address = {New York},
	edition = {2},
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	isbn = {978-0-429-02960-8},
	shorttitle = {Statistical {Rethinking}},
	abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.

The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.

The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.

Features


Integrates working code into the main text


Illustrates concepts through worked data analysis examples


Emphasizes understanding assumptions and how assumptions are reflected in code


Offers more detailed explanations of the mathematics in optional sections


Presents examples of using the dagitty R package to analyze causal graphs


Provides the rethinking R package on the author's website and on GitHub},
	publisher = {Chapman and Hall/CRC},
	author = {McElreath, Richard},
	month = mar,
	year = {2020},
	doi = {10.1201/9780429029608},
}

@misc{noauthor_httpwwwstatisticalsleuthcom_nodate,
	title = {http://www.statisticalsleuth.com/},
	url = {http://www.statisticalsleuth.com/},
	urldate = {2022-02-06},
}

@article{weeden_case_2005,
	title = {The {Case} for a {New} {Class} {Map}},
	volume = {111},
	issn = {0002-9602, 1537-5390},
	url = {http://www.journals.uchicago.edu/doi/10.1086/428815},
	doi = {10.1086/428815},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {American Journal of Sociology},
	author = {Weeden, Kim A. and Grusky, David B.},
	month = jul,
	year = {2005},
	pages = {141--212},
}

@article{hothorn_model-based_2010,
	title = {Model-based {Boosting} 2.0},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/hothorn10a.html},
	abstract = {We describe version 2.0 of the R add-on package mboost. The package implements boosting for optimizing general risk functions using component-wise (penalized) least squares estimates or regression trees as base-learners for fitting generalized linear, additive and interaction models to potentially high-dimensional data.},
	number = {71},
	urldate = {2021-11-04},
	journal = {Journal of Machine Learning Research},
	author = {Hothorn, Torsten and Bühlmann, Peter and Kneib, Thomas and Schmid, Matthias and Hofner, Benjamin},
	year = {2010},
	pages = {2109--2113},
}

@article{schonlau_semi-automated_2016,
	title = {Semi-automated categorization of open-ended questions},
	volume = {10},
	copyright = {Copyright (c) 2016 Survey Research Methods},
	issn = {1864-3361},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/6213},
	doi = {10.18148/srm/2016.v10i2.6213},
	abstract = {Text data from open-ended questions in surveys are difficult to analyze and are frequently ignored. Yet open-ended questions are important because they do not constrain respondents’ answer choices. Where open-ended questions are necessary, sometimes multiple human coders hand-code answers into one of several categories. At the same time, computer scientists have made impressive advances in text mining that may allow automation of such coding.  Automated algorithms do not achieve an overall accuracy high enough to entirely replace humans.  We categorize open-ended questions soliciting narrative responses using text mining for easy-to-categorize answers and humans for the remainder using expected accuracies to guide the choice of the threshold delineating between “easy” and “hard”.  Employing multinomial boosting avoids the common practice of converting machine learning “confidence scores” into pseudo-probabilities. This approach is illustrated with examples from open-ended questions related to respondents’ advice to a patient in a hypothetical dilemma, a follow-up probe related to respondents’ perception of disclosure/privacy risk, and from a question on reasons for quitting smoking from a follow-up survey from the Ontario Smoker’s Helpline. Targeting 80\% combined accuracy, we found that 54\%-80\% of the data could be categorized automatically in research surveys.},
	language = {en},
	number = {2},
	urldate = {2021-11-05},
	journal = {Survey Research Methods},
	author = {Schonlau, Matthias and Couper, Mick P.},
	month = aug,
	year = {2016},
	note = {Number: 2},
	pages = {143--152},
}

@article{gentzkow_text_2019,
	title = {Text as {Data}},
	volume = {57},
	issn = {0022-0515},
	url = {https://www.aeaweb.org/articles?id=10.1257/jel.20181020},
	doi = {10.1257/jel.20181020},
	abstract = {An ever-increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.},
	language = {en},
	number = {3},
	urldate = {2022-01-16},
	journal = {Journal of Economic Literature},
	author = {Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt},
	month = sep,
	year = {2019},
	pages = {535--574},
}

@book{radermacher_official_2020,
	address = {Cham, Switzerland},
	title = {Official statistics 4.0: verified facts for people in the 21st century},
	isbn = {978-3-030-31492-7 978-3-030-31491-0},
	shorttitle = {Official statistics 4.0},
	language = {eng},
	publisher = {Springer},
	author = {Radermacher, Walter},
	year = {2020},
}

@article{glazier_self-coding_2021,
	title = {Self-coding: {A} method to assess semantic validity and bias when coding open-ended responses},
	volume = {8},
	issn = {2053-1680},
	shorttitle = {Self-coding},
	url = {https://doi.org/10.1177/20531680211031752},
	doi = {10.1177/20531680211031752},
	abstract = {Open-ended survey questions can provide researchers with nuanced and rich data, but content analysis is subject to misinterpretation and can introduce bias into subsequent analysis. We present a simple method to improve the semantic validity of a codebook and test for bias: a “self-coding” method where respondents first provide open-ended responses and then self-code those responses into categories. We demonstrated this method by comparing respondents’ self-coding to researcher-based coding using an established codebook. Our analysis showed significant disagreement between the codebook’s assigned categorizations of responses and respondents’ self-codes. Moreover, this technique uncovered instances where researcher-based coding disproportionately misrepresented the views of certain demographic groups. We propose using the self-coding method to iteratively improve codebooks, identify bad-faith respondents, and, perhaps, to replace researcher-based content analysis.},
	language = {en},
	number = {3},
	urldate = {2022-02-04},
	journal = {Research \& Politics},
	author = {Glazier, Rebecca A. and Boydstun, Amber E. and Feezell, Jessica T.},
	month = jul,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	pages = {20531680211031752},
}

@misc{hardt_patterns_2022,
	title = {Patterns, {Predictions}, and {Actions}},
	url = {https://mlstory.org/},
	urldate = {2022-02-04},
	author = {Hardt, Moritz and Recht, Benjamin},
	year = {2022},
}

@article{gebru_datasheets_2021,
	title = {Datasheets for {Datasets}},
	url = {http://arxiv.org/abs/1803.09010},
	abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
	urldate = {2022-02-03},
	journal = {arXiv:1803.09010 [cs]},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
	month = dec,
	year = {2021},
	note = {arXiv: 1803.09010},
}

@article{acosta-ballesteros_measuring_2021,
	title = {Measuring the effect of gender segregation on the gender gap in time-related underemployment},
	volume = {55},
	issn = {2510-5027},
	url = {https://doi.org/10.1186/s12651-021-00305-0},
	doi = {10.1186/s12651-021-00305-0},
	abstract = {This paper focuses on the impact that gender segregation in the labour market exerts on the underemployment gender gap for young adult workers in Spain. In order to analyse the relative importance of segregation in this gap, we develop a methodology based on two counterfactual simulations that provides a detailed decomposition of the gap into endowments and coefficients effects as well as the interaction of these effects. To the best of our knowledge, we are the first to perform a decomposition using bivariate probit models with sample selection. Using annual samples of the Spanish Labour Force Survey 2006–2016, the results show that working in female-dominated occupations or industries hinders working as many hours as desired, especially for women. Furthermore, we conclude that the gender gap in underemployment is mainly due to the different distribution of male and female workers across occupations and industries. Additionally, the different impact by gender that working in the same gender-typing jobs exerts on the risk of underemployment contributes to widening the gap.},
	number = {1},
	urldate = {2022-02-03},
	journal = {Journal for Labour Market Research},
	author = {Acosta-Ballesteros, Juan and Osorno-del Rosal, María del Pilar and Rodríguez-Rodríguez, Olga María},
	month = nov,
	year = {2021},
	pages = {22},
}

@book{heumann_introduction_2016,
	address = {Cham},
	edition = {1st ed. 2016},
	title = {Introduction to {Statistics} and {Data} {Analysis}: {With} {Exercises}, {Solutions} and {Applications} in {R}},
	isbn = {978-3-319-46162-5},
	shorttitle = {Introduction to {Statistics} and {Data} {Analysis}},
	abstract = {This introductory statistics textbook conveys the essential concepts and tools needed to develop and nurture statistical thinking. It presents descriptive, inductive and explorative statistical methods and guides the reader through the process of quantitative data analysis. In the experimental sciences and interdisciplinary research, data analysis has become an integral part of any scientific study. Issues such as judging the credibility of data, analyzing the data, evaluating the reliability of the obtained results and finally drawing the correct and appropriate conclusions from the results are vital. The text is primarily intended for undergraduate students in disciplines like business administration, the social sciences, medicine, politics, macroeconomics, et cetera It features a wealth of examples, exercises and solutions with computer code in the statistical programming language R as well as supplementary material that will enable the reader to quickly adapt all methods to their own applications},
	publisher = {Springer International Publishing : Imprint: Springer},
	author = {Heumann, Christian and Schomaker, Michael and Shalabh},
	year = {2016},
	doi = {10.1007/978-3-319-46162-5},
}

@book{kauermann_statistical_2021,
	address = {Cham Springer},
	series = {Springer series in statistics},
	title = {Statistical foundations, reasoning and inference: for science and data science},
	isbn = {978-3-030-69827-0 978-3-030-69826-3},
	shorttitle = {Statistical foundations, reasoning and inference},
	language = {eng},
	author = {Kauermann, Göran and Küchenhoff, Helmut and Heumann, Christian G.},
	year = {2021},
}

@misc{breznau_zotero_2022,
	title = {Zotero {Library} {Parser}},
	copyright = {GPL-3.0},
	url = {https://github.com/nbreznau/zotero_library_parser},
	abstract = {A simple routine to download and parse all PDF attachments in a Zotero library.},
	urldate = {2022-02-01},
	author = {Breznau, Nate},
	month = jan,
	year = {2022},
	note = {original-date: 2022-01-29T16:08:51Z},
}

@misc{bundesregierung_kunstliche_2022,
	title = {Künstliche {Intelligenz} im {Geschäftsbereich} der {Bundesregierung}},
	author = {Bundesregierung},
	year = {2022},
}

@article{buchmann_swiss_2022,
	title = {Swiss {Job} {Market} {Monitor}: {A} {Rich} {Source} of {Demand}-{Side} {Micro} {Data} of the {Labour} {Market}},
	issn = {0266-7215},
	shorttitle = {Swiss {Job} {Market} {Monitor}},
	url = {https://doi.org/10.1093/esr/jcac002},
	doi = {10.1093/esr/jcac002},
	abstract = {The Swiss Job Market Monitor (SJMM) is a data collection of job ads for a national labour market from 1950 onwards. It is a prime example for demonstrating how to turn digital data into a high-quality social science data set. It is also exemplary for combining digital data with historically grown job ads data to provide a comparable time series covering the past 70 years. Paying close attention to sampling procedures, coverage, and key variables, the Data Brief shows how the SJMM profits from opportunities offered by computational social science and computational linguistics to navigate a wholly new set of challenges involved in creating such a data set. It closes with a discussion of new research opportunities this data set is opening for investigating the labour market as a core institution of modern society.},
	urldate = {2022-01-26},
	journal = {European Sociological Review},
	author = {Buchmann, Marlis and Buchs, Helen and Busch, Felix and Clematide, Simon and Gnehm, Ann-Sophie and Müller, Jan},
	month = jan,
	year = {2022},
	pages = {jcac002},
}

@article{benoit_crowd-sourced_2016,
	title = {Crowd-sourced {Text} {Analysis}: {Reproducible} and {Agile} {Production} of {Political} {Data}},
	volume = {110},
	issn = {0003-0554, 1537-5943},
	shorttitle = {Crowd-sourced {Text} {Analysis}},
	url = {https://www.cambridge.org/core/journals/american-political-science-review/article/crowdsourced-text-analysis-reproducible-and-agile-production-of-political-data/EC674A9384A19CFA357BC2B525461AC3},
	doi = {10.1017/S0003055416000058},
	abstract = {Empirical social science often relies on data that are not observed in the field, but are transformed into quantitative variables by expert researchers who analyze and interpret qualitative raw sources. While generally considered the most valid way to produce data, this expert-driven process is inherently difficult to replicate or to assess on grounds of reliability. Using crowd-sourcing to distribute text for reading and interpretation by massive numbers of nonexperts, we generate results comparable to those using experts to read and interpret the same texts, but do so far more quickly and flexibly. Crucially, the data we collect can be reproduced and extended transparently, making crowd-sourced datasets intrinsically reproducible. This focuses researchers’ attention on the fundamental scientific objective of specifying reliable and replicable methods for collecting the data needed, rather than on the content of any particular dataset. We also show that our approach works straightforwardly with different types of political text, written in different languages. While findings reported here concern text analysis, they have far-reaching implications for expert-generated data in the social sciences.},
	language = {en},
	number = {2},
	urldate = {2022-01-25},
	journal = {American Political Science Review},
	author = {Benoit, Kenneth and Conway, Drew and Lauderdale, Benjamin E. and Laver, Michael and Mikhaylov, Slava},
	month = may,
	year = {2016},
	note = {Publisher: Cambridge University Press},
	pages = {278--295},
}

@misc{sheldon_we_2022,
	title = {We need to talk about {Statistics}},
	author = {Sheldon, Neil},
	month = jan,
	year = {2022},
}

@article{barbera_automated_2021,
	title = {Automated {Text} {Classification} of {News} {Articles}: {A} {Practical} {Guide}},
	volume = {29},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Automated {Text} {Classification} of {News} {Articles}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/automated-text-classification-of-news-articles-a-practical-guide/10462DB284B1CD80C0FAE796AD786BC6#access-block},
	doi = {10.1017/pan.2020.8},
	abstract = {Automated text analysis methods have made possible the classification of large corpora of text by measures such as topic and tone. Here, we provide a guide to help researchers navigate the consequential decisions they need to make before any measure can be produced from the text. We consider, both theoretically and empirically, the effects of such choices using as a running example efforts to measure the tone of New York Times coverage of the economy. We show that two reasonable approaches to corpus selection yield radically different corpora and we advocate for the use of keyword searches rather than predefined subject categories provided by news archives. We demonstrate the benefits of coding using article segments instead of sentences as units of analysis. We show that, given a fixed number of codings, it is better to increase the number of unique documents coded rather than the number of coders for each document. Finally, we find that supervised machine learning algorithms outperform dictionaries on a number of criteria. Overall, we intend this guide to serve as a reminder to analysts that thoughtfulness and human validation are key to text-as-data methods, particularly in an age when it is all too easy to computationally classify texts without attending to the methodological choices therein.},
	language = {en},
	number = {1},
	urldate = {2022-01-23},
	journal = {Political Analysis},
	author = {Barberá, Pablo and Boydstun, Amber E. and Linn, Suzanna and McMahon, Ryan and Nagler, Jonathan},
	month = jan,
	year = {2021},
	note = {Publisher: Cambridge University Press},
	pages = {19--42},
}

@misc{noauthor_text_nodate,
	title = {Text {Mining} (and {Information} {Retrieval})},
	url = {http://bactra.org/notebooks/text-mining.html},
	abstract = {mostly topic models},
	urldate = {2021-10-20},
}

@article{roberts_common_2021,
	title = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for {COVID}-19 using chest radiographs and {CT} scans},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00307-0},
	doi = {10.1038/s42256-021-00307-0},
	abstract = {Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
	language = {en},
	number = {3},
	urldate = {2022-01-17},
	journal = {Nature Machine Intelligence},
	author = {Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
	month = mar,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational science;Diagnostic markers;Prognostic markers;SARS-CoV-2
Subject\_term\_id: computational-science;diagnostic-markers;prognostic-markers;sars-cov-2},
	pages = {199--217},
}

@techreport{collischon_methods_2021,
	title = {Methods to {Estimate} {Causal} {Effects} - {An} {Overview} on {IV}, {DiD} and {RDD} and a {Guide} on {How} to {Apply} them in {Practice}},
	url = {https://osf.io/preprints/socarxiv/usvta/},
	abstract = {The identification of causal effects has gained increasing attention in social sciences over the last years and this trend also has found its way into sociology, albeit on a relatively small scale. This article provides an overview of three methods to identify causal effects that are rarely used in sociology: instrumental variable (IV) regression, difference-in-differences (DiD), and regression discontinuity design (RDD). I provide intuitive introductions to these methods, discuss identifying assumptions, limitations of the methods, promising extension, and present an exemplary study for each estimation method that can serve as a benchmark when applying these estimation techniques. Furthermore, the online appendix to this article contains Stata syntax that simulates data and shows how to apply these techniques in practice.},
	language = {en-us},
	urldate = {2022-01-16},
	institution = {SocArXiv},
	author = {Collischon, Matthias},
	month = mar,
	year = {2021},
	doi = {10.31235/osf.io/usvta},
	note = {type: article},
}

@book{baruffa_big_nodate,
	title = {Big {Book} of {R}},
	url = {https://www.bigbookofr.com/},
	abstract = {200+ Free R programming books},
	urldate = {2022-01-16},
	author = {Baruffa, Oscar},
}

@inproceedings{tyler_getting_2020,
	title = {Getting more out of human coders with statistical models},
	author = {Tyler, Matthew},
	year = {2020},
}

@article{fong_machine_2021,
	title = {Machine {Learning} {Predictions} as {Regression} {Covariates}},
	volume = {29},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/machine-learning-predictions-as-regression-covariates/462A74A46A97C20A17CF640BDA72B826},
	doi = {10.1017/pan.2020.38},
	abstract = {In text, images, merged surveys, voter files, and elsewhere, data sets are often missing important covariates, either because they are latent features of observations (such as sentiment in text) or because they are not collected (such as race in voter files). One promising approach for coping with this missing data is to find the true values of the missing covariates for a subset of the observations and then train a machine learning algorithm to predict the values of those covariates for the rest. However, plugging in these predictions without regard for prediction error renders regression analyses biased, inconsistent, and overconfident. We characterize the severity of the problem posed by prediction error, describe a procedure to avoid these inconsistencies under comparatively general assumptions, and demonstrate the performance of our estimators through simulations and a study of hostile political dialogue on the Internet. We provide software implementing our approach.},
	language = {en},
	number = {4},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Fong, Christian and Tyler, Matthew},
	month = oct,
	year = {2021},
	note = {Publisher: Cambridge University Press},
	pages = {467--484},
}

@misc{noauthor_brendan_nodate,
	title = {Brendan {T}. {O}'{Connor} - {UMass} {Amherst}, {Computer} {Science}},
	url = {http://brenocon.com/},
	urldate = {2022-01-16},
}

@article{grimmer_we_2015,
	title = {We {Are} {All} {Social} {Scientists} {Now}: {How} {Big} {Data}, {Machine} {Learning}, and {Causal} {Inference} {Work} {Together}},
	volume = {48},
	issn = {1049-0965, 1537-5935},
	shorttitle = {We {Are} {All} {Social} {Scientists} {Now}},
	url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/we-are-all-social-scientists-now-how-big-data-machine-learning-and-causal-inference-work-together/34F3B787525ED8289185C4CFB775376C},
	doi = {10.1017/S1049096514001784},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096514001784/resource/name/firstPage-S1049096514001784a.jpg},
	language = {en},
	number = {1},
	urldate = {2022-01-16},
	journal = {PS: Political Science \& Politics},
	author = {Grimmer, Justin},
	month = jan,
	year = {2015},
	note = {Publisher: Cambridge University Press},
	pages = {80--83},
}

@article{wallach_computational_2018,
	title = {Computational social science ≠ computer science + social data},
	volume = {61},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3132698},
	doi = {10.1145/3132698},
	abstract = {The important intersection of computer science and social science.},
	language = {en},
	number = {3},
	urldate = {2022-01-16},
	journal = {Communications of the ACM},
	author = {Wallach, Hanna},
	month = feb,
	year = {2018},
	pages = {42--44},
}

@article{stewart_spring_nodate,
	title = {Spring 2019, {Soc592}: {Text} as {Data}: {Statistical} {Text} {Analysis} for the {Social} {Sciences}},
	language = {en},
	author = {Stewart, Brandon and Naslund, Cambria},
	pages = {14},
}

@misc{noauthor_teaching_nodate,
	title = {Teaching by {Brandon} {Stewart}},
	url = {https://scholar.princeton.edu/bstewart/teaching},
	abstract = {I primarily teach courses in Sociology which are sometimes cross listed with Statistics and Machine Learning.  I created the Sociology Summer Methods Camp with my amazing graduate student instructors Rebecca Johnson and Janet Xu. I also founded and run the Sociology Statistics Reading Group.},
	language = {en},
	urldate = {2022-01-16},
}

@article{dimaggio_adapting_2015,
	title = {Adapting computational text analysis to social science (and vice versa)},
	volume = {2},
	issn = {2053-9517},
	url = {https://doi.org/10.1177/2053951715602908},
	doi = {10.1177/2053951715602908},
	abstract = {Social scientists and computer scientist are divided by small differences in perspective and not by any significant disciplinary divide. In the field of text analysis, several such differences are noted: social scientists often use unsupervised models to explore corpora, whereas many computer scientists employ supervised models to train data; social scientists hold to more conventional causal notions than do most computer scientists, and often favor intense exploitation of existing algorithms, whereas computer scientists focus more on developing new models; and computer scientists tend to trust human judgment more than social scientists do. These differences have implications that potentially can improve the practice of social science.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Big Data \& Society},
	author = {DiMaggio, Paul},
	year = {2015},
	note = {Publisher: SAGE Publications Ltd},
	pages = {2053951715602908},
}

@incollection{heiberger_text_2021,
	title = {Text mining and topic modeling},
	isbn = {978-1-00-302524-5},
	url = {https://books.google.com/books?hl=de&lr=&id=9PtNEAAAQBAJ&oi=fnd&pg=PT395&ots=_JzTdveZtI&sig=5_RiwHzV-yRSd-OtiJUuG4x8eTU#v=onepage&q&f=false},
	abstract = {Working with text poses important conceptual and methodological challenges. Topic models are a popular tool to reduce texts’ complexity and find meaningful themes in large corpora. After an overview of existing work, we explain how to employ structural topic models, one of the variations of topic modeling of most relevance to social researchers. In particular, however, this chapter emphasizes the selection of an appropriate number of topics K and its relation to preprocessing. We investigate the influence of preprocessing decisions on (i) the choice of K and (ii) the quality of a topic model (i.e., its predictive power and consistency). For that purpose, we examine a multitude of model setups by employing both established metrics and innovative measures. From our empirical results, we derive several practical recommendations for researchers and provide easy-to-use code to approximate an appropriate number of topics and test the robustness of one’s choice. We develop these arguments with comprehensive data on over 137,000 education-related dissertations completed at U.S. universities.},
	booktitle = {Handbook of {Computational} {Social} {Science}, {Volume} 2},
	publisher = {Routledge},
	author = {Heiberger, Raphael H. and Galvez, Sebastian Munoz-Najar},
	year = {2021},
	note = {https://github.com/RapHei/Handbook\_CSS\_TM},
}

@phdthesis{keith_social_2021,
	title = {Social {Measurement} and {Causal} {Inference} with {Text}},
	author = {Keith, Katherine A.},
	year = {2021},
}

@article{conrad_new_2021,
	title = {New {Data} in {Social} and {Behavioral} {Research}},
	volume = {85},
	issn = {0033-362X},
	url = {https://doi.org/10.1093/poq/nfab027},
	doi = {10.1093/poq/nfab027},
	abstract = {A decade ago, in this journal’s 75th anniversary special issue, Groves (2011) called for the survey research community to collectively assess the role of what he called “organic” data in scientifically measuring social phenomena. What distinguishes these new potential sources of data–also called “found” (Taylor 2013) or “ready-made” (Salganik 2018)–is that they are not elicited by researchers, but rather are continuously generated by ongoing, everyday processes that happen independently of social researchers, who then repurpose the data to address social research questions.The idea that exploiting such new data sources will revolutionize social and behavioral research has generated both excitement (e.g., Savage and Burrows 2009; Mayer-Schönberger and Cukier 2013; Golder and Macy 2014; Ceron, Curini, and Iacus 2015) and skepticism (e.g., Smith 2013; Couper 2013; Jungherr, Jürgens, and Schoen 2012). The past decade has seen a proliferation of substantive and methodological research across a range of disciplines and in a range of venues using new data sources and exploring their potential confluence with traditional data (see, e.g., Eck et al. 2019; Edgerly and Thorson 2020; Hill et al. 2021, among many others).},
	number = {S1},
	urldate = {2022-01-16},
	journal = {Public Opinion Quarterly},
	author = {Conrad, Frederick G and Keusch, Florian and Schober, Michael F},
	month = sep,
	year = {2021},
	pages = {253--263},
}

@incollection{radford_big_2019,
	title = {Big {Data} for {Sociological} {Research}},
	isbn = {978-1-119-42933-3},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119429333.ch24},
	abstract = {This chapter details the early successes of the application of big data to sociological questions. It highlights the kinds of novel insights available to sociologists using big data while also pointing out the shortcomings and pitfalls even the best researchers fall victim to. One of these insights is that what constitutes “big data” is always changing as the size, type, and scale of data evolves. Big data comes in three varieties: digitalized life, digital trace data, and digital life. All three of these sources of big data can be instrumented. People can build new repositories of data, create applications that collect digital traces, or simulate online social interaction in digital worlds. Much of sociology is based on self-reported data. National surveys, political polls, even the census are premised on people accurately and faithfully remembering and reporting things about themselves. The chapter concludes by articulating some coming changes in big data likely to reshape sociological research.},
	language = {en},
	urldate = {2022-01-16},
	booktitle = {The {Wiley} {Blackwell} {Companion} to {Sociology}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Radford, Jason and Lazer, David},
	year = {2019},
	doi = {10.1002/9781119429333.ch24},
	note = {Section: 24
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119429333.ch24},
	pages = {417--443},
}

@article{goldenstein_analyzing_2019,
	title = {Analyzing {Meaning} in {Big} {Data}: {Performing} a {Map} {Analysis} {Using} {Grammatical} {Parsing} and {Topic} {Modeling} (with discussion)},
	volume = {49},
	issn = {0081-1750},
	shorttitle = {Analyzing {Meaning} in {Big} {Data}},
	url = {https://doi.org/10.1177/0081175019852762},
	doi = {10.1177/0081175019852762},
	abstract = {Social scientists have recently started discussing the utilization of text-mining tools as being fruitful for scaling inductively grounded close reading. We aim to progress in this direction and provide a contemporary contribution to the literature. By focusing on map analysis, we demonstrate the potential of text-mining tools for text analysis that approaches inductive but still formal in-depth analysis. We propose that a combination of text-mining tools addressing different layers of meaning facilitates a closer analysis of the dynamics of manifest and latent meanings than is currently acknowledged. To illustrate our approach, we combine grammatical parsing and topic modeling to operationalize communication structures within sentences and the semantic surroundings of these communication structures. We use a reliable and downloadable software application to analyze the dynamic interlacement of two layers of meaning over time. We do so by analyzing 15,371 newspaper articles on corporate responsibility published in the United States from 1950 to 2013.},
	language = {en},
	number = {1},
	urldate = {2022-01-16},
	journal = {Sociological Methodology},
	author = {Goldenstein, Jan and Poschmann, Philipp},
	month = aug,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {83--131},
}

@article{di_franco_machine_2021,
	title = {Machine learning, artificial neural networks and social research},
	volume = {55},
	issn = {1573-7845},
	url = {https://doi.org/10.1007/s11135-020-01037-y},
	doi = {10.1007/s11135-020-01037-y},
	abstract = {Machine learning (ML), and particularly algorithms based on artificial neural networks (ANNs), constitute a field of research lying at the intersection of different disciplines such as mathematics, statistics, computer science and neuroscience. This approach is characterized by the use of algorithms to extract knowledge from large and heterogeneous data sets. In addition to offering a brief introduction to ANN algorithms-based ML, in this paper we will focus our attention on its possible applications in the social sciences and, in particular, on its potential in the data analysis procedures. In this regard, we will provide three examples of applications on sociological data to assess the impact of ML in the study of relationships between variables. Finally, we will compare the potential of ML with traditional data analysis models.},
	language = {en},
	number = {3},
	urldate = {2022-01-16},
	journal = {Quality \& Quantity},
	author = {Di Franco, Giovanni and Santurro, Michele},
	month = jun,
	year = {2021},
	pages = {1007--1025},
}

@article{molina_machine_2019,
	title = {Machine {Learning} for {Sociology}},
	volume = {45},
	url = {https://doi.org/10.1146/annurev-soc-073117-041106},
	doi = {10.1146/annurev-soc-073117-041106},
	abstract = {Machine learning is a field at the intersection of statistics and computer science that uses algorithms to extract information and knowledge from data. Its applications increasingly find their way into economics, political science, and sociology. We offer a brief introduction to this vast toolbox and illustrate its current uses in the social sciences, including distilling measures from new data sources, such as text and images; characterizing population heterogeneity; improving causal inference; and offering predictions to aid policy decisions and theory development. We argue that, in addition to serving similar purposes in sociology, machine learning tools can speak to long-standing questions on the limitations of the linear modeling framework, the criteria for evaluating empirical findings, transparency around the context of discovery, and the epistemological core of the discipline.},
	number = {1},
	urldate = {2022-01-16},
	journal = {Annual Review of Sociology},
	author = {Molina, Mario and Garip, Filiz},
	year = {2019},
	note = {\_eprint: https://doi.org/10.1146/annurev-soc-073117-041106},
	pages = {27--45},
}

@article{evans_machine_2016,
	title = {Machine {Translation}: {Mining} {Text} for {Social} {Theory}},
	volume = {42},
	issn = {0360-0572, 1545-2115},
	shorttitle = {Machine {Translation}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-soc-081715-074206},
	doi = {10.1146/annurev-soc-081715-074206},
	abstract = {More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online transactions, government intelligence, and digitized libraries. This supply of text has elicited demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data. We survey some of the most exciting computational approaches to text analysis, highlighting both supervised methods that extend old theories to new data and unsupervised techniques that discover hidden regularities worth theorizing. We then review recent research that uses these tools to develop social insight by exploring (a) collective attention and reasoning through the content of communication; (b) social relationships through the process of communication; and (c) social states, roles, and moves identified through heterogeneous signals within communication. We highlight social questions for which these advances could offer powerful new insight.},
	language = {en},
	number = {1},
	urldate = {2022-01-16},
	journal = {Annual Review of Sociology},
	author = {Evans, James A. and Aceves, Pedro},
	month = jul,
	year = {2016},
	pages = {21--50},
}

@article{hovy_text_2020,
	title = {Text {Analysis} in {Python} for {Social} {Scientists}: {Discovery} and {Exploration}},
	shorttitle = {Text {Analysis} in {Python} for {Social} {Scientists}},
	url = {https://www.cambridge.org/core/elements/text-analysis-in-python-for-social-scientists/BFAB0A3604C7E29F6198EA2F7941DFF3},
	doi = {10.1017/9781108873352},
	abstract = {Cambridge Core - Political Theory - Text Analysis in Python for Social Scientists},
	language = {en},
	urldate = {2022-01-16},
	journal = {Elements in Quantitative and Computational Methods for the Social Sciences},
	author = {Hovy, Dirk},
	month = dec,
	year = {2020},
	note = {ISBN: 9781108873352 9781108819824
Publisher: Cambridge University Press},
}

@article{denny_text_2018,
	title = {Text {Preprocessing} {For} {Unsupervised} {Learning}: {Why} {It} {Matters}, {When} {It} {Misleads}, {And} {What} {To} {Do} {About} {It}},
	volume = {26},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Text {Preprocessing} {For} {Unsupervised} {Learning}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/text-preprocessing-for-unsupervised-learning-why-it-matters-when-it-misleads-and-what-to-do-about-it/AA7D4DE0AA6AB208502515AE3EC6989E},
	doi = {10.1017/pan.2017.44},
	abstract = {Despite the popularity of unsupervised techniques for political science text-as-data research, the importance and implications of preprocessing decisions in this domain have received scant systematic attention. Yet, as we show, such decisions have profound effects on the results of real models for real data. We argue that substantive theory is typically too vague to be of use for feature selection, and that the supervised literature is not necessarily a helpful source of advice. To aid researchers working in unsupervised settings, we introduce a statistical procedure and software that examines the sensitivity of findings under alternate preprocessing regimes. This approach complements a researcher’s substantive understanding of a problem by providing a characterization of the variability changes in preprocessing choices may induce when analyzing a particular dataset. In making scholars aware of the degree to which their results are likely to be sensitive to their preprocessing decisions, it aids replication efforts.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Denny, Matthew J. and Spirling, Arthur},
	month = apr,
	year = {2018},
	note = {Publisher: Cambridge University Press},
	pages = {168--189},
}

@article{jerzak_improved_2022,
	title = {An {Improved} {Method} of {Automated} {Nonparametric} {Content} {Analysis} for {Social} {Science}},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/an-improved-method-of-automated-nonparametric-content-analysis-for-social-science/D3C7441B17313F6E33A7BF2E781B5086},
	doi = {10.1017/pan.2021.36},
	abstract = {Some scholars build models to classify documents into chosen categories. Others, especially social scientists who tend to focus on population characteristics, instead usually estimate the proportion of documents in each category—using either parametric “classify-and-count” methods or “direct” nonparametric estimation of proportions without individual classification. Unfortunately, classify-and-count methods can be highly model-dependent or generate more bias in the proportions even as the percent of documents correctly classified increases. Direct estimation avoids these problems, but can suffer when the meaning of language changes between training and test sets or is too similar across categories. We develop an improved direct estimation approach without these issues by including and optimizing continuous text features, along with a form of matching adapted from the causal inference literature. Our approach substantially improves performance in a diverse collection of 73 datasets. We also offer easy-to-use software that implements all ideas discussed herein.},
	language = {en},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Jerzak, Connor T. and King, Gary and Strezhnev, Anton},
	month = jan,
	year = {2022},
	note = {Publisher: Cambridge University Press},
	pages = {1--17},
}

@inproceedings{cranmer_introduction_2018,
	title = {Introduction to the {Virtual} {Issue}: {Machine} {Learning} in {Political} {Science}},
	url = {https://www.cambridge.org/core/journals/political-analysis/special-collections/machine-learning-in-political-science},
	author = {Cranmer, Skyler J.},
	year = {2018},
}

@article{lowe_validating_2013,
	title = {Validating {Estimates} of {Latent} {Traits} from {Textual} {Data} {Using} {Human} {Judgment} as a {Benchmark}},
	volume = {21},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/validating-estimates-of-latent-traits-from-textual-data-using-human-judgment-as-a-benchmark/8E55A149753CE11CC3388A4408C55F48},
	doi = {10.1093/pan/mpt002},
	abstract = {Automated and statistical methods for estimating latent political traits and classes from textual data hold great promise, because virtually every political act involves the production of text. Statistical models of natural language features, however, are heavily laden with unrealistic assumptions about the process that generates these data, including the stochastic process of text generation, the functional link between political variables and observed text, and the nature of the variables (and dimensions) on which observed text should be conditioned. While acknowledging statistical models of latent traits to be “wrong,” political scientists nonetheless treat their results as sufficiently valid to be useful. In this article, we address the issue of substantive validity in the face of potential model failure, in the context of unsupervised scaling methods of latent traits. We critically examine one popular parametric measurement model of latent traits for text and then compare its results to systematic human judgments of the texts as a benchmark for validity.},
	language = {en},
	number = {3},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Lowe, Will and Benoit, Kenneth},
	year = {2013},
	note = {Publisher: Cambridge University Press},
	pages = {298--313},
}

@article{elff_dynamic_2013,
	title = {A {Dynamic} {State}-{Space} {Model} of {Coded} {Political} {Texts}},
	volume = {21},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/dynamic-statespace-model-of-coded-political-texts/B29D9DD75CCBC37248CCBAF708D24917},
	doi = {10.1093/pan/mps042},
	abstract = {This article presents a new method of reconstructing actors' political positions from coded political texts. It is based on a model that combines a dynamic perspective on actors' political positions with a probabilistic account of how these positions are translated into emphases of policy topics in political texts. In the article it is shown how model parameters can be estimated based on a maximum marginal likelihood principle and how political actors' positions can be reconstructed using empirical Bayes techniques. For this purpose, a Monte Carlo Expectation Maximization algorithm is used that employs independent sample techniques with automatic Monte Carlo sample size adjustment. An example application is given by estimating a model of an economic policy space and a noneconomic policy space based on the data from the Comparative Manifesto Project. Parties' positions in policy spaces reconstructed using these models are made publicly available for download.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Elff, Martin},
	year = {2013},
	note = {Publisher: Cambridge University Press},
	pages = {217--232},
}

@article{dorazio_separating_2014,
	title = {Separating the {Wheat} from the {Chaff}: {Applications} of {Automated} {Document} {Classification} {Using} {Support} {Vector} {Machines}},
	volume = {22},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Separating the {Wheat} from the {Chaff}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/separating-the-wheat-from-the-chaff-applications-of-automated-document-classification-using-support-vector-machines/1E5431CC964E45218255584A4331D423},
	doi = {10.1093/pan/mpt030},
	abstract = {Due in large part to the proliferation of digitized text, much of it available for little or no cost from the Internet, political science research has experienced a substantial increase in the number of data sets and large-n research initiatives. As the ability to collect detailed information on events of interest expands, so does the need to efficiently sort through the volumes of available information. Automated document classification presents a particularly attractive methodology for accomplishing this task. It is efficient, widely applicable to a variety of data collection efforts, and considerably flexible in tailoring its application for specific research needs. This article offers a holistic review of the application of automated document classification for data collection in political science research by discussing the process in its entirety. We argue that the application of a two-stage support vector machine (SVM) classification process offers advantages over other well-known alternatives, due to the nature of SVMs being a discriminative classifier and having the ability to effectively address two primary attributes of textual data: high dimensionality and extreme sparseness. Evidence for this claim is presented through a discussion of the efficiency gains derived from using automated document classification on the Militarized Interstate Dispute 4 (MID4) data collection project.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {D'Orazio, Vito and Landis, Steven T. and Palmer, Glenn and Schrodt, Philip},
	year = {2014},
	note = {Publisher: Cambridge University Press},
	pages = {224--242},
}

@article{lucas_computer-assisted_2015,
	title = {Computer-{Assisted} {Text} {Analysis} for {Comparative} {Politics}},
	volume = {23},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/computerassisted-text-analysis-for-comparative-politics/CC8B2CF63A8CC36FE00A13F9839F92BB},
	doi = {10.1093/pan/mpu019},
	abstract = {Recent advances in research tools for the systematic analysis of textual data are enabling exciting new research throughout the social sciences. For comparative politics, scholars who are often interested in non-English and possibly multilingual textual datasets, these advances may be difficult to access. This article discusses practical issues that arise in the processing, management, translation, and analysis of textual data with a particular focus on how procedures differ across languages. These procedures are combined in two applied examples of automated text analysis using the recently introduced Structural Topic Model. We also show how the model can be used to analyze data that have been translated into a single language via machine translation tools. All the methods we describe here are implemented in open-source software packages available from the authors.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Lucas, Christopher and Nielsen, Richard A. and Roberts, Margaret E. and Stewart, Brandon M. and Storer, Alex and Tingley, Dustin},
	year = {2015},
	note = {Publisher: Cambridge University Press},
	pages = {254--277},
}

@article{harris_whats_2015,
	title = {What's in a {Name}? {A} {Method} for {Extracting} {Information} about {Ethnicity} from {Names}},
	volume = {23},
	issn = {1047-1987, 1476-4989},
	shorttitle = {What's in a {Name}?},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/whats-in-a-name-a-method-for-extracting-information-about-ethnicity-from-names/8A97A3091967FAA124150145DD9B9B37},
	doi = {10.1093/pan/mpu038},
	abstract = {Questions about racial or ethnic group identity feature centrally in many social science theories, but detailed data on ethnic composition are often difficult to obtain, out of date, or otherwise unavailable. The proliferation of publicly available geocoded person names provides one potential source of such data'if researchers can effectively link names and group identity. This article examines that linkage and presents a methodology for estimating local ethnic or racial composition using the relationship between group membership and person names. Common approaches for linking names and identity groups perform poorly when estimating group proportions. I have developed a new method for estimating racial or ethnic composition from names which requires no classification of individual names. This method provides more accurate estimates than the standard approach and works in any context where person names contain information about group membership. Illustrations from two very different contexts are provided: the United States and the Republic of Kenya.},
	language = {en},
	number = {2},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Harris, J. Andrew},
	year = {2015},
	note = {Publisher: Cambridge University Press},
	pages = {212--224},
}

@article{roberts_introduction_2016,
	title = {Introduction to the {Virtual} {Issue}: {Recent} {Innovations} in {Text} {Analysis} for {Social} {Science}},
	volume = {24},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Introduction to the {Virtual} {Issue}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/introduction-to-the-virtual-issue-recent-innovations-in-text-analysis-for-social-science/8CFCA019C25D82254EE0CAC28F3F650E},
	doi = {10.1017/S1047198700014418},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1047198700014418/resource/name/firstPage-S1047198700014418a.jpg},
	language = {en},
	number = {V10},
	urldate = {2022-01-16},
	journal = {Political Analysis},
	author = {Roberts, Margaret E.},
	month = apr,
	year = {2016},
	note = {Publisher: Cambridge University Press},
	pages = {1--5},
}

@article{grimmer_machine_2021,
	title = {Machine {Learning} for {Social} {Science}: {An} {Agnostic} {Approach}},
	volume = {24},
	shorttitle = {Machine {Learning} for {Social} {Science}},
	url = {https://doi.org/10.1146/annurev-polisci-053119-015921},
	doi = {10.1146/annurev-polisci-053119-015921},
	abstract = {Social scientists are now in an era of data abundance, and machine learning tools are increasingly used to extract meaning from data sets both massive and small. We explain how the inclusion of machine learning in the social sciences requires us to rethink not only applications of machine learning methods but also best practices in the social sciences. In contrast to the traditional tasks for machine learning in computer science and statistics, when machine learning is applied to social scientific data, it is used to discover new concepts, measure the prevalence of those concepts, assess causal effects, and make predictions. The abundance of data and resources facilitates the move away from a deductive social science to a more sequential, interactive, and ultimately inductive approach to inference. We explain how an agnostic approach to machine learning methods focused on the social science tasks facilitates progress across a wide range of questions.},
	number = {1},
	urldate = {2022-01-16},
	journal = {Annual Review of Political Science},
	author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-polisci-053119-015921},
	pages = {395--419},
}

@article{feder_causal_2021,
	title = {Causal {Inference} in {Natural} {Language} {Processing}: {Estimation}, {Prediction}, {Interpretation} and {Beyond}},
	shorttitle = {Causal {Inference} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2109.00725},
	abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the computational linguistics community.},
	urldate = {2022-01-16},
	journal = {arXiv:2109.00725 [cs]},
	author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.00725},
}

@inproceedings{egami_how_nodate,
	title = {How {To} {Make} {Causal} {Inferences} {Using} {Texts}},
	url = {https://www.dropbox.com/s/b1n1r2qpyn3m77c/ais_final.pdf?dl=0},
	author = {Egami, Naoki and Fong, Christian J and Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	note = {https://paperswithcode.com/paper/how-to-make-causal-inferences-using-texts/review/},
}

@book{grimmer_text_2021,
	address = {S.l.},
	title = {{TEXT} {AS} {DATA}: a new framework for machine learning and the social sciences.},
	isbn = {978-0-691-20755-1 978-0-691-20754-4},
	shorttitle = {{TEXT} {AS} {DATA}},
	language = {English},
	publisher = {Princeton University Press},
	author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	year = {2021},
	note = {OCLC: 1246624235},
}

@book{huntington-klein_effect_2021,
	address = {New York},
	title = {The {Effect}: {An} {Introduction} to {Research} {Design} and {Causality}},
	isbn = {978-1-00-322605-5},
	shorttitle = {The {Effect}},
	url = {https://theeffectbook.net/index.html},
	abstract = {The Effect: An Introduction to Research Design and Causality is about research design, specifically concerning research that uses observational data to make a causal inference. It is separated into two halves, each with different approaches to that subject. The first half goes through the concepts of causality, with very little in the way of estimation. It introduces the concept of identification thoroughly and clearly and discusses it as a process of trying to isolate variation that has a causal interpretation. Subjects include heavy emphasis on data-generating processes and causal diagrams.
Concepts are demonstrated with a heavy emphasis on graphical intuition and the question of what we do to data. When we “add a control variable” what does that actually do?
Key Features:

 • Extensive code examples in R, Stata, and Python
• Chapters on overlooked topics in econometrics classes: heterogeneous treatment effects, simulation and power analysis, new cutting-edge methods, and uncomfortable ignored assumptions
• An easy-to-read conversational tone
• Up-to-date coverage of methods with fast-moving literatures like difference-in-differences},
	publisher = {Chapman and Hall/CRC},
	author = {Huntington-Klein, Nick},
	month = dec,
	year = {2021},
	doi = {10.1201/9781003226055},
}

@book{baur_handbuch_2019,
	address = {Wiesbaden},
	title = {Handbuch {Methoden} der empirischen {Sozialforschung}},
	isbn = {978-3-658-21307-7 978-3-658-21308-4},
	url = {http://link.springer.com/10.1007/978-3-658-21308-4},
	language = {de},
	urldate = {2022-01-16},
	publisher = {Springer Fachmedien Wiesbaden},
	editor = {Baur, Nina and Blasius, Jörg},
	year = {2019},
	doi = {10.1007/978-3-658-21308-4},
}

@book{beaulieu_data_2021,
	title = {Data and {Society}: {A} {Critical} {Introduction}},
	language = {English},
	publisher = {SAGE Publications Sage CA: Los Angeles, CA},
	author = {Beaulieu, Anne and Leonelli, Sabina},
	month = nov,
	year = {2021},
}

@article{gratz_when_2022,
	title = {When less conditioning provides better estimates: overcontrol and endogenous selection biases in research on intergenerational mobility},
	issn = {1573-7845},
	shorttitle = {When less conditioning provides better estimates},
	url = {https://doi.org/10.1007/s11135-021-01310-8},
	doi = {10.1007/s11135-021-01310-8},
	abstract = {The counterfactual approach to causality has become the dominant approach to understand causality in contemporary social science research. Whilst most sociologists are aware that unobserved, confounding variables may bias the estimates of causal effects (omitted variable bias), the threats of overcontrol and endogenous selection biases are less well known. In particular, widely used practices in research on intergenerational mobility are affected by these biases. I review four of these practices from the viewpoint of the counterfactual approach to causality and show why overcontrol and endogenous selection biases arise when these practices are implemented. I use data from the German Socio-Economic Panel Study (SOEP) to demonstrate the practical consequences of these biases for conclusions about intergenerational mobility. I conclude that future research on intergenerational mobility should reflect more upon the possibilities of bias introduced by conditioning on variables.},
	language = {en},
	urldate = {2022-01-09},
	journal = {Quality \& Quantity},
	author = {Grätz, Michael},
	month = jan,
	year = {2022},
}

@article{anderlucci_importance_2019,
	title = {The {Importance} of {Being} {Clustered}: {Uncluttering} the {Trends} of {Statistics} from 1970 to 2015},
	volume = {34},
	issn = {0883-4237, 2168-8745},
	shorttitle = {The {Importance} of {Being} {Clustered}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-34/issue-2/The-Importance-of-Being-Clustered--Uncluttering-the-Trends-of/10.1214/18-STS686.full},
	doi = {10.1214/18-STS686},
	abstract = {In this paper, we retrace the recent history of statistics by analyzing all the papers published in five prestigious statistical journals since 1970, namely: The Annals of Statistics, Biometrika, Journal of the American Statistical Association, Journal of the Royal Statistical Society, Series B and Statistical Science. The aim is to construct a kind of “taxonomy” of the statistical papers by organizing and clustering them in main themes. In this sense being identified in a cluster means being important enough to be uncluttered in the vast and interconnected world of the statistical research. Since the main statistical research topics naturally born, evolve or die during time, we will also develop a dynamic clustering strategy, where a group in a time period is allowed to migrate or to merge into different groups in the following one. Results show that statistics is a very dynamic and evolving science, stimulated by the rise of new research questions and types of data.},
	number = {2},
	urldate = {2022-01-08},
	journal = {Statistical Science},
	author = {Anderlucci, Laura and Montanari, Angela and Viroli, Cinzia},
	month = may,
	year = {2019},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {280--300},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {Pyro} — {Pyro} {Tutorials} 1.8.0 documentation},
	url = {https://pyro.ai/examples/intro_long.html},
	urldate = {2022-01-08},
}

@article{gelman_what_2021,
	title = {What are the {Most} {Important} {Statistical} {Ideas} of the {Past} 50 {Years}?},
	volume = {116},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1938081},
	doi = {10.1080/01621459.2021.1938081},
	abstract = {We review the most important statistical ideas of the past half century, which we categorize as: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, Bayesian multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss key contributions in these subfields, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
	number = {536},
	urldate = {2022-01-07},
	journal = {Journal of the American Statistical Association},
	author = {Gelman, Andrew and Vehtari, Aki},
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2021.1938081},
	pages = {2087--2097},
}

@article{pedersen_lessons_2022,
	title = {Lessons {Learned} {Developing} and {Using} a {Machine} {Learning} {Model} to {Automatically} {Transcribe} 2.3 {Million} {Handwritten} {Occupation} {Codes}},
	volume = {11},
	copyright = {Copyright (c) 2022 Bjørn-Richard Pedersen, Einar Holsbø, Trygve Andersen, Nikita Shvetsov, Johan Ravn, Hilde Leikny Sommerseth, Lars Ailo Bongo},
	issn = {2352-6343},
	url = {https://hlcs.nl/article/view/11331},
	doi = {10.51964/hlcs11331},
	abstract = {Machine learning approaches achieve high accuracy for text recognition and are therefore increasingly used for the transcription of handwritten historical sources. However, using machine learning in production requires a streamlined end-to-end pipeline that scales to the dataset size and a model that achieves high accuracy with few manual transcriptions. The correctness of the model results must also be verified. This paper describes our lessons learned developing, tuning and using the Occode end-to-end machine learning pipeline for transcribing 2.3 million handwritten occupation codes from the Norwegian 1950 population census. We achieve an accuracy of 97\% for the automatically transcribed codes, and we send 3\% of the codes for manual verification . We verify that the occupation code distribution found in our results matches the distribution found in our training data, which should be representative for the census as a whole. We believe our approach and lessons learned may be useful for other transcription projects that plan to use machine learning in production. The source code is available at https://github.com/uit-hdl/rhd-codes.},
	language = {en},
	urldate = {2022-01-07},
	journal = {Historical Life Course Studies},
	author = {Pedersen, Bjørn-Richard and Holsbø, Einar and Andersen, Trygve and Shvetsov, Nikita and Ravn, Johan and Sommerseth, Hilde Leikny and Bongo, Lars Ailo},
	month = jan,
	year = {2022},
	pages = {1--17},
}

@book{prado_time_2010,
	address = {Boca Raton},
	series = {Chapman \& {Hall}/{CRC} texts in statistical science series},
	title = {Time series: modeling, computation, and inference},
	isbn = {978-1-4200-9336-0},
	shorttitle = {Time series},
	abstract = {"Focusing on Bayesian approaches and computations using up-to-date simulation-based methods for inference, Time Series: Modeling, Computation, and Inference integrates mainstream approaches for time series modeling with significant recent developments in methodology and applications of time series analysis. It encompasses a graduate-level account of Bayesian time series modeling and analysis, a broad range of references to state-of-the-art approaches to univariate and multivariate time series analysis, and emerging topics at research frontiers. The book presents overviews of several classes of models and related methodology for inference, statistical computation for model fitting and assessment, and forecasting. The authors also explore the connections between time- and frequency-domain approaches and develop various models and analyses using Bayesian tools, such as Markov chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) methods. They illustrate the models and methods with examples and case studies from a variety of fields, including signal processing, biomedicine, and finance. Data sets, R and MATLAB® code, and other material are available on the authors' websites. Along with core models and methods, this text offers sophisticated tools for analyzing challenging time series problems. It also demonstrates the growth of time series analysis into new application areas."--Publisher's description},
	publisher = {CRC Press},
	author = {Prado, Raquel and West, Mike},
	year = {2010},
}

@book{pearl_causal_2016,
	address = {Chichester, West Sussex},
	title = {Causal inference in statistics: a primer},
	isbn = {978-1-119-18684-7},
	shorttitle = {Causal inference in statistics},
	publisher = {Wiley},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
	year = {2016},
}

@book{national_academies_of_sciences_engineering_and_medicine_transparency_2021,
	address = {Washington, D.C.},
	title = {Transparency in {Statistical} {Information} for the {National} {Center} for {Science} and {Engineering} {Statistics} and {All} {Federal} {Statistical} {Agencies}},
	isbn = {978-0-309-27045-8},
	url = {https://www.nap.edu/catalog/26360},
	language = {en},
	urldate = {2022-01-03},
	publisher = {National Academies Press},
	author = {{National Academies of Sciences, Engineering, and Medicine}},
	year = {2021},
	doi = {10.17226/26360},
	note = {Pages: 26360},
}

@book{bowen_protecting_2021,
	address = {New York},
	title = {Protecting {Your} {Privacy} in a {Data}-{Driven} {World}},
	isbn = {978-1-00-312204-3},
	abstract = {At what point does the sacrifice to our personal information outweigh the public good?

If public policymakers had access to our personal and confidential data, they could make more evidence-based, data-informed decisions that could accelerate economic recovery and improve COVID-19 vaccine distribution. However, access to personal data comes at a steep privacy cost for contributors, especially underrepresented groups.

Protecting Your Privacy in a Data-Driven World is a practical, nontechnical guide that explains the importance of balancing these competing needs and calls for careful consideration of how data are collected and disseminated by our government and the private sector. Not addressing these concerns can harm the same communities policymakers are trying to protect through data privacy and confidentiality legislation.},
	publisher = {Chapman and Hall/CRC},
	author = {Bowen, Claire McKay},
	month = nov,
	year = {2021},
	doi = {10.1201/9781003122043},
}

@book{grant_data_2018,
	address = {New York},
	title = {Data {Visualization}: {Charts}, {Maps}, and {Interactive} {Graphics}},
	isbn = {978-1-315-20135-1},
	shorttitle = {Data {Visualization}},
	abstract = {This is the age of data. There are more innovations and more opportunities for interesting work with data than ever before, but there is also an overwhelming amount of quantitative information being published every day. Data visualisation has become big business, because communication is the difference between success and failure, no matter how clever the analysis may have been. The ability to visualize data is now a skill in demand across business, government, NGOs and academia. Data Visualization: Charts, Maps, and Interactive Graphics gives an overview of a wide range of techniques and challenges, while staying accessible to anyone interested in working with and understanding data.
Features:

Focusses on concepts and ways of thinking about data rather than algebra or computer code.
Features 17 short chapters that can be read in one sitting.
Includes chapters on big data, statistical and machine learning models, visual perception, high-dimensional data, and maps and geographic data.
Contains more than 125 visualizations, most created by the author.
Supported by a website with all code for creating the visualizations, further reading, datasets and practical advice on crafting the images.

Whether you are a student considering a career in data science, an analyst who wants to learn more about visualization, or the manager of a team working with data, this book will introduce you to a broad range of data visualization methods.

Cover image: Landscape of Change uses data about sea level rise, glacier volume decline,
increasing global temperatures, and the increasing use of fossil fuels. These data lines
compose a landscape shaped by the changing climate, a world in which we are now living.
Copyright © Jill Pelto (jillpelto.com).},
	publisher = {Chapman and Hall/CRC},
	author = {Grant, Robert},
	month = dec,
	year = {2018},
	doi = {10.1201/9781315201351},
}

@book{varshney_trustworthy_2021,
	address = {Chappaqua, NY},
	title = {Trustworthy {Machine} {Learning}},
	url = {http://www.trustworthymachinelearning.com/trustworthymachinelearning.pdf},
	urldate = {2022-01-03},
	author = {Varshney, Kush R.},
	year = {2021},
}

@article{christen_common_2021,
	title = {Common {Misconceptions} about {Population} {Data}},
	url = {http://arxiv.org/abs/2112.10912},
	abstract = {Databases covering all individuals of a population are increasingly used for research studies in domains ranging from public health to the social sciences. There is also growing interest by governments and businesses to use population data to support data-driven decision making. The massive size of such databases is often mistaken as a guarantee for valid inferences on the population of interest. However, population data have characteristics that make them challenging to use, including various assumptions being made how such data were collected and what types of processing have been applied to them. Furthermore, the full potential of population data can often only be unlocked when such data are linked to other databases, a process that adds fresh challenges. This article discusses a diverse range of misconceptions about population data that we believe anybody who works with such data needs to be aware of. Many of these misconceptions are not well documented in scientific publications but only discussed anecdotally among researchers and practitioners. We conclude with a set of recommendations for inference when using population data.},
	urldate = {2021-12-31},
	journal = {arXiv:2112.10912 [cs]},
	author = {Christen, Peter and Schnell, Rainer},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.10912},
}

@article{peycheva_occupation_2021,
	title = {Occupation {Coding} {During} the {Interview} in a {Web}-{First} {Sequential} {Mixed}-{Mode} {Survey}},
	volume = {37},
	url = {https://sciendo.com/article/10.2478/jos-2021-0042},
	doi = {10.2478/jos-2021-0042},
	abstract = {Abstract
Coding respondent occupation is one of the most challenging aspects of survey data collection. Traditionally performed manually by office coders post-interview, previous research has acknowledged the advantages of coding occupation during the interview, including reducing costs, processing time and coding uncertainties that are more difficult to address post-interview. However, a number of concerns have been raised as well, including the potential for interviewer effects, the challenge of implementing the coding system in a web survey, in which respondents perform the coding procedure themselves, or the feasibility of implementing the same standardized coding system in a mixed-mode self- and interviewer-administered survey. This study sheds light on these issues by presenting an evaluation of a new occupation coding method administered during the interview in a large-scale sequential mixed-mode (web, telephone, face-to-face) cohort study of young adults in the UK. Specifically, we assess the take-up rates of this new coding method across the different modes and report on several other performance measures thought to impact the quality of the collected occupation data. Furthermore, we identify factors that affect the coding of occupation during the interview, including interviewer effects. The results carry several implications for survey practice and directions for future research.},
	language = {en},
	number = {4},
	urldate = {2021-12-31},
	journal = {Journal of Official Statistics},
	author = {Peycheva, Darina N. and Sakshaug, Joseph W. and Calderwood, Lisa},
	month = nov,
	year = {2021},
	pages = {981--1007},
}

@book{berger_einladung_2011,
	address = {Konstanz},
	edition = {Neuaufl.},
	series = {{UTB} {Soziologie}},
	title = {Einladung zur {Soziologie}: eine humanistische {Perspektive}},
	isbn = {978-3-8252-3495-9},
	shorttitle = {Einladung zur {Soziologie}},
	abstract = {Im Alter von nur 35 Jahren schrieb Peter L. Berger seinen Bestseller "Invitation to Sociology" (1963), der zwischenzeitlich in 15 Sprachen übersetzt wurde und nach wie vor auf den Semesterliteraturlisten von soziologischen Einführungsveranstaltungen steht. Berger eröffnet mit seiner konkurrenz- und zeitlosen "Einladung zur Soziologie" auf leichtfüi︢ge und eingängige Art einen Zugang in die Denk- und Arbeitsweisen des Fachs. Er versucht dabei nicht, einen aktuellen Überblick über relevante Theorien, Ansätze, Methoden der Soziologie zu geben, sondern vielmehr eine Art des Denkens zu vermitteln, die als soziologischer Blick bezeichnet werden kann. Mit dieser UTB-Studienausgabe wird das wichtige Werk Studienanfängern wieder zugänglich gemacht},
	language = {ger},
	number = {3495},
	publisher = {UVK-Verl.-Ges},
	author = {Berger, Peter L. and Berger, Peter L.},
	editor = {Pfadenhauer, Michaela},
	year = {2011},
}

@book{kron_soziologie_2022,
	address = {Stuttgart},
	edition = {1. Auflage},
	title = {Soziologie verstehen: eine problemorientierte {Einführung}},
	isbn = {978-3-17-036861-3},
	shorttitle = {Soziologie verstehen},
	language = {ger},
	publisher = {Verlag W. Kohlhammer},
	author = {Kron, Thomas and Laut, Christina},
	year = {2022},
}

@book{manzo_research_2021,
	address = {Northampton},
	series = {Research handbooks in sociology series},
	title = {Research handbook on analytical sociology},
	isbn = {978-1-78990-684-4},
	abstract = {"Providing an up-to-date portrait of the concepts and methods of analytical sociology, this pivotal Research Handbook traces the historical evolution of the field, utilising key research examples to illustrate its core principles. It investigates how analytical sociology engages with other approaches such as analytical philosophy, structural individualism, social stratification research, complexity science, pragmatism and critical realism, exploring the foundations of the topic as well as its major explanatory mechanisms and methods. Chapters examine the ways in which analytical sociology addresses crucial concepts, including norms, structures, context, contingency, action theory, and models of social interactions. Offering an in-depth analysis of cumulative advantage, complex contagions, and network amplification, this comprehensive Research Handbook discusses the range of data sources and methods available to analytical sociologists for empirical research, in particular digital traces, historical archives, game-theoretic models, causal inference techniques, social networks analysis, and agent-based simulations. Creating a new synthesis of the theoretical and methodological resources required to carry out research using analytical sociology tools, the Research Handbook will be a key pedagogical resource for students and scholars of sociology and sociological theory, research methods, demography, social psychology, economics and computer science"--},
	publisher = {Edward Elgar Publishing},
	editor = {Manzo, Gianluca},
	year = {2021},
}

@book{kaltheuner_fake_2021,
	title = {Fake {AI}},
	url = {https://ia801504.us.archive.org/6/items/fake-ai/Fake_AI.pdf},
	publisher = {Meatspace Press},
	author = {Kaltheuner, Frederike},
	year = {2021},
}

@article{rauchberg_deutsche_1888,
	title = {Die deutsche {Berufs}- und {Betriebszählung} vom 5. {Juni} 1882},
	volume = {14},
	journal = {Statistische Monatsschrift},
	author = {Rauchberg, Heinrich},
	year = {1888},
	pages = {569--603},
}

@article{damour_underspecification_2020,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2021-12-05},
	journal = {arXiv:2011.03395 [cs, stat]},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03395},
}

@book{reich_system_2021,
	title = {System error: where big tech went wrong and how we can reboot},
	isbn = {978-0-06-306488-1},
	shorttitle = {System error},
	abstract = {"System Error" exposes the root of our current predicament: how big tech's relentless focus on optimization is driving a future that reinforces discrimination, erodes privacy, displaces workers, and pollutes the information we get. Armed with an understanding of how technologists think and exercise their power, three Stanford professors share their provocative insights and concrete solutions to help everyone understand what is happening, what is at stake, and what we can do to control technology instead of letting it control us. A forward-thinking manifesto from three Stanford professors--experts who have worked at ground zero of the tech revolution for decades--which reveals how big tech's obsession with optimization and efficiency has sacrificed fundamental human values and outlines steps we can take to change course, renew our democracy, and save ourselves.},
	language = {English},
	author = {Reich, Rob and Sahami, Mehran and Weinstein, Jeremy M},
	year = {2021},
	note = {OCLC: 1262666927},
}

@article{arkhangelsky_synthetic_2021,
	title = {Synthetic {Difference} {In} {Differences}},
	volume = {111},
	url = {https://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.20190159},
	language = {en},
	number = {12},
	urldate = {2021-11-30},
	journal = {American Economic Review},
	author = {Arkhangelsky, Dmitry and Athey, Susan and Hirshberg, David and Imbens, Guido and Wager, Stefan},
	year = {2021},
}

@book{huber_causal_2021,
	address = {University of Fribourg},
	title = {Causal {Analysis}: {Impact} evaluation and causal machine learning with applications in {R}},
	publisher = {Preprint},
	author = {Huber, Martin},
	year = {2021},
}

@article{rauber_precisely_2021,
	title = {Precisely and {Persistently} {Identifying} and {Citing} {Arbitrary} {Subsets} of {Dynamic} {Data}},
	volume = {3},
	url = {https://hdsr.mitpress.mit.edu/pub/si7wzxxa/release/2},
	doi = {10.1162/99608f92.be565013},
	abstract = {Precisely identifying arbitrary subsets of data so that these can be reproduced is a daunting challenge in data-driven science, the more so if the underlying data source is dynamically evolving. Yet an increasing number of settings exhibit exactly those characteristics. Larger amounts of data are being continuously ingested from a range of sources (be it sensor values, online questionnaires, documents, etc.), with error correction and quality improvement processes adding to the dynamics. Yet, for studies to be reproducible, for decision-making to be transparent, and for meta studies to be performed conveniently, having a precise identification mechanism to reference, retrieve, and work with such data is essential. The Research Data Alliance (RDA) Working Group on Dynamic Data Citation has published 14 recommendations that are centered around time-stamping and versioning evolving data sources and identifying subsets dynamically via persistent identifiers that are assigned to the queries selecting the respective subsets. These principles are generic and work for virtually any kind of data. In the past few years numerous repositories around the globe have implemented these recommendations and deployed solutions. We provide an overview of the recommendations, reference implementations, and pilot systems deployed and then analyze lessons learned from these implementations. This article provides a basis for institutions and data stewards considering adding this functionality to their data systems.},
	language = {en},
	number = {4},
	urldate = {2021-11-21},
	journal = {Harvard Data Science Review},
	author = {Rauber, Andreas and Gößwein, Bernhard and Zwölf, Carlo Maria and Schubert, Chris and Wörister, Florian and Duncan, James and Flicker, Katharina and Zettsu, Koji and Meixner, Kristof and McIntosh, Leslie D. and Jenkyns, Reyna and Pröll, Stefan and Miksa, Tomasz and Parsons, Mark A.},
	month = nov,
	year = {2021},
}

@misc{cinelli_rubinangristimbens_2021,
	title = {The {Rubin}/{Angrist}/{Imbens} vs {Pearl}/{Spires}/{Schölkopf} schools of causal inference and discovery},
	author = {Cinelli, Carlos},
	month = nov,
	year = {2021},
}

@techreport{engelmann_ki_2020,
	title = {{KI} im {Behördeneinsatz}: {Erfahrungen} und {Empfehlungen}},
	language = {de},
	institution = {Kompetenzzentrum Öffentliche IT},
	author = {Engelmann, Jan and Puntschuh, Michael},
	month = dec,
	year = {2020},
	pages = {35},
}

@book{shadish_experimental_2001,
	address = {Boston},
	title = {Experimental and quasi-experimental designs for generalized causal inference},
	isbn = {978-0-395-61556-0},
	publisher = {Houghton Mifflin},
	author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald T.},
	year = {2001},
}

@article{cinelli_making_2020,
	title = {Making sense of sensitivity: extending omitted variable bias},
	volume = {82},
	issn = {1467-9868},
	shorttitle = {Making sense of sensitivity},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12348},
	doi = {10.1111/rssb.12348},
	abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as ‘extreme scenarios’. Finally, we describe problems with a common ‘benchmarking’ practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
	language = {en},
	number = {1},
	urldate = {2021-11-16},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Cinelli, Carlos and Hazlett, Chad},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12348},
	pages = {39--67},
}

@misc{noauthor_informationsmaterial_v02_2021,
	title = {Informationsmaterial\_V02 - {Kompetenznetzwerk} {Forschungsdatenmanagement} {Thüringen}},
	url = {https://forschungsdaten-thueringen.de/Informationsmaterial.html},
	urldate = {2021-11-16},
	year = {2021},
}

@techreport{fitzenberger_zeitnahe_2021,
	address = {Nuremberg},
	type = {{IAB}-{Kurzbericht}, 24/2021},
	title = {Zeitnahe {Daten} in der {Corona}-{Krise}: {Von} der schwierigen {Vermessung} der {Kurzarbeit}},
	copyright = {All rights reserved},
	url = {https://www.iab.de/194/section.aspx/Publikation/K211109MR9},
	abstract = {"Kurzarbeit wurde in der Corona-Krise in bisher nicht gekannten Umfang genutzt und hat sich als zentraler Rettungsschirm für den Arbeitsmarkt erwiesen. Entsprechend groß war das öffentliche Interesse an zeitnahen Daten, die zunächst nur über Sondererhebungen in den Betrieben zu bekommen waren. Dabei zeigte sich allerdings, dass die Zahl der Kurzarbeitenden in solchen Betriebsbefragungen deutlich überschätzt wird. Die Autoren untersuchen, warum es dazu kommt und wie groß diese Überschätzung - gemessen an der später vorliegenden amtlichen Statistik - ist." (Autorenreferat, IAB-Doku)},
	language = {de},
	institution = {Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Fitzenberger, Bernd and Kagerl, Christian and Schierholz, Malte and Stegmaier, Jens},
	month = nov,
	year = {2021},
	pages = {12},
}

@book{kadane_principles_2020,
	address = {Boca Raton},
	edition = {2},
	title = {Principles of {Uncertainty}},
	isbn = {978-1-315-16756-5},
	abstract = {Praise for the first edition: 
Principles of Uncertainty is a profound and mesmerising book on the foundations and principles of subjectivist or behaviouristic Bayesian analysis. … the book is a pleasure to read. And highly recommended for teaching as it can be used at many different levels. … A must-read for sure!—Christian Robert, CHANCE
It's a lovely book, one that I hope will be widely adopted as a course textbook.—Michael Jordan, University of California, Berkeley, USA
Like the prize-winning first edition, Principles of Uncertainty, Second Edition is an accessible, comprehensive text on the theory of Bayesian Statistics written in an appealing, inviting style, and packed with interesting examples. It presents an introduction to the subjective Bayesian approach which has played a pivotal role in game theory, economics, and the recent boom in Markov Chain Monte Carlo methods. This new edition has been updated throughout and features new material on Nonparametric Bayesian Methods, the Dirichlet distribution, a simple proof of the central limit theorem, and new problems.
Key Features: 


First edition won the 2011 DeGroot Prize


Well-written introduction to theory of Bayesian statistics


Each of the introductory chapters begins by introducing one new concept or assumption


Uses "just-in-time mathematics"—the introduction to mathematical ideas just before they are applied},
	publisher = {Chapman and Hall/CRC},
	author = {Kadane, Joseph B.},
	month = aug,
	year = {2020},
	doi = {10.1201/9781315167565},
}

@book{kadane_pragmatics_2016,
	address = {New York},
	title = {Pragmatics of {Uncertainty}},
	isbn = {978-1-315-37096-5},
	abstract = {A fair question to ask of an advocate of subjective Bayesianism (which the author is) is "how would you model uncertainty?" In this book, the author writes about how he has done it using real problems from the past, and offers additional comments about the context in which he was working.},
	publisher = {Chapman and Hall/CRC},
	author = {Kadane, Joseph B.},
	month = oct,
	year = {2016},
	doi = {10.1201/9781315370965},
}

@article{ludwig_fragile_2021,
	title = {Fragile {Algorithms} and {Fallible} {Decision}-{Makers}: {Lessons} from the {Justice} {System}},
	volume = {35},
	issn = {0895-3309},
	shorttitle = {Fragile {Algorithms} and {Fallible} {Decision}-{Makers}},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.35.4.71},
	doi = {10.1257/jep.35.4.71},
	abstract = {Algorithms (in some form) are already widely used in the criminal justice system. We draw lessons from this experience for what is to come for the rest of society as machine learning diffuses. We find economists and other social scientists have a key role to play in shaping the impact of algorithms, in part through improving the tools used to build them.},
	language = {en},
	number = {4},
	urldate = {2021-11-10},
	journal = {Journal of Economic Perspectives},
	author = {Ludwig, Jens and Mullainathan, Sendhil},
	month = nov,
	year = {2021},
	pages = {71--96},
}

@book{engel_handbook_2021,
	address = {S.l.},
	edition = {1st edition},
	title = {Handbook of {Computational} {Social} {Science} - {Vol} 1 \& {Vol} 2},
	isbn = {978-1-03-211143-8},
	language = {English},
	publisher = {Routledge},
	editor = {Engel, Uwe and Quan-Haase, Anabel and Liu, Xun and Lyberg, Lars},
	month = sep,
	year = {2021},
}

@techreport{keusch_mobile_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Mobile {Web} {Surveys} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Keusch, Florian},
	month = aug,
	year = {2015},
}

@techreport{behr_questionnaire_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Questionnaire {Translation} in {Cross}-{Cultural} {Surveys}: {From} {Designing} {Source} {Questionnaires} to {Organizing} {Translation} {Projects} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Behr, Dorothée},
	month = aug,
	year = {2015},
}

@techreport{eifler_research_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Research {Designs} and {Causal} {Inference} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Eifler, Stefanie and Leitgöb, Heinz},
	month = aug,
	year = {2015},
}

@book{gorard_research_2013,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Research {Design}: {Creating} {Robust} {Approaches} for the {Social} {Sciences}},
	isbn = {978-1-4462-4902-4 978-1-5264-3148-6},
	shorttitle = {Research {Design}},
	url = {http://methods.sagepub.com/book/research-design-creating-robust-approaches-for-the-social-sciences},
	urldate = {2021-11-06},
	publisher = {SAGE Publications, Inc.},
	author = {Gorard, Stephen},
	year = {2013},
	doi = {10.4135/9781526431486},
}

@techreport{netscher_introduction_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Introduction to {Research} {Data} {Management} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Netscher, Sebastian and Katsanidou, Alexia and Recker, Astrid and Trixa, Jessica},
	month = aug,
	year = {2015},
}

@techreport{sand_samling_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Samling, {Weighting} and {Estimation} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Sand, Matthias and Zins, Stefan},
	month = aug,
	year = {2015},
}

@techreport{cieciuch_testing_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Testing survey data for measurement equivalence across countries and time [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Cieciuch, Jan and Davidov, Eldad and Schmidt, Peter},
	month = aug,
	year = {2015},
}

@techreport{toepoel_mixed-mode_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Mixed-mode and mixed-device surveys [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Toepoel, Vera and de Leeuw, Edith and Klausch, Thomas},
	month = aug,
	year = {2015},
}

@techreport{feskens_planning_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Planning and {Monitoring} {Survey} {Fieldwork} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Feskens, Remco and Kappelhof, Joost},
	month = aug,
	year = {2015},
}

@techreport{lynn_design_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Design and {Implementation} of {Longitudinal} {Surveys} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Lynn, Peter and Al Baghal, Tarek},
	month = aug,
	year = {2015},
}

@techreport{fuchs_questionnaire_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Questionnaire {Design} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Fuchs, Marek and Kunz, Tanja and Metzler, Anke},
	month = aug,
	year = {2015},
}

@techreport{rassler_item_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Item {Nonresponse} and {Multiple} {Imputation} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Rässler, Susanne and Meinfelder, Florian},
	month = aug,
	year = {2015},
}

@techreport{billiet_understanding_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Understanding and {Modelling} {Measurement} {Error} in {Social} {Surveys} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Billiet, Jaak and Revilla, Melanie},
	month = aug,
	year = {2015},
}

@techreport{manfreda_introduction_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Introduction to {Web} {Surveys} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Manfreda, Katja Lozar and Berzelak, Nejc},
	month = aug,
	year = {2015},
}

@techreport{menold_development_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Development, {Evaluation} and {Quality} {Assurance} of {Measurement} {Instruments} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Menold, Natalja and Beierlein, Constanze},
	month = aug,
	year = {2015},
}

@techreport{steinhauer_introduction_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Introduction to {Data} {Analysis} {Using} {R} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Steinhauer, Hans Walter and Würbach, Ariane},
	month = aug,
	year = {2015},
}

@techreport{reinecke_introduction_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Introduction to {Structural} {Equation} {Modeling}: {Confirmatory} {Factor} {Analysis} with {Mplus} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Reinecke, Jost and Kessler, Georg},
	month = aug,
	year = {2015},
}

@techreport{blom_introduction_2015,
	address = {Cologne},
	type = {Course syllabus},
	title = {Introduction to {Survey} {Design} [{Gesis} {Summer} {School}]},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Blom, Annelies and Lugtig, Peter},
	month = aug,
	year = {2015},
}

@article{simon_prognosen_2012,
	title = {Prognosen zum {Thema} “{Fachkräftemangel} in der {Pflege}“: {Limitationen} amtlicher {Statistiken} und methodische {Probleme} bisheriger {Studien}},
	volume = {61},
	shorttitle = {Prognosen zum {Thema} “{Fachkräftemangel} in der {Pflege}“},
	url = {http://elibrary.duncker-humblot.com/zeitschriften/id/21/vol/61/iss/1212/art/3994/},
	abstract = {Duncker \& Humblot - Berlin. Ein Forum für die Wissenschaft seit 1798},
	language = {de},
	number = {2-3},
	urldate = {2021-11-05},
	journal = {Sozialer Fortschritt},
	author = {Simon, Michael},
	month = feb,
	year = {2012},
	pages = {25--39},
}

@article{matthes_collecting_2014,
	title = {Collecting information on job tasks—an instrument to measure tasks required at the workplace in a multi-topic survey},
	volume = {47},
	issn = {1614-3485, 1867-8343},
	url = {http://link.springer.com/10.1007/s12651-014-0155-4},
	doi = {10.1007/s12651-014-0155-4},
	abstract = {The analysis of job tasks has become a ﬁeld of growing scientiﬁc activity in recent years. Information on such tasks has been used to analyze various research questions, especially regarding changes in the overall structure of the economy and their implications for persons and ﬁrms. Arguably the most prominent of these research questions is the analysis of the consequences of technological change for job tasks, skill demand, and wage inequality.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Journal for Labour Market Research},
	author = {Matthes, Britta and Christoph, Bernhard and Janik, Florian and Ruland, Michael},
	month = dec,
	year = {2014},
	pages = {273--297},
}

@article{ganzeboom_standard_1992,
	title = {A standard international socio-economic index of occupational status},
	volume = {21},
	issn = {0049-089X},
	url = {https://www.sciencedirect.com/science/article/pii/0049089X9290017B},
	doi = {10.1016/0049-089X(92)90017-B},
	abstract = {In this paper we present an International Socio-Economic Index of occupational status (ISEI), derived from the International Standard Classification of Occupations (ISCO), using comparably coded data on education, occupation, and income for 73,901 full-time employed men from 16 countries. We use an optimal scaling procedure, assigning scores to each of 271 distinct occupation categories in such a way as to maximize the role of occupation as an intervening variable between education and income (in contrast to taking prestige as the criterion for weighting education and income, as in the Duncan scale). We compare the resulting scale to two existing internationally standardized measures of occupational status, Treiman's international prestige scale (SIOPS) and Goldthorpe's class categories (EGP), and also with several locally developed SEI scales. The performance of the new ISEI scale compares favorably with these alternatives, both for the data sets used to construct the scale and for five additional data sets.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Social Science Research},
	author = {Ganzeboom, Harry B. G. and De Graaf, Paul M. and Treiman, Donald J.},
	month = mar,
	year = {1992},
	pages = {1--56},
}

@article{singer_methodological_2017,
	title = {Some {Methodological} {Uses} of {Responses} to {Open} {Questions} and {Other} {Verbatim} {Comments} in {Quantitative} {Surveys}},
	volume = {data},
	copyright = {This work is licensed under a Creative Commons Attribution 4.0 International License.},
	url = {http://mda.gesis.org/index.php/mda/article/view/2017.01},
	doi = {10.12758/MDA.2017.01},
	abstract = {The use of open-ended questions in survey research has a very long history. In this paper, building on the work of Paul F. Lazarsfeld and Howard Schuman, we review the methodological uses of open-ended questions and verbatim responses in surveys. We draw on prior research, our own and that of others, to argue for increasing the use of open-ended questions in quantitative surveys. The addition of open-ended questions – and the capture and analysis of respondents’ verbatim responses to other types of questions – may yield important insights, not only into respondents’ substantive answers, but also into how they understand the questions we ask and arrive at an answer. Adding a limited number of such questions to computerized surveys, whether self- or interviewer-administered, is neither expensive nor time-consuming, and in our experience respondents are quite willing and able to answer such questions.},
	language = {en},
	urldate = {2021-11-05},
	journal = {methods},
	author = {Singer, Eleanor and Couper, Mick P.},
	month = jul,
	year = {2017},
	note = {Artwork Size: 20 Pages
Publisher: methods, data, analyses},
	pages = {20 Pages},
}

@inproceedings{esuli_training_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Training {Data} {Cleaning} for {Text} {Classification}},
	isbn = {978-3-642-04417-5},
	doi = {10.1007/978-3-642-04417-5_4},
	abstract = {In text classification (TC) and other tasks involving supervised learning, labelled data may be scarce or expensive to obtain; strategies are thus needed for maximizing the effectiveness of the resulting classifiers while minimizing the required amount of training effort. Training data cleaning (TDC) consists in devising ranking functions that sort the original training examples in terms of how likely it is that the human annotator has misclassified them, thereby providing a convenient means for the human annotator to revise the training set so as to improve its quality. Working in the context of boosting-based learning methods we present three different techniques for performing TDC and, on two widely used TC benchmarks, evaluate them by their capability of spotting misclassified texts purposefully inserted in the training set.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval} {Theory}},
	publisher = {Springer},
	author = {Esuli, Andrea and Sebastiani, Fabrizio},
	editor = {Azzopardi, Leif and Kazai, Gabriella and Robertson, Stephen and Rüger, Stefan and Shokouhi, Milad and Song, Dawei and Yilmaz, Emine},
	year = {2009},
	pages = {29--41},
}

@inproceedings{esuli_active_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Active {Learning} {Strategies} for {Multi}-{Label} {Text} {Classification}},
	isbn = {978-3-642-00958-7},
	doi = {10.1007/978-3-642-00958-7_12},
	abstract = {Active learning refers to the task of devising a ranking function that, given a classifier trained from relatively few training examples, ranks a set of additional unlabeled examples in terms of how much further information they would carry, once manually labeled, for retraining a (hopefully) better classifier. Research on active learning in text classification has so far concentrated on single-label classification; active learning for multi-label classification, instead, has either been tackled in a simulated (and, we contend, non-realistic) way, or neglected tout court. In this paper we aim to fill this gap by examining a number of realistic strategies for tackling active learning for multi-label classification. Each such strategy consists of a rule for combining the outputs returned by the individual binary classifiers as a result of classifying a given unlabeled document. We present the results of extensive experiments in which we test these strategies on two standard text classification datasets.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer},
	author = {Esuli, Andrea and Sebastiani, Fabrizio},
	editor = {Boughanem, Mohand and Berrut, Catherine and Mothe, Josiane and Soule-Dupuy, Chantal},
	year = {2009},
	pages = {102--113},
}

@article{esuli_machines_2010,
	title = {Machines that {Learn} how to {Code} {Open}-{Ended} {Survey} {Data}},
	volume = {52},
	issn = {1470-7853, 2515-2173},
	url = {http://journals.sagepub.com/doi/10.2501/S147078531020165X},
	doi = {10.2501/S147078531020165X},
	language = {en},
	number = {6},
	urldate = {2021-11-05},
	journal = {International Journal of Market Research},
	author = {Esuli, Andrea and Sebastiani, Fabrizio},
	month = nov,
	year = {2010},
	pages = {775--800},
}

@inproceedings{patil_surveycoder_2013,
	title = {{SurveyCoder}: {A} {System} for {Classification} of {Survey} {Responses}},
	shorttitle = {{SurveyCoder}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-38824-8_52},
	doi = {10.1007/978-3-642-38824-8_52},
	abstract = {Survey coding is the process of analyzing text responses to open-ended questions in surveys. We present SurveyCoder, a research prototype which helps the survey analysts to achieve significant...},
	language = {en},
	urldate = {2021-11-05},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Patil, Sangameshwar and Palshikar, Girish K.},
	month = jun,
	year = {2013},
	pages = {417--420},
}

@techreport{hacking_coding_2012,
	address = {The Hague/Heerlen},
	title = {Coding; interpreting short descriptions using a classification},
	institution = {Statistics Netherlands},
	author = {Hacking, Wim and Willenborg, Leon},
	year = {2012},
	pages = {59},
}

@inproceedings{zelenak_forced-choice_2008,
	address = {New Orleans},
	title = {Forced-{Choice} {Versus} {Open}-{Ended} {Versions} of the {Field} of {Bachelor}'s {Degree} {Question} in the 2007 {ACS} {Content} {Test}},
	url = {http://www.asasrms.org/Proceedings/y2008f.html},
	booktitle = {Proceedings of {AAPOR} 2008},
	author = {Zelenak, Mary Frances and Raglin, David and Davis, Mary and Tancreto, Jennifer},
	month = may,
	year = {2008},
}

@article{reja_open-ended_2003,
	title = {Open-ended vs. {Close}-ended {Questions} in {Web} {Questionnaires}},
	volume = {19},
	url = {http://mrvar.fdv.uni-lj.si/pub/mz/mz19/cont19.htm},
	abstract = {Two quite different reasons for using open-ended as opposed to closeended questions can be distinguished. One is to discover the responses that individuals give spontaneously; the other is to avoid the bias that may result from suggesting responses to individuals. However, open-ended questions also have disadvantages in comparison to close-ended, such as the need for extensive coding and larger item non-response. While this issue has already been well researched for traditional survey questionnaires, not much research has been devoted to it in recently used Web questionnaires. We therefore examine the differences between the open-ended and the closeended question form in Web questionnaires by means of experiments within the large-scale RIS 2001 Web survey. The question “What is the most important, critical problem the Internet is facing today?” was asked in an open-ended and two close-ended question forms in a split-ballot experiment. The results show that there were differences between question forms in univariate distributions, though no significant differences were found in the ranking of values. Close-ended questions in general yield higher percentages than open-ended question for answers that are identical in both question forms. It seems that respondents restricted themselves with apparent ease to the alternatives offered on the close-ended forms, whereas on the open-ended question they produced a much more diverse set of answers. In addition, our results suggest that open-ended questions produce more missing data than close-ended. Moreover, there were more inadequate answers for open-ended question. This suggests that open-ended questions should be more explicit in their wording (at least for Web surveys, as a self administered mode of data collection) than close-ended questions, which are more specified with given response alternatives.},
	language = {en},
	journal = {Advances in methodology and statistics (Metodološki zvezki)},
	author = {Reja, Urša and Lozar Manfreda, Katja and Hlebec, Valentina and Vehovar, Vasja},
	year = {2003},
	pages = {159--177},
}

@article{yamanishi_mining_2002,
	title = {Mining open answers in questionnaire data},
	volume = {17},
	issn = {1941-1294},
	doi = {10.1109/MIS.2002.1039833},
	abstract = {Surveys are important tools for marketing and for managing customer relationships; the answers to open-ended questions, in particular, often contain valuable information and provide an important basis for business decisions. The summaries that human analysts make of these open answers, however, tend to rely too much on intuition and so aren't satisfactorily reliable. Moreover, because the Web makes it so easy to take surveys and solicit comments, companies are finding themselves inundated with data from questionnaires and other sources. Handling it all manually would be not only cumbersome but also costly. Thus, devising a computer system that can automatically mine useful information from open answers has become an important issue. We have developed a survey analysis system that works on these principles. The system mines open answers through two statistical learning techniques: rule learning (which we call rule analysis) and correspondence analysis.},
	number = {5},
	journal = {IEEE Intelligent Systems},
	author = {Yamanishi, K. and Li, Hang},
	month = sep,
	year = {2002},
	note = {Conference Name: IEEE Intelligent Systems},
	pages = {58--63},
}

@inproceedings{emde_using_2012,
	address = {Orlando},
	title = {Using {Adaptive} {Questionnaire} {Design} in {Open}-{Ended} {Questions}: {A} {Field} {Experiment}},
	url = {http://www.asasrms.org/Proceedings/y2012f.html},
	booktitle = {Proceedings of {AAPOR} 2012},
	author = {Emde, Matthias and Fuchs, Marek},
	month = may,
	year = {2012},
}

@techreport{krosnick_problems_2008,
	title = {Problems with {ANES} {Questions} {Measuring} {Political} {Knowledge}},
	author = {Krosnick, Jon A. and Lupia, Arthur and DeBell, Matthew and Donakowski, Darrell},
	year = {2008},
}

@article{jackson_concept_2002,
	title = {Concept {Mapping} as an {Alternative} {Approach} for the {Analysis} of {Open}-{Ended} {Survey} {Responses}},
	volume = {5},
	issn = {1094-4281, 1552-7425},
	url = {http://journals.sagepub.com/doi/10.1177/109442802237114},
	doi = {10.1177/109442802237114},
	abstract = {This article presents concept mapping as an alternative method to existing code-based and word-based text analysis techniques for one type of qualitative text data—open-ended survey questions. It is argued that the concept mapping method offers a unique blending of the strengths of these approaches while minimizing some of their weaknesses. This method appears to be especially well suited for the type of text generated by open-ended questions as well for organizational research questions that are exploratory in nature, aimed at scale or interview question development, and/or developing conceptual coding schemes. A detailed example of concept mapping on open-ended survey data is presented. Reliability and validity issues associated with concept mapping are also discussed.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Organizational Research Methods},
	author = {Jackson, Kristin M. and Trochim, William M. K.},
	month = oct,
	year = {2002},
	pages = {307--336},
}

@article{fine_ethnography_2000,
	title = {Ethnography and {Experiment} in {Social} {Psychological} {Theory} {Building}: {Tactics} for {Integrating} {Qualitative} {Field} {Data} with {Quantitative} {Lab} {Data}},
	volume = {36},
	issn = {0022-1031},
	shorttitle = {Ethnography and {Experiment} in {Social} {Psychological} {Theory} {Building}},
	url = {https://www.sciencedirect.com/science/article/pii/S002210319991394X},
	doi = {10.1006/jesp.1999.1394},
	abstract = {While both sociologists and organizational theorists have incorporated qualitative data into theory building, contemporary social psychologists have resisted this trend. This resistance may be the product of long-standing perceptions of the discipline of social psychology that equate it with controlled experimentation. Yet, this was not always the case. Many respected social psychologists, including Muzafer Sherif, Edgar Schein, and Leon Festinger, relied on qualitative data from real-world contexts to ground theory building. Following their example, we discuss the possibilities of reviving social psychological approaches to theory building that integrate qualitative field data with quantitative data collected in laboratory experiments. We first justify why qualitative data are important to social psychological theory building by examining some of the strengths and weaknesses that have been demonstrated in other research domains. We then use several “classic” social psychological studies to illustrate specific tactics for integrating qualitative data with traditional experimental data in social psychological research. These examples demonstrate the flexibility and synergies of combining qualitative and quantitative data. They also suggest that social psychological theory building may benefit from a “return to our roots” and an embrace of qualitative data.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Journal of Experimental Social Psychology},
	author = {Fine, Gary Alan and Elsbach, Kimberly D.},
	month = jan,
	year = {2000},
	pages = {51--76},
}

@article{fielding_opening_2013,
	title = {Opening up open-ended survey data using qualitative software},
	volume = {47},
	issn = {0033-5177, 1573-7845},
	url = {http://link.springer.com/10.1007/s11135-012-9716-1},
	doi = {10.1007/s11135-012-9716-1},
	abstract = {This article considers the contribution that qualitative software can make to ‘opening up’ open-ended question (‘OEQ’) data from surveys. While integrating OEQ data with the analysis of ﬁxed response items is a challenge, it is also an endeavour for which qualitative software offers considerable support. For survey researchers who wish to derive more analytic value from OEQ data, qualitative software can be a useful resource. We proﬁle the systematic use of qualitative software for such purposes, and the procedures and practical considerations involved. The discussion is illustrated by examples derived from a survey dataset relating to environmental risk in the UK.},
	language = {en},
	number = {6},
	urldate = {2021-11-05},
	journal = {Quality \& Quantity},
	author = {Fielding, Jane and Fielding, Nigel and Hughes, Graham},
	month = oct,
	year = {2013},
	pages = {3261--3276},
}

@article{debell_harder_2013,
	title = {Harder {Than} {It} {Looks}: {Coding} {Political} {Knowledge} on the {ANES}},
	volume = {21},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Harder {Than} {It} {Looks}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700013486/type/journal_article},
	doi = {10.1093/pan/mpt010},
	abstract = {Political knowledge research faces a problem, perhaps even a crisis. For two decades, the American National Election Studies asked open-ended questions about political knowledge and coded answers using procedures that are neither reliable nor replicable and that were never shown to be optimally valid. Consequently, conclusions based on these widely used measures of the public's competence are in doubt. This article presents several new and overdue methodological improvements: coding knowledge data using formal and specific coding rules based on a substantive rationale for the validity of the codes, recognizing partially correct answers, using multiple coders working independently, using machine coding, and testing reliability and validity. The new methods are an improvement because they are transparent and replicable and they produce valid and extremely reliable knowledge data. Further, machine coding produces codes nearly identical to those from a team of human coders, at much lower cost.},
	language = {en},
	number = {4},
	urldate = {2021-11-05},
	journal = {Political Analysis},
	author = {DeBell, Matthew},
	year = {2013},
	pages = {393--406},
}

@article{schuman_perceived_1986,
	title = {The {Perceived} {Threat} of {Nuclear} {War}, {Salience}, and {Open} {Questions}},
	volume = {50},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/2748756},
	abstract = {The most serious problem facing the United States, according to many scientific and political leaders, is the threat of nuclear war. Yet the standard survey question on the most important problem facing the country has often shown little public concurrence with this assumption. Our article uses experimentation in national samples to test whether this difference can be traced to limitations in either the form or the wording of the standard question. The results indicate that there are some important systematic differences between open and closed versions of the question, and also differences that result from reference to the nation as distinct from the world, but neither type of difference accounts for the infrequent mention of nuclear war on the standard question. Instead, other evidence indicates that most American believe that nuclear war is not going to happen at all, or that if it does happen it will be too far in the distant future to be of pressing concern to them personally.},
	number = {4},
	urldate = {2021-11-05},
	journal = {The Public Opinion Quarterly},
	author = {Schuman, Howard and Ludwig, Jacob and Krosnick, Jon A.},
	year = {1986},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {519--536},
}

@inproceedings{bela_structured_2017,
	title = {Structured {Derivation} of {Variables} from {Occupational} {Classifications} using {Stata}. {The} {Stata} package derivescores.},
	copyright = {Creative Commons Attribution Share Alike 4.0 International, Open Access},
	url = {https://zenodo.org/record/827308},
	doi = {10.5281/ZENODO.827308},
	abstract = {Perhaps the most used source for deriving prestige and status scores (e.g. ISEI, SIOPS, EGP) from occupational classifications are Harry Ganzeboom’s SPSS codes (Ganzeboom 2016), which he publishes on his website. The well-known Stata modules by John Hendrickx (2002, 2004) adapt these scripts in Stata.{\textless}br{\textgreater} Albeit these scripts being the most sophisticated mechanism publicly available to calculate prestige and status scores, the approach via syntax codes has two main shortcomings: (1) the necessarily complex architecture of those scripts makes it hard for users to fully comprehend the derivation process in its details; (2) currently, code for deriving prestige and status score variables from the latest ISCO-08 is available for SPSS only. Packages for other statistical platforms, such as R or SAS, also are only partially available.{\textless}br{\textgreater} We present a Stata module which tries to overcome these issues. By creating a framework that establishes all variables’ derivation via lookup tables, the whole process becomes more flexible. This approach can produce the same results as the established way of using Harry Ganzeboom’s SPSS or John Hendrickx Stata codes, when used with the appropriate lookup tables delivered within the package. However, several benefits emerge from the concept of lookup tables: The end user can easily understand why a certain code led to a certain status score by having a detailed look at the tables. Additionally, he can customize the whole process to his needs by using self-administered lookup tables, either by only slightly adapting the original tables, or by interchanging the lookup information with completely different tables.{\textless}br{\textgreater} Using the latter path, the full flexibility of the lookup table approach leads to considerable advantages. Not only does it enable the user to derive other than the “standard” prestige or status scores, like the Magnitude Prestige Scale (MPS) or Blossfeld’s occupational classification (BLK). Our approach also gives the possibility to run the derivation process from totally different input, from national classifications (like the German KldB) up to bare answers from surveys. It also makes it possible to create cross-walk tables between classifications. Thereby, error checking and reporting, as well as the possibility of “inline documentation” of the derivation process (by citation of the lookup tables) and (multilingual) labeling of values, is handled by the Stata procedure in a structured, standard way. User defined lookup tables can, when properly documented, be submitted to the authors and become part of the package and its documentation.{\textless}br{\textgreater} Finally, our approach of using lookup tables for variable derivation can be ported to other statistical software platforms, like SPSS, R or SAS. All tables are convertible between platforms, so that only the program logic of the derivation process has to be translated into the specific platform’s language. This can eventually lead to a default way of deriving occupational scores from classifications across different software platforms and harmonization of these variables between surveys producing research data, such as NEPS, SOEP or PASS.},
	urldate = {2021-11-05},
	author = {Bela, Daniel and Wenzig, Knut},
	month = jul,
	year = {2017},
	note = {Publisher: Zenodo},
}

@misc{hartmann_occupational_2017,
	address = {Lisbon},
	title = {Occupational classifications in {Germany}. {Same} but different?},
	author = {Hartmann, Florian},
	month = jul,
	year = {2017},
}

@misc{zielonka_computer_2017,
	address = {Lisbon},
	title = {Computer {Assisted} {Manual} {Coding} of {Occupations}: {Best} {Practice} and {First} {Results} on {Reliability} and {Productivity}},
	author = {Zielonka, Markus and Czerner, Gregor},
	month = jul,
	year = {2017},
}

@techreport{paulus_klassifikation_2013,
	address = {Nuremberg},
	type = {{FDZ}-{Methodenreport} 08/2013},
	title = {Klassifikation der {Berufe} * {Struktur}, {Codierung} und {Umsteigeschlüssel}},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Paulus, Wiebke and Matthes, Britta},
	month = aug,
	year = {2013},
}

@techreport{drasch_arbeiten_2012,
	address = {Nuremberg},
	type = {{FDZ}-{Methodenreport} 04/2012},
	title = {Arbeiten und {Lernen} im {Wandel} * {Teil} {V}: {Die} {Codierung} der offenen {Angaben} zur beruflichen {Tätigkeit}, {Ausbildung} und {Branche}},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Drasch, Katrin and Matthes, Britta and Munz, Manuel and Paulus, Wiebke and Valentin, Margot-Anna},
	year = {2012},
}

@book{tutz_regression_2011,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Regression for {Categorical} {Data}},
	isbn = {978-1-107-00965-3},
	url = {https://www.cambridge.org/core/books/regression-for-categorical-data/B71F71F2A484E2DF88256C8DF004108C},
	abstract = {This book introduces basic and advanced concepts of categorical regression with a focus on the structuring constituents of regression, including regularization techniques to structure predictors. In addition to standard methods such as the logit and probit model and extensions to multivariate settings, the author presents more recent developments in flexible and high-dimensional regression, which allow weakening of assumptions on the structuring of the predictor and yield fits that are closer to the data. A generalized linear model is used as a unifying framework whenever possible in particular parametric models that are treated within this framework. Many topics not normally included in books on categorical data analysis are treated here, such as nonparametric regression; selection of predictors by regularized estimation procedures; ternative models like the hurdle model and zero-inflated regression models for count data; and non-standard tree-based ensemble methods. The book is accompanied by an R package that contains data sets and code for all the examples.},
	urldate = {2021-11-05},
	publisher = {Cambridge University Press},
	author = {Tutz, Gerhard},
	year = {2011},
	doi = {10.1017/CBO9780511842061},
}

@techreport{held_bayesian_1993,
	address = {Madison},
	type = {Technical {Report} {No}. 900},
	title = {The {Bayesian} analysis of categorical data - {A} selective review},
	language = {en},
	number = {900},
	institution = {University of Wisconsin},
	author = {Held, Leonhard and Hsu, John},
	month = jan,
	year = {1993},
}

@article{raftery_statistics_2001,
	title = {Statistics {In} {Sociology}, 1950–2000: {A} {Selective} {Review}},
	volume = {31},
	issn = {1467-9531},
	shorttitle = {Statistics {In} {Sociology}, 1950–2000},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/0081-1750.00088},
	doi = {10.1111/0081-1750.00088},
	abstract = {Statistical methods have had a successful half-century in sociology, contributing to a greatly improved standard of scientific rigor in the discipline. I identify three overlapping postwar generations of statistical methods in sociology, based on the kinds of data they address. The first generation, which started in the late 1940s, deals with cross-tabulations and focuses on measures of association and log-linear models, perhaps the area of statistics to which sociology has contributed the most. The second generation, which began in the 1960s, deals with unit-level survey data and focuses on LISREL-type causal models and event-history analysis. The third generation, starting to emerge in the late 1980s, deals with data that do not fall easily into either of these categories, either because they have a different form, such as texts or narratives, or because dependence is a crucial aspect, as with spatial or social network data. There are many new challenges, and the area is ripe for statistical research; several major institutions have recently launched new initiatives in statistics and the social sciences.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Sociological Methodology},
	author = {Raftery, Adrian E.},
	year = {2001},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/0081-1750.00088},
	pages = {1--45},
}

@article{dickey_bayesian_2008,
	title = {Bayesian methods for categorical data under informative censoring},
	volume = {3},
	issn = {1936-0975},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3/Bayesian-methods-for-categorical-data-under-informative-censoring/10.1214/08-BA321.full},
	doi = {10.1214/08-BA321},
	number = {3},
	urldate = {2021-11-05},
	journal = {Bayesian Analysis},
	author = {Dickey, James M. and Jiang, Thomas J.},
	month = sep,
	year = {2008},
}

@techreport{tutz_formelsammlung_2012,
	address = {Munich},
	type = {Formelsammlung},
	title = {Formelsammlung zur {Vorlesung} {Kategoriale} {Daten} 2012/13},
	institution = {LMU Munich},
	author = {Tutz, Gerhard and Heinzl, Felix},
	year = {2012},
}

@article{ferguson_bayesian_1973,
	title = {A {Bayesian} {Analysis} of {Some} {Nonparametric} {Problems}},
	volume = {1},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/A-Bayesian-Analysis-of-Some-Nonparametric-Problems/10.1214/aos/1176342360.full},
	doi = {10.1214/aos/1176342360},
	abstract = {The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let \${\textbackslash}mathscr\{X\}\$ be a space and \${\textbackslash}mathscr\{A\}\$ a \${\textbackslash}sigma\$-field of subsets, and let \${\textbackslash}alpha\$ be a finite non-null measure on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$. Then a stochastic process \$P\$ indexed by elements \$A\$ of \${\textbackslash}mathscr\{A\}\$, is said to be a Dirichlet process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with parameter \${\textbackslash}alpha\$ if for any measurable partition \$(A\_1, {\textbackslash}cdots, A\_k)\$ of \${\textbackslash}mathscr\{X\}\$, the random vector \$(P(A\_1), {\textbackslash}cdots, P(A\_k))\$ has a Dirichlet distribution with parameter \$({\textbackslash}alpha(A\_1), {\textbackslash}cdots, {\textbackslash}alpha(A\_k)). P\$ may be considered a random probability measure on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$, The main theorem states that if \$P\$ is a Dirichlet process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with parameter \${\textbackslash}alpha\$, and if \$X\_1, {\textbackslash}cdots, X\_n\$ is a sample from \$P\$, then the posterior distribution of \$P\$ given \$X\_1, {\textbackslash}cdots, X\_n\$ is also a Dirichlet process on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$ with a parameter \${\textbackslash}alpha + {\textbackslash}sum{\textasciicircum}n\_1 {\textbackslash}delta\_\{x\_i\}\$, where \${\textbackslash}delta\_x\$ denotes the measure giving mass one to the point \$x\$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on \$({\textbackslash}mathscr\{X\}, {\textbackslash}mathscr\{A\})\$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space \${\textbackslash}mathscr\{X\}\$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where \${\textbackslash}mathscr\{X\}\$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general \${\textbackslash}mathscr\{X\}\$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that \$P\$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a \$P\$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis \$H\_0\$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter \${\textbackslash}alpha\$ itself a uniform measure on [0, 1], and if we are given a sample of size \$n {\textbackslash}geqq 2\$, the only nontrivial nonrandomized Bayes rule is to reject \$H\_0\$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.},
	number = {2},
	urldate = {2021-11-05},
	journal = {The Annals of Statistics},
	author = {Ferguson, Thomas S.},
	month = mar,
	year = {1973},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {209--230},
}

@article{agresti_bayesian_2005,
	title = {Bayesian inference for categorical data analysis},
	volume = {14},
	issn = {1618-2510, 1613-981X},
	url = {http://link.springer.com/10.1007/s10260-005-0121-y},
	doi = {10.1007/s10260-005-0121-y},
	language = {en},
	number = {3},
	urldate = {2021-11-05},
	journal = {Statistical Methods and Applications},
	author = {Agresti, Alan and Hitchcock, David B.},
	month = dec,
	year = {2005},
	pages = {297--330},
}

@article{albert_empirical_1987,
	title = {Empirical bayes estimation in contingency tables},
	volume = {16},
	issn = {0361-0926, 1532-415X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03610928708829518},
	doi = {10.1080/03610928708829518},
	language = {en},
	number = {8},
	urldate = {2021-11-05},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Albert, James H.},
	month = jan,
	year = {1987},
	pages = {2459--2485},
}

@inproceedings{xue_deep_2008,
	address = {New York, NY, USA},
	series = {{SIGIR} '08},
	title = {Deep classification in large-scale text hierarchies},
	isbn = {978-1-60558-164-4},
	url = {https://doi.org/10.1145/1390334.1390440},
	doi = {10.1145/1390334.1390440},
	abstract = {Most classification algorithms are best at categorizing the Web documents into a few categories, such as the top two levels in the Open Directory Project. Such a classification method does not give very detailed topic-related class information for the user because the first two levels are often too coarse. However, classification on a large-scale hierarchy is known to be intractable for many target categories with cross-link relationships among them. In this paper, we propose a novel deep-classification approach to categorize Web documents into categories in a large-scale taxonomy. The approach consists of two stages: a search stage and a classification stage. In the first stage, a category-search algorithm is used to acquire the category candidates for a given document. Based on the category candidates, we prune the large-scale hierarchy to focus our classification effort on a small subset of the original hierarchy. As a result, the classification model is trained on the small subset before being applied to assign the category for a new document. Since the category candidates are sufficiently close to each other in the hierarchy, a statistical-language-model based classifier using n-gram features is exploited. Furthermore, the structure of the taxonomy can be utilized in this stage to improve the performance of classification. We demonstrate the performance of our proposed algorithms on the Open Directory Project with over 130,000 categories. Experimental results show that our proposed approach can reach 51.8\% on the measure of Mi-F1 at the 5th level, which is 77.7\% improvement over top-down based SVM classification algorithms.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 31st ann. int. {ACM} {SIGIR} conference information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Xue, Gui-Rong and Xing, Dikan and Yang, Qiang and Yu, Yong},
	month = jul,
	year = {2008},
	pages = {619--626},
}

@inproceedings{koller_hierarchically_1997,
	address = {San Francisco, CA, USA},
	series = {{ICML} '97},
	title = {Hierarchically {Classifying} {Documents} {Using} {Very} {Few} {Words}},
	isbn = {978-1-55860-486-5},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 14th {Int}. {Conf}. on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Koller, Daphne and Sahami, Mehran},
	month = jul,
	year = {1997},
	pages = {170--178},
}

@article{silla_survey_2011,
	title = {A survey of hierarchical classification across different application domains},
	volume = {22},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-010-0175-9},
	doi = {10.1007/s10618-010-0175-9},
	abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Data Mining and Knowledge Discovery},
	author = {Silla, Carlos N. and Freitas, Alex A.},
	month = jan,
	year = {2011},
	pages = {31--72},
}

@article{liangxiao_jiang_novel_2009,
	title = {A {Novel} {Bayes} {Model}: {Hidden} {Naive} {Bayes}},
	volume = {21},
	issn = {1041-4347},
	shorttitle = {A {Novel} {Bayes} {Model}},
	url = {http://ieeexplore.ieee.org/document/4721435/},
	doi = {10.1109/TKDE.2008.234},
	number = {10},
	urldate = {2021-11-05},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {{Liangxiao Jiang} and Zhang, H. and {Zhihua Cai}},
	month = oct,
	year = {2009},
	pages = {1361--1371},
}

@article{peng_augmenting_2004,
	title = {Augmenting {Naive} {Bayes} {Classifiers} with {Statistical} {Language} {Models}},
	volume = {7},
	issn = {1573-7659},
	url = {https://doi.org/10.1023/B:INRT.0000011209.19643.e2},
	doi = {10.1023/B:INRT.0000011209.19643.e2},
	abstract = {We augment naive Bayes models with statistical n-gram language models to address short-comings of the standard naive Bayes text classifier. The result is a generalized naive Bayes classifier which allows for a local Markov dependence among observations; a model we refer to as the ChainAugmentedNaiveBayes (CAN) Bayes classifier. CAN models have two advantages over standard naive Bayes classifiers. First, they relax some of the independence assumptions of naive Bayes—allowing a local Markov chain dependence in the observed variables—while still permitting efficient inference and learning. Second, they permit straightforward application of sophisticated smoothing techniques from statistical language modeling, which allows one to obtain better parameter estimates than the standard Laplace smoothing used in naive Bayes classification. In this paper, we introduce CAN models and apply them to various text classification problems. To demonstrate the language independent and task independent nature of these classifiers, we present experimental results on several text classification problems—authorship attribution, text genre classification, and topic detection—in several languages—Greek, English, Japanese and Chinese. We then systematically study the key factors in the CAN model that can influence the classification performance, and analyze the strengths and weaknesses of the model.},
	language = {en},
	number = {3},
	urldate = {2021-11-05},
	journal = {Information Retrieval},
	author = {Peng, Fuchun and Schuurmans, Dale and Wang, Shaojun},
	month = sep,
	year = {2004},
	pages = {317--345},
}

@inproceedings{mccallum_comparison_1998,
	title = {A {Comparison} of {Event} {Models} for {Naive} {Bayes} {Text} {Classification}},
	url = {http://www.kamalnigam.com/papers/multinomial-aaaiws98.pdf},
	booktitle = {Learning for {Text} {Categorization}: {Papers} from the 1998 {AAAI} {Workshop}},
	author = {McCallum, Andrew and Nigam, Kamal},
	year = {1998},
	keywords = {bayes bernoulli classification ereignis event model multinomial naive text vergleich},
	pages = {41--48},
}

@inproceedings{lewis_naive_1998,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Naive ({Bayes}) at forty: {The} independence assumption in information retrieval},
	isbn = {978-3-540-69781-7},
	shorttitle = {Naive ({Bayes}) at forty},
	doi = {10.1007/BFb0026666},
	abstract = {The naive Bayes classifier, currently experiencing a renaissance ] in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML}-98},
	publisher = {Springer},
	author = {Lewis, David D.},
	editor = {Nédellec, Claire and Rouveirol, Céline},
	year = {1998},
	pages = {4--15},
}

@inproceedings{liu_cluster-based_2004,
	address = {New York, NY, USA},
	series = {{SIGIR} '04},
	title = {Cluster-based retrieval using language models},
	isbn = {978-1-58113-881-8},
	url = {https://doi.org/10.1145/1008992.1009026},
	doi = {10.1145/1008992.1009026},
	abstract = {Previous research on cluster-based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document-based retrieval. Recent developments in the language modeling approach to IR have motivated us to re-examine this problem within this new retrieval framework. We propose two new models for cluster-based retrieval and evaluate them on several TREC collections. We show that cluster-based retrieval can perform consistently across collections of realistic size, and significant improvements over document-based retrieval can be obtained in a fully automatic manner and without relevance information provided by human.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 27th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Liu, Xiaoyong and Croft, W. Bruce},
	month = jul,
	year = {2004},
	pages = {186--193},
}

@inproceedings{bar-yossef_context-sensitive_2011,
	address = {New York, NY, USA},
	series = {{WWW} '11},
	title = {Context-sensitive query auto-completion},
	isbn = {978-1-4503-0632-4},
	url = {https://doi.org/10.1145/1963405.1963424},
	doi = {10.1145/1963405.1963424},
	abstract = {Query auto completion is known to provide poor predictions of the user's query when her input prefix is very short (e.g., one or two characters). In this paper we show that context, such as the user's recent queries, can be used to improve the prediction quality considerably even for such short prefixes. We propose a context-sensitive query auto completion algorithm, NearestCompletion, which outputs the completions of the user's input that are most similar to the context queries. To measure similarity, we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity. The mapping from queries to vectors is done through a new query expansion technique that we introduce, which expands a query by traversing the query recommendation tree rooted at the query. In order to evaluate our approach, we performed extensive experimentation over the public AOL query log. We demonstrate that when the recent user's queries are relevant to the current query she is typing, then after typing a single character, NearestCompletion's MRR is 48\% higher relative to the MRR of the standard MostPopularCompletion algorithm on average. When the context is irrelevant, however, NearestCompletion's MRR is essentially zero. To mitigate this problem, we propose HybridCompletion, which is a hybrid of NearestCompletion with MostPopularCompletion. HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion, achieving a total improvement of 31.5\% in MRR relative to MostPopularCompletion on average.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 20th international conference on {World} wide web},
	publisher = {Association for Computing Machinery},
	author = {Bar-Yossef, Ziv and Kraus, Naama},
	month = mar,
	year = {2011},
	pages = {107--116},
}

@misc{yu_libshorttext_nodate,
	title = {{LibShortText}: {A} {Library} for {Short}-text {Classification} and {Analysis} {LibShortText}: {A} {Library} for {Short}-text {Classification} and {Analysis}},
	author = {Yu, Hsiang-fu and Ho, Chia-hua and Juan, Yu-chin and Lin, Chih-jen},
}

@article{neuhaus_edit_2006,
	title = {Edit distance-based kernel functions for structural pattern classification},
	volume = {39},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320306001609},
	doi = {10.1016/j.patcog.2006.04.012},
	language = {en},
	number = {10},
	urldate = {2021-11-05},
	journal = {Pattern Recognition},
	author = {Neuhaus, Michel and Bunke, Horst},
	month = oct,
	year = {2006},
	pages = {1852--1863},
}

@inproceedings{sun_fast_2012,
	address = {New York, NY, USA},
	series = {{CIKM} '12},
	title = {Fast multi-task learning for query spelling correction},
	isbn = {978-1-4503-1156-4},
	url = {https://doi.org/10.1145/2396761.2396800},
	doi = {10.1145/2396761.2396800},
	abstract = {In this paper, we explore the use of a novel online multi-task learning framework for the task of search query spelling correction. In our procedure, correction candidates are initially generated by a ranker-based system and then re-ranked by our multi-task learning algorithm. With the proposed multi-task learning method, we are able to effectively transfer information from different and highly biased training datasets, for improving spelling correction on all datasets. Our experiments are conducted on three query spelling correction datasets including the well-known TREC benchmark dataset. The experimental results demonstrate that our proposed method considerably outperforms the existing baseline systems in terms of accuracy. Importantly, the proposed method is about one order of magnitude faster than baseline systems in terms of training speed. Compared to the commonly used online learning methods which typically require more than (e.g.,) 60 training passes, our proposed method is able to closely reach the empirical optimum in about 5 passes.},
	urldate = {2021-11-04},
	booktitle = {Proceedings of the 21st {ACM} international conference on {Information} and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Sun, Xu and Shrivastava, Anshumali and Li, Ping},
	month = oct,
	year = {2012},
	pages = {285--294},
}

@inproceedings{agarwal_how_2007,
	title = {How {Much} {Noise} {Is} {Too} {Much}: {A} {Study} in {Automatic} {Text} {Classification}},
	shorttitle = {How {Much} {Noise} {Is} {Too} {Much}},
	doi = {10.1109/ICDM.2007.21},
	abstract = {Noise is a stark reality in real life data. Especially in the domain of text analytics, it has a significant impact as data cleaning forms a very large part of the data processing cycle. Noisy unstructured text is common in informal settings such as on-line chat, SMS, email, newsgroups and blogs, automatically transcribed text from speech, and automatically recognized text from printed or handwritten material. Gigabytes of such data is being generated everyday on the Internet, in contact centers, and on mobile phones. Researchers have looked at various text mining issues such as pre-processing and cleaning noisy text, information extraction, rule learning, and classification for noisy text. This paper focuses on the issues faced by automatic text classifiers in analyzing noisy documents coming from various sources. The goal of this paper is to bring out and study the effect of different kinds of noise on automatic text classification. Does the nature of such text warrant moving beyond traditional text classification techniques? We present detailed experimental results with simulated noise on the Reuters- 21578 and 20-newsgroups benchmark datasets. We present interesting results on real-life noisy datasets from various CRM domains.},
	booktitle = {Seventh {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM} 2007)},
	author = {Agarwal, Sumeet and Godbole, Shantanu and Punjani, Diwakar and Roy, Shourya},
	month = oct,
	year = {2007},
	note = {ISSN: 2374-8486},
	pages = {3--12},
}

@article{zhang_review_2014,
	title = {A {Review} on {Multi}-{Label} {Learning} {Algorithms}},
	volume = {26},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2013.39},
	abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	month = aug,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	pages = {1819--1837},
}

@article{rubin_statistical_2012,
	title = {Statistical topic models for multi-label document classification},
	volume = {88},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-011-5272-5},
	doi = {10.1007/s10994-011-5272-5},
	abstract = {Machine learning approaches to multi-label document classiﬁcation have to date largely relied on discriminative modeling techniques such as support vector machines. A drawback of these approaches is that performance rapidly drops off as the total number of labels and the number of labels per document increase. This problem is ampliﬁed when the label frequencies exhibit the type of highly skewed distributions that are often observed in real-world datasets. In this paper we investigate a class of generative statistical topic models for multi-label documents that associate individual word tokens with different labels. We investigate the advantages of this approach relative to discriminative models, particularly with respect to classiﬁcation problems involving large numbers of relatively rare labels. We compare the performance of generative and discriminative approaches on document labeling tasks ranging from datasets with several thousand labels to datasets with tens of labels. The experimental results indicate that probabilistic generative models can achieve competitive multi-label classiﬁcation performance compared to discriminative methods, and have advantages for datasets with many labels and skewed label frequencies.},
	language = {en},
	number = {1-2},
	urldate = {2021-11-05},
	journal = {Machine Learning},
	author = {Rubin, Timothy N. and Chambers, America and Smyth, Padhraic and Steyvers, Mark},
	month = jul,
	year = {2012},
	pages = {157--208},
}

@article{leopold_text_2002,
	title = {Text {Categorization} with {Support} {Vector} {Machines}. {How} to {Represent} {Texts} in {Input} {Space}?},
	volume = {46},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1012491419635},
	doi = {10.1023/A:1012491419635},
	abstract = {The choice of the kernel function is crucial to most applications of support vector machines. In this paper, however, we show that in the case of text classification, term-frequency transformations have a larger impact on the performance of SVM than the kernel itself. We discuss the role of importance-weights (e.g. document frequency and redundancy), which is not yet fully understood in the light of model complexity and calculation cost, and we show that time consuming lemmatization or stemming can be avoided even when classifying a highly inflectional language like German.},
	language = {en},
	number = {1},
	urldate = {2021-11-05},
	journal = {Machine Learning},
	author = {Leopold, Edda and Kindermann, Jörg},
	month = jan,
	year = {2002},
	pages = {423--444},
}

@article{hornik_textcat_2013,
	title = {The textcat {Package} for n-{Gram} {Based} {Text} {Categorization} in {R}},
	volume = {52},
	copyright = {Copyright (c) 2011 Kurt Hornik, Patrick Mair, Johannes Rauch, Wilhelm Geiger, Christian Buchta, Ingo Feinerer},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v052.i06},
	doi = {10.18637/jss.v052.i06},
	abstract = {Identifying the language used will typically be the first step in most natural language processing tasks. Among the wide variety of language identification methods discussed in the literature, the ones employing the Cavnar and Trenkle (1994) approach to text categorization based on character n-gram frequencies have been particularly successful. This paper presents the R extension package textcat for n-gram based text categorization which implements both the Cavnar and Trenkle approach as well as a reduced n-gram approach designed to remove redundancies of the original approach. A multi-lingual corpus obtained from the Wikipedia pages available on a selection of topics is used to illustrate the functionality of the package and the performance of the provided language identification methods.},
	language = {en},
	urldate = {2021-11-04},
	journal = {Journal of Statistical Software},
	author = {Hornik, Kurt and Mair, Patrick and Rauch, Johannes and Geiger, Wilhelm and Buchta, Christian and Feinerer, Ingo},
	month = feb,
	year = {2013},
	pages = {1--17},
}

@article{costa_automatic_2013,
	title = {Automatic normalization of short texts by combining statistical and rule-based techniques},
	volume = {47},
	issn = {1574-020X, 1574-0218},
	shorttitle = {Automatic normalization of short texts},
	url = {http://link.springer.com/10.1007/s10579-012-9187-y},
	doi = {10.1007/s10579-012-9187-y},
	abstract = {Short texts are typically composed of small number of words, most of which are abbreviations, typos and other kinds of noise. This makes the noise to signal ratio relatively high for this speciﬁc category of text. A high proportion of noise in the data is undesirable for analysis procedures as well as machine learning applications. Text normalization techniques are used to reduce the noise and improve the quality of text for processing and analysis purposes. In this work, we propose a combination of statistical and rule-based techniques to normalize short texts. More speciﬁcally, we focus our attention on SMS messages. We base our normalization approach on a statistical machine translation system which translates from noisy data to clean data. This system is trained on a small manually annotated set. Then, we study several automatic methods to extract more general rules from the normalizations generated with the statistical machine translation system. We illustrate the proposed methodology by conducting some experiments with a SMS Haitian-Cre´ole data collection. In order to evaluate the performance of our methodology we use several Haitian-Cre´ole dictionaries, the well-known perplexity criteria and the achieved reduction of vocabulary.},
	language = {en},
	number = {1},
	urldate = {2021-11-04},
	journal = {Language Resources and Evaluation},
	author = {Costa, Marta R. and Banchs, Rafael E.},
	month = mar,
	year = {2013},
	pages = {179--193},
}

@article{berrocal_synthesizing_2013,
	title = {Synthesizing categorical datasets to enhance inference},
	volume = {15},
	issn = {15723127},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1572312713000300},
	doi = {10.1016/j.stamet.2013.04.001},
	abstract = {A common data analysis setting consists of a collection of datasets of varying sizes that are all relevant to a particular scientific question, but which include different subsets of the relevant variables, presumably with some overlap. Here, we demonstrate that synthesizing cross-classified categorical datasets drawn from an incompletely cross-classified common population, where many of the sets are incomplete (i.e., one or more of the classification variables is unobserved), but at least one is completely observed is expected to reduce uncertainty about the cell probabilities in the associated multi-way contingency table as well as for derived quantities such as relative risks and odds ratios. The use of the word ‘‘expected’’ here is the key point. When synthesizing complete datasets from a common population, we are assured to reduce uncertainty. However, when we work with a log-linear model to explain the complete table, because this model cannot be fitted to any of the incomplete datasets, improvement is not assured. We provide technical clarification of this point as well as a series of simulation examples, motivated by an adverse birth outcomes investigation, to illustrate what can be expected under such synthesis.},
	language = {en},
	urldate = {2021-11-04},
	journal = {Statistical Methodology},
	author = {Berrocal, Veronica J. and Miranda, Marie Lynn and Gelfand, Alan E. and Bhattacharya, Sourabh},
	month = nov,
	year = {2013},
	pages = {25--45},
}

@article{karatzoglou_kernlab_2004,
	title = {kernlab - {An} {S4} {Package} for {Kernel} {Methods} in {R}},
	volume = {11},
	copyright = {Copyright (c) 2004 Alexandros Karatzoglou, Alexandros Smola, Kurt Hornik, Achim Zeileis},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v011.i09},
	doi = {10.18637/jss.v011.i09},
	abstract = {kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.},
	language = {en},
	urldate = {2021-11-04},
	journal = {Journal of Statistical Software},
	author = {Karatzoglou, Alexandros and Smola, Alexandros and Hornik, Kurt and Zeileis, Achim},
	month = nov,
	year = {2004},
	pages = {1--20},
}

@article{lodhi_text_2002,
	title = {Text {Classification} using {String} {Kernels}},
	volume = {2},
	issn = {ISSN 1533-7928},
	url = {https://www.jmlr.org/papers/v2/lodhi02a.html},
	abstract = {We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets.},
	number = {Feb},
	urldate = {2021-11-04},
	journal = {Journal of Machine Learning Research},
	author = {Lodhi, Huma and Saunders, Craig and Shawe-Taylor, John and Cristianini, Nello and Watkins, Chris},
	year = {2002},
	pages = {419--444},
}

@article{gupta_training_2014,
	title = {Training {Highly} {Multiclass} {Classifiers}},
	volume = {15},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v15/gupta14a.html},
	abstract = {Classification problems with thousands or more classes often have a large range of class-confusabilities, and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters, and focuses the training on pairs of classes that are easier to differentiate at any given time in the training. We also show that the adagrad method, recently proposed for automatically decreasing step sizes for convex stochastic gradient descent, can also be profitably applied to the nonconvex joint training of supervised dimensionality reduction and linear classifiers as done in Wsabie. Experiments on ImageNet benchmark data sets and proprietary image recognition problems with 15,000 to 97,000 classes show substantial gains in classification accuracy compared to one-vs- all linear SVMs and Wsabie.},
	number = {43},
	urldate = {2021-11-04},
	journal = {Journal of Machine Learning Research},
	author = {Gupta, Maya R. and Bengio, Samy and Weston, Jason},
	year = {2014},
	pages = {1461--1492},
}

@techreport{willenborg_semantic_2012,
	address = {The Hague/Heerlen},
	title = {Semantic networks for automatic coding},
	abstract = {In this paper a methodology for automatic coding is suggested, 
which is fairly general. Characteristic for this approach  is that each code 
(say for a business activity) is characterized by one or more combinations of 
code words, or C-words. These combinations of C-words can be seen as 
definitions of the various codes used. It is assumed that the order of the C-
words is irrelevant to describe a code. Because it is unlikely that people will 
use exactly those C-words when describing a business activity, synonyms, 
hyponyms and hyperonyms for the C-words are needed as well. These words  
connect the definition of the codes in the classification used on the one hand 
and the descriptions provided by respondent on the other. These words are 
called D-words. A semantic network is used to provide a bridge between the 
descriptions and the codes.},
	institution = {Statistics Netherlands},
	author = {Willenborg, Leon},
	year = {2012},
	pages = {44},
}

@inproceedings{macchia_integration_2010,
	address = {Rome},
	title = {Integration between automatic coding and statistical analysis of textual data systems},
	shorttitle = {{JADT} 2010},
	author = {Macchia, Stefania and Murgia, Manuela and Vicari, Paola},
	year = {2010},
}

@article{macchia_system_2001,
	title = {A system to monitor the quality of automated coding of textual answers to open questions},
	volume = {4},
	number = {2},
	journal = {Research in Official Statistics},
	author = {Macchia, Stefania and D'Orazio, Marcello},
	year = {2001},
	pages = {7--21},
}

@article{kalpic_automated_1994,
	title = {Automated {Coding} of {Census} {Data}},
	volume = {10},
	issn = {0282-423X},
	abstract = {Experiences from Croation and Bosnia-Herzegowina},
	number = {4},
	journal = {Journal of Official Statistics},
	author = {Kalpi´c, Damir},
	year = {1994},
	pages = {449--463},
}

@techreport{instituto_nacional_de_estadistica_autocod_2012,
	title = {{AutoCod}: a tool for the automatic coding of the variables 'occupation' and 'economic activity'},
	institution = {Instituto Nacional de Estadistica},
	author = {Instituto Nacional de Estadistica},
	month = may,
	year = {2012},
}

@article{bao_occupation_2020,
	title = {Occupation {Coding} of {Job} {Titles}: {Iterative} {Development} of an {Automated} {Coding} {Algorithm} for the {Canadian} {National} {Occupation} {Classification} ({ACA}-{NOC})},
	volume = {4},
	copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
	shorttitle = {Occupation {Coding} of {Job} {Titles}},
	url = {https://formative.jmir.org/2020/8/e16422},
	doi = {10.2196/16422},
	abstract = {Background: In many research studies, the identification of social determinants is an important activity, in particular, information about occupations is frequently added to existing patient data. Such information is usually solicited during interviews with open-ended questions such as “What is your job?” and “What industry sector do you work in?” Before being able to use this information for further analysis, the responses need to be categorized using a coding system, such as the Canadian National Occupational Classification (NOC). Manual coding is the usual method, which is a time-consuming and error-prone activity, suitable for automation.
Objective: This study aims to facilitate automated coding by introducing a rigorous algorithm that will be able to identify the NOC (2016) codes using only job title and industry information as input. Using manually coded data sets, we sought to benchmark and iteratively improve the performance of the algorithm.
Methods: We developed the ACA-NOC algorithm based on the NOC (2016), which allowed users to match NOC codes with job and industry titles. We employed several different search strategies in the ACA-NOC algorithm to find the best match, including exact search, minor exact search, like search, near (same order) search, near (different order) search, any search, and weak match search. In addition, a filtering step based on the hierarchical structure of the NOC data was applied to the algorithm to select the best matching codes.
Results: The ACA-NOC was applied to over 500 manually coded job and industry titles. The accuracy rate at the four-digit NOC code level was 58.7\% (332/566) and improved when broader job categories were considered (65.0\% at the three-digit NOC code level, 72.3\% at the two-digit NOC code level, and 81.6\% at the one-digit NOC code level).
Conclusions: The ACA-NOC is a rigorous algorithm for automatically coding the Canadian NOC system and has been evaluated using real-world data. It allows researchers to code moderate-sized data sets with occupation in a timely and cost-efficient manner such that further analytics are possible. Initial assessments indicate that it has state-of-the-art performance and is readily extensible upon further benchmarking on larger data sets.},
	language = {EN},
	number = {8},
	urldate = {2021-11-04},
	journal = {JMIR Formative Research},
	author = {Bao, Hongchang and Baker, Christopher J. O. and Adisesh, Anil},
	month = aug,
	year = {2020},
	note = {Company: JMIR Formative Research
Distributor: JMIR Formative Research
Institution: JMIR Formative Research
Label: JMIR Formative Research
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e16422},
}

@incollection{conrad_using_1997,
	title = {Using {Expert} {Systems} to {Model} and {Improve} {Survey} {Classification} {Processes}},
	isbn = {978-1-118-49001-3},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118490013.ch17},
	abstract = {This chapter contains sections titled: Introduction Automated and Computer-Assisted Coding Concepts Behind Expert Systems Case Studies Evaluation and Resource Issues Conclusions Acknowledgments},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {Survey {Measurement} and {Process} {Quality}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Conrad, Frederick},
	year = {1997},
	doi = {10.1002/9781118490013.ch17},
	note = {Section: 17
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118490013.ch17},
	pages = {393--414},
}

@mastersthesis{kaptein_meta-classifier_2005,
	address = {Maastricht},
	title = {Meta-{Classifier} {Approaches} to {Reliable} {Text} {Classification}},
	school = {Universiteit Maastricht},
	author = {Kaptein, Rianne},
	month = aug,
	year = {2005},
}

@inproceedings{winkler_machine_2000,
	title = {Machine {Learning}, {Information} {Retrieval}, and {Record} {Linkage}},
	abstract = {Classification into groups using terms available in the data underlies machine learning, information retrieval, and record linkage. Classifiers such as Bayesian networks in machine learning and term weighting in information retrieval depend primarily on training data sets for which truth is known. These classifiers may be relatively slow to adapt to new situations in which new data have characteristics significantly different from the training data. Record linkage has been characterized by data in which training data can differ significantly from new data being classified. By using structuring ideas introduced by Fellegi and Sunter in their classic 1969 JASA paper, record linkage researchers have been able to apply the EM algorithm and Markov Chain Monte Carlo ideas to make classifiers automatically adapt to new data. We show how these ideas can be used to improve the learnability of Bayesian networks. We also use some of the ideas from machine learning that are superficially related to boosting to show how non-naïve Bayesian classifiers can make better use of training data.},
	language = {en},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	publisher = {American Statistical Association},
	author = {Winkler, William E},
	month = aug,
	year = {2000},
	pages = {11},
}

@article{nigam_text_2000,
	title = {Text {Classification} from {Labeled} and {Unlabeled} {Documents} using {EM}},
	volume = {39},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007692713085},
	doi = {10.1023/A:1007692713085},
	abstract = {This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.},
	language = {en},
	number = {2},
	urldate = {2021-11-04},
	journal = {Machine Learning},
	author = {Nigam, Kamal and Mccallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
	month = may,
	year = {2000},
	pages = {103--134},
}

@book{engel_improving_2014,
	address = {New York},
	title = {Improving {Survey} {Methods}: {Lessons} from {Recent} {Research}},
	isbn = {978-1-315-75628-8},
	shorttitle = {Improving {Survey} {Methods}},
	abstract = {This state-of-the-art volume provides insight into the recent developments in survey research. It covers topics like: survey modes and response effects, bio indicators and paradata, interviewer and survey error, mixed-mode panels, sensitive questions, conducting web surveys and access panels, coping with non-response, and handling missing data.　The authors are leading scientists in the field, and discuss the latest methods and challenges with respect to these topics.

Each of the book’s eight parts starts with a brief chapter that provides an historical context along with an overview of today’s most critical survey methods. Chapters in the sections focus on research applications in practice and discuss results from field studies. As such, the book will help researchers design surveys according to today’s best practices.　
The book’s website www.survey-methodology.de provides additional information, statistical analyses, tables and figures.　

An indispensable reference for practicing researchers and methodologists or any professional who uses surveys in their work, this book also serves as a supplement for graduate or upper level-undergraduate courses on survey methods taught in psychology, sociology, education, economics, and business. Although the book focuses on European findings, all of the research is discussed with reference to the entire survey-methodology area, including the US. As such, the insights in this book will apply to surveys conducted around the world.},
	publisher = {Routledge},
	editor = {Engel, Uwe and Jann, Ben and Lynn, Peter and Scherpenzeel, Annette and Sturgis, Patrick},
	month = sep,
	year = {2014},
	doi = {10.4324/9781315756288},
}

@inproceedings{russ_computer-based_2014,
	title = {Computer-{Based} {Coding} of {Occupation} {Codes} for {Epidemiological} {Analyses}},
	shorttitle = {Proc {IEEE} {Int} {Symp} {Comput} {Based} {Med} {Syst}},
	doi = {10.1109/CBMS.2014.79},
	booktitle = {27th {International} {Symposium} on {Computer}-{Based} {Medical} {Systems}},
	author = {Russ, Daniel E. and Ho, Kwan-Yuet and Johnson, Calvin A. and Friesen, Melissa C.},
	month = may,
	year = {2014},
	note = {ISSN: 2372-9198},
	pages = {347--350},
}

@article{munteanu_coresets-methods_2018,
	title = {Coresets-{Methods} and {History}: {A} {Theoreticians} {Design} {Pattern} for {Approximation} and {Streaming} {Algorithms}},
	volume = {32},
	issn = {0933-1875, 1610-1987},
	shorttitle = {Coresets-{Methods} and {History}},
	url = {http://link.springer.com/10.1007/s13218-017-0519-3},
	doi = {10.1007/s13218-017-0519-3},
	abstract = {We present a technical survey on the state of the art approaches in data reduction and the coreset framework. These include geometric decompositions, gradient methods, random sampling, sketching and random projections. We further outline their importance for the design of streaming algorithms and give a brief overview on lower bounding techniques.},
	language = {en},
	number = {1},
	urldate = {2021-11-04},
	journal = {KI - Künstliche Intelligenz},
	author = {Munteanu, Alexander and Schwiegelshohn, Chris},
	month = feb,
	year = {2018},
	pages = {37--53},
}

@incollection{phillips_coresets_2017,
	edition = {3},
	title = {Coresets and {Sketches}},
	isbn = {978-1-315-11960-1},
	abstract = {Geometric data summarization has become an essential tool in both geometric approximation algorithms and where geometry intersects with big data problems. In linear or near-linear time, large data sets can be compressed into a summary, and then more intricate algorithms can be run on the summaries whose results approximate those of the full data set. Coresets and sketches are the two most important classes of these summaries.},
	booktitle = {Handbook of {Discrete} and {Computational} {Geometry}},
	publisher = {Chapman and Hall/CRC},
	author = {Phillips, Jeff M.},
	year = {2017},
	note = {Num Pages: 20},
}

@article{kish_retaining_1971,
	title = {Retaining {Units} after {Changing} {Strata} and {Probabilities}},
	volume = {66},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482286},
	doi = {10.1080/01621459.1971.10482286},
	language = {en},
	number = {335},
	urldate = {2021-11-01},
	journal = {Journal of the American Statistical Association},
	author = {Kish, Leslie and Scott, Alastair},
	month = sep,
	year = {1971},
	pages = {461--470},
}

@book{maimon_data_2005,
	address = {New York},
	title = {Data {Mining} and {Knowledge} {Discovery} {Handbook}, 1st ed.},
	isbn = {978-0-387-24435-8},
	url = {http://link.springer.com/10.1007/b107408},
	language = {en},
	urldate = {2021-10-19},
	publisher = {Springer-Verlag},
	editor = {Maimon, Oded and Rokach, Lior},
	year = {2005},
	doi = {10.1007/b107408},
}

@book{maimon_data_2010,
	address = {Boston, MA},
	edition = {2nd ed.},
	title = {Data {Mining} and {Knowledge} {Discovery} {Handbook}, 2nd. ed.},
	isbn = {978-0-387-09822-7 978-0-387-09823-4},
	url = {http://link.springer.com/10.1007/978-0-387-09823-4},
	language = {en},
	urldate = {2021-11-01},
	publisher = {Springer US},
	editor = {Maimon, Oded and Rokach, Lior},
	year = {2010},
	doi = {10.1007/978-0-387-09823-4},
}

@book{goodman_handbook_2017,
	address = {Boca Raton},
	edition = {3},
	title = {Handbook of {Discrete} and {Computational} {Geometry}},
	isbn = {978-1-315-11960-1},
	abstract = {The Handbook of Discrete and Computational Geometry is intended as a reference book fully accessible to nonspecialists as well as specialists, covering all major aspects of both fields. 
The book offers the most important results and methods in discrete and computational geometry to those who use them in their work, both in the academic world—as researchers in mathematics and computer science—and in the professional world—as practitioners in ﬁelds as diverse as operations research, molecular biology, and robotics. 
Discrete geometry has contributed signiﬁcantly to the growth of discrete mathematics in recent years. This has been fueled partly by the advent of powerful computers and by the recent explosion of activity in the relatively young ﬁeld of computational geometry. This synthesis between discrete and computational geometry lies at the heart of this Handbook. 
A growing list of application fields includes combinatorial optimization, computer-aided design, computer graphics, crystallography, data analysis, error-correcting codes, geographic information systems, motion planning, operations research, pattern recognition, robotics, solid modeling, and tomography.},
	publisher = {Chapman and Hall/CRC},
	editor = {Goodman, Jacob E. and O’Rourke, Joseph and Tóth, Csaba D.},
	month = nov,
	year = {2017},
	doi = {10.1201/9781315119601},
}

@misc{dfg_linkliste_2021,
	title = {Linkliste: {DFG}-{Förderangebote} für die wissenschaftliche {Karriere}},
	url = {https://www.dfg.de/foerderung/wissenschaftliche_karriere/veranstaltungen/index.html},
	urldate = {2021-10-31},
	author = {DFG},
	month = oct,
	year = {2021},
}

@article{mcguire_assessment_1998,
	title = {Assessment of {Occupational} {Exposures} in {Community}-{Based} {Case}-{Control} {Studies}},
	volume = {19},
	url = {https://doi.org/10.1146/annurev.publhealth.19.1.35},
	doi = {10.1146/annurev.publhealth.19.1.35},
	abstract = {Assessing occupational exposures in community-based studies is a challenge for investigators because there are no standardized or validated approaches for collecting information regarding occupational history. The strengths and limitations of the methods available for assessing occupational exposures are reviewed. In community-based case-control studies, the prevalence of most chemical agents is low. The common sources of misclassification in these studies are addressed, as are strategies for dealing with misclassification bias. Methods to assess the presence and magnitude of differential reporting by cases and controls are outlined, together with analytic strategies to improve the classification of occupational exposures.},
	number = {1},
	urldate = {2021-10-28},
	journal = {Annual Review of Public Health},
	author = {McGuire, Valerie and Nelson, Lorene M. and Koepsell, Thomas D. and Checkoway, Harvey and Longstreth, W. T.},
	year = {1998},
	pmid = {9611611},
	note = {\_eprint: https://doi.org/10.1146/annurev.publhealth.19.1.35},
	pages = {35--53},
}

@article{schonbach_berufsverschlusselungen_1977,
	title = {Berufsverschlüsselungen: {Ein} {Bericht} aus der {Codeabteilung}},
	volume = {1},
	number = {1},
	journal = {ZUMA-Nachrichten},
	author = {Schönbach, Klaus},
	month = dec,
	year = {1977},
	pages = {18--20},
}

@techreport{office_for_national_statistics_data_2012,
	address = {Titchfield},
	type = {2011 {Census}: {Methods} and {Quality} {Report}},
	title = {Data {Capture}, {Coding} and {Cleaning} for the 2011 {Census}},
	institution = {Office for National Statistics},
	author = {{Office for National Statistics}},
	month = dec,
	year = {2012},
}

@inproceedings{martin_comparison_1995,
	title = {A {Comparison} of {Interviewer} and {Office} {Coding} of {Occupations}},
	url = {http://www.asasrms.org/Proceedings/y1995f.html},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}},
	publisher = {American Statistical Association},
	author = {Martin, Jean and Bushnell, Diane and Campanelli, Pamela and Thomas, Roger},
	month = may,
	year = {1995},
	pages = {1122--1127},
}

@misc{hunter_training_2015,
	title = {Training on {ISCO}},
	url = {https://slideplayer.com/slide/7348741/},
	author = {Hunter, David},
	year = {2015},
}

@article{hoffmeyer-zlotnik_berufsklassifikation_2003,
	title = {Berufsklassifikation und {Messung} des beruflichen {Status}/{Prestige}},
	volume = {27},
	number = {52},
	journal = {ZUMA-Nachrichten},
	author = {Hoffmeyer-Zlotnik, Jürgen H.P. and Geis, Alfons J.},
	month = may,
	year = {2003},
	pages = {125--139},
}

@article{hoffmeyer-zlotnik_computerunterstutzte_2004,
	title = {Computerunterstützte {Vercodung} der {International} {Standard} {Classification} of {Occupations} ({ISCO}-88)},
	volume = {28},
	number = {55},
	journal = {ZUMA-Nachrichten},
	author = {Hoffmeyer-Zlotnik, Jürgen H.P. and Hess, Doris and Geis, Alfons J.},
	month = nov,
	year = {2004},
	pages = {29--52},
}

@techreport{hartmann_klassifizierung_2002,
	address = {Munich},
	title = {Die {Klassifizierung} der {Berufe} und der {Wirtschaftszweige} im {Sozio}-oekonomischen {Panel}: {Neuvercodung} der {Daten} 1984 - 2001},
	institution = {Infratest Sozialforschung},
	author = {Hartmann, Josef and Schütz, Gerd},
	month = nov,
	year = {2002},
}

@article{hak_coder_1996,
	title = {Coder training: {Theoretical} training or practical socialization?},
	volume = {19},
	issn = {1573-7837},
	shorttitle = {Coder training},
	url = {https://doi.org/10.1007/BF02393420},
	doi = {10.1007/BF02393420},
	abstract = {Usually the effectiveness of coder training as a means to improve the inter-coder-reliability of the coding of responses to open-ended questions is considered a result of (successfully) communicating the coding scheme to coders. However, the actual practice of coder training has never been studied empirically. In this article we present fragments of a transcript of a coder training that suggest that inter-coder-reliability is improved not only by communicating the coding instructions to coders (theoretical training) but also bysocializing coders into practical rules which are not part of the coding instructions and are not warranted by them. Hence, it cannot be excluded that the improvement of the inter-coder-reliability by means of coder training is atraining artifact: an artificial outcome affected through the training process. It follows that, in each particular case, the researcher must make plausible that the actual coding process has yielded valid data.},
	language = {en},
	number = {2},
	urldate = {2021-10-28},
	journal = {Qualitative Sociology},
	author = {Hak, Tony and Bernts, Ton},
	month = jun,
	year = {1996},
	pages = {235--257},
}

@article{gilbert_unbelieving_1990,
	title = {Unbelieving the unbelievable: {Some} problems in the rejection of false information},
	volume = {59},
	issn = {1939-1315},
	shorttitle = {Unbelieving the unbelievable},
	doi = {10.1037/0022-3514.59.4.601},
	abstract = {B. Spinoza (1677 [1982]) suggested that all information is accepted during comprehension and that false information is then unaccepted. Subjects were presented with true and false linguistic propositions and, on some trials, their processing of that information was interrupted. As Spinoza's model predicted, interruption increased the likelihood that subjects would consider false propositions true but not vice versa (Study 1). This was so even when the proposition was iconic and when its veracity was revealed before its comprehension (Study 2). In fact, merely comprehending a false proposition increased the likelihood that subjects would later consider it true (Study 3). The results suggest that both true and false information are initially represented as true and that people are not easily able to alter this method of representation. Results are discussed in terms of contemporary research on attribution, lie detection, hypothesis testing, and attitude change. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Journal of Personality and Social Psychology},
	author = {Gilbert, Daniel T. and Krull, Douglas S. and Malone, Patrick S.},
	year = {1990},
	pages = {601--613},
}

@article{geis_kompabilitat_2001,
	title = {Kompabilität von {ISCO}-68, {ISCO}-88 und {KldB}-92},
	volume = {25},
	number = {48},
	journal = {ZUMA-Nachrichten},
	author = {Geis, Alfons and Hoffmeyer-Zlotnik, Jürgen H.P.},
	month = may,
	year = {2001},
	pages = {117--138},
}

@inproceedings{gaertner_tests_1989,
	title = {Tests of {Alternative} {Questions} for {Measuring} {Industry} and {Occupation} in the {CPS}},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}},
	publisher = {American Statistical Association},
	author = {Gaertner, Gregory and Cantor, David and Gay, Nancy and Shank, Susan},
	year = {1989},
	pages = {490--495},
}

@article{elias_skill_2001,
	title = {Skill measurement in official statistics: recent developments in the {UK} and the rest of {Europe}},
	volume = {53},
	issn = {14643812},
	shorttitle = {Skill measurement in official statistics},
	url = {https://academic.oup.com/oep/article-lookup/doi/10.1093/oep/53.3.508},
	doi = {10.1093/oep/53.3.508},
	language = {en},
	number = {3},
	urldate = {2021-10-28},
	journal = {Oxford Economic Papers},
	author = {Elias, P.},
	month = jul,
	year = {2001},
	pages = {508--540},
}

@techreport{bergman_comparing_2005,
	type = {Cambridge {Studies} in {Social} {Research}},
	title = {Comparing {Social} {Stratification} {Schemata}: {CAMSIS}, {CSP}-{CH}, {Goldthorpe}, {ISCO}-88, {Treiman}, and {Wright}},
	shorttitle = {Comparing {Social} {Stratification} {Schemata}},
	language = {en},
	number = {10},
	author = {Bergman, Manfred Max and Joye, Dominique},
	year = {2005},
	pages = {35},
}

@article{kleinberg_human_2017,
	title = {Human {Decisions} and {Machine} {Predictions}},
	issn = {0033-5533, 1531-4650},
	url = {http://academic.oup.com/qje/article/doi/10.1093/qje/qjx032/4095198/Human-Decisions-and-Machine-Predictions},
	doi = {10.1093/qje/qjx032},
	language = {en},
	urldate = {2021-10-23},
	journal = {The Quarterly Journal of Economics},
	author = {Kleinberg, Jon and Lakkaraju, Himabindu and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
	month = aug,
	year = {2017},
}

@incollection{presser_methods_2004,
	address = {Hoboken, NJ, USA},
	title = {Methods for {Testing} and {Evaluating} {Survey} {Questions} - [{Methods} for {Testing} and {Evaluating} {Survey} {Questions}]},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch1},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Presser, Stanley and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Rothgeb, Jennifer M. and Singer, Eleanor},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch1},
	pages = {1--22},
}

@article{sakshaug_evaluating_2016,
	title = {Evaluating {Active} ({Opt}-{In}) and {Passive} ({Opt}-{Out}) {Consent} {Bias} in the {Transfer} of {Federal} {Contact} {Data} to a {Third}-{Party} {Survey} {Agency}},
	volume = {4},
	issn = {2325-0984, 2325-0992},
	url = {https://academic.oup.com/jssam/article-lookup/doi/10.1093/jssam/smw020},
	doi = {10.1093/jssam/smw020},
	abstract = {Obtaining informed consent from individuals to participate in voluntary research studies is widely considered to be an ethical research practice. However, there is considerable debate over how consent should be obtained from subjects. Many researchers argue that active (opt-in) consent is the only type of consent that accurately reflects the true wishes of the subject and is closer to the informed consent ideal than passive (opt-out) consent procedures. Opponents of active consent procedures argue that such procedures harm study participation rates and increase the risk of self-selection bias to a greater extent than passive consent procedures. Empirical evaluations of these claims are rare, given the lack of studies that experimentally assign subjects to different consent procedures and utilize a control group (in which no consent is sought) to facilitate comparison. We report on an experiment that overcomes these issues in an study of consent to transfer contact data from a federal register to a third-party data collector for purposes of carrying out a telephone survey. Specifically, we evaluate the impact of requiring consent on survey participation rates, self-selection bias, and the resulting survey estimates. We find that the passive consent procedure does a better job of minimizing self-selection bias and maximizing the validity of the survey estimates (relative to the control group) compared with the active consent procedure. However, neither procedure is ideal: Both consent procedures increase the total self-selection bias and reduce the sample size. We conclude with a general discussion of the main findings and their practical implications.},
	language = {en},
	number = {3},
	urldate = {2021-10-22},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Sakshaug, Joseph W. and Schmucker, Alexandra and Kreuter, Frauke and Couper, Mick P. and Singer, Eleanor},
	month = sep,
	year = {2016},
	pages = {382--416},
}

@techreport{roberts_data_2010,
	address = {University of Essex, Colchester},
	type = {Research {Working} {Paper} 2010-36},
	title = {Data quality in telephone surveys and the effect of questionnaire length: a cross- national experiment},
	shorttitle = {Data quality in telephone surveys and the effect of questionnaire length},
	abstract = {Respondents in long telephone survey interviews may adopt satisficing strategies as they approach the end of the questionnaire (Holbrook, Green and Krosnick, 2003). However, there is inconsistency regarding the relationship between questionnaire length and different forms of satisficing. We investigate whether long questionnaires are associated with a reduction in response quality using data from a cross-national survey experiment. Sample members were randomly assigned to interviews of 60, 45 or 30 minutes. We compare responses to attitudinal measures from a module on happiness and well-being, which was asked at different points in the interview in each of the three groups.},
	language = {en},
	institution = {Institute for Social and Economic Research},
	author = {Roberts, Caroline and Gillian, Eva and Allum, Nick and Lynn, Peter},
	month = nov,
	year = {2010},
}

@article{holbrook_telephone_2003,
	title = {Telephone versus {Face}-to-{Face} {Interviewing} of {National} {Probability} {Samples} with {Long} {Questionnaires}: {Comparisons} of {Respondent} {Satisficing} and {Social} {Desirability} {Response} {Bias}},
	volume = {67},
	issn = {0033-362X},
	shorttitle = {Telephone versus {Face}-to-{Face} {Interviewing} of {National} {Probability} {Samples} with {Long} {Questionnaires}},
	url = {https://doi.org/10.1086/346010},
	doi = {10.1086/346010},
	abstract = {The last 50 years have seen a gradual replacement of face-to-face interviewing with telephone interviewing as the dominant mode of survey data collection in the United States. But some of the most expensive and large-scale nationally funded, long-term survey research projects involving national area-probability samples and long questionnaires retain face-to-face interviewing as their mode. In this article, we propose two ways in which shifting such surveys to random digit dialing (RDD) telephone interviewing might affect the quality of data acquired, and we test these hypotheses using data from three national mode experiments. Random digit dialing telephone respondents were more likely to satisfice (as evidenced by no-opinion responding, nondifferentiation, and acquiescence), to be less cooperative and engaged in the interview, and were more likely to express dissatisfaction with the length of the interview than were face-to-face respondents, despite the fact that the telephone interviews were completed more quickly than the face-to-face interviews. Telephone respondents were also more suspicious about the interview process and more likely to present themselves in socially desirable ways than were face-to-face respondents. These findings shed light on the nature of the survey response process, on the costs and benefits associated with particular survey modes, and on the nature of social interaction generally.},
	number = {1},
	journal = {Public Opinion Quarterly},
	author = {Holbrook, Allyson L. and Green, Melanie C. and Krosnick, Jon A.},
	year = {2003},
	note = {\_eprint: http://poq.oxfordjournals.org/content/67/1/79.full.pdf+html},
	pages = {79--125},
}

@article{galesic_effects_2009,
	title = {Effects of {Questionnaire} {Length} on {Participation} and {Indicators} of {Response} {Quality} in a {Web} {Survey}},
	volume = {73},
	issn = {0033-362X},
	url = {https://doi.org/10.1093/poq/nfp031},
	doi = {10.1093/poq/nfp031},
	abstract = {This paper investigates how expected and actual questionnaire length affects cooperation rates and a variety of indicators of data quality in web surveys. We hypothesized that the expected length of a web-based questionnaire is negatively related to the initial willingness to participate. Moreover, the serial position of questions was predicted to influence four indicators of data quality. We hypothesized that questions asked later in a web-based questionnaire will, compared to those asked earlier, be associated with (a) shorter response times, (b) higher item-nonresponse rates, (c) shorter answers to open-ended questions, and (d) less variability to items arranged in grids. To test these assumptions, we manipulated the stated length (10, 20, and 30 minutes) and the position of questions in an online questionnaire consisting of randomly ordered blocks of thematically related questions. As expected, the longer the stated length, the fewer respondents started and completed the questionnaire. In addition, answers to questions positioned later in the questionnaire were faster, shorter, and more uniform than answers to questions positioned near the beginning.},
	number = {2},
	journal = {Public Opinion Quarterly},
	author = {Galesic, Mirta and Bosnjak, Michael},
	year = {2009},
	note = {\_eprint: http://poq.oxfordjournals.org/content/73/2/349.full.pdf+html},
	pages = {349--360},
}

@article{couper_helping_2016,
	title = {Helping {Respondents} {Provide} {Good} {Answers} in {Web} {Surveys}},
	volume = {10},
	issn = {1864-3361},
	abstract = {This paper reports on a series of experiments to explore ways to use the technology of Web surveys to help respondents provide well-formed answers to questions that may be difficult to answer. Specifically, we focus on the use of drop-down or select lists and JavaScript lookup tables as alternatives to open text fields for the collection of information on prescription drugs. The first two experiments were conducted among members of opt-in panels in the U.S. The third experiment was conducted in the 2013 Health and Retirement Study Internet Survey. Respondents in each of the studies were randomly assigned to one of three input methods: text field, drop box, or JavaScript lookup, and asked to provide the names of prescription drugs they were taking. We compare both the quality of answers obtained using the three methods, and the effort (time) taken to provide such answers. We examine differences in performance on the three input format types by key respondent demographics and Internet experience. We discuss some of the technical challenges of implementing complex question types and offer some recommendations for the use of such tools in Web surveys.},
	number = {1},
	journal = {Survey Research Methods},
	author = {Couper, Mick and Zhang, Chan},
	year = {2016},
	pages = {49--64},
}

@book{de_waal_handbook_2011,
	address = {Hoboken},
	series = {Wiley {Handbooks} in {Survey} {Methodology}},
	title = {Handbook of {Statistical} {Data} {Editing} and {Imputation}},
	isbn = {978-0-470-54280-4},
	shorttitle = {Handbook of {Statistical} {Data} {Editing} and {Imputation}},
	url = {http://doi.wiley.com/10.1002/9780470904848},
	publisher = {John Wiley \& Sons, Inc.},
	author = {de Waal, Ton and Pannekoek, Jeroen and Scholtus, Sander},
	month = mar,
	year = {2011},
	doi = {10.1002/9780470904848},
}

@misc{noauthor_euroccupations_nodate,
	title = {{EurOccupations} - {WageIndicator}.org},
	url = {https://wageindicator.org/Wageindicatorfoundation/projects/euroccp},
	urldate = {2021-10-28},
}

@article{tijdens_dropout_2014,
	title = {Dropout {Rates} and {Response} {Times} of an {Occupation} {Search} {Tree} in a {Web} {Survey}},
	volume = {30},
	url = {https://sciendo.com/article/10.2478/jos-2014-0002},
	doi = {10.2478/jos-2014-0002},
	abstract = {Occupation is key in socioeconomic research. As in other survey modes, most web surveys use an open-ended question for occupation, though the absence of interviewers elicits unidentifiable or aggregated responses. Unlike other modes, web surveys can use a search tree with an occupation database. They are hardly ever used, but this may change due to technical advancements. This article evaluates a three-step search tree with 1,700 occupational titles, used in the 2010 multilingual WageIndicator web survey for UK, Belgium and Netherlands (22,990 observations). Dropout rates are high; in Step 1 due to unemployed respondents judging the question not to be adequate, and in Step 3 due to search tree item length. Median response times are substantial due to search tree item length, dropout in the next step and invalid occupations ticked. Overall the validity of the occupation data is rather good, 1.7-7.5\% of the respondents completing the search tree have ticked an invalid occupation.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Journal of Official Statistics},
	author = {Tijdens, Kea},
	month = feb,
	year = {2014},
	pages = {23--43},
}

@article{tijdens_measuring_2012,
	title = {Measuring work activities and skill requirements of occupations: {Experiences} from a {European} pilot study with a web‐survey},
	volume = {36},
	issn = {2046-9012},
	shorttitle = {Measuring work activities and skill requirements of occupations},
	url = {https://www.emerald.com/insight/content/doi/10.1108/03090591211255575/full/html},
	doi = {10.1108/03090591211255575},
	language = {en},
	number = {7},
	urldate = {2021-10-28},
	journal = {European Journal of Training and Development},
	author = {Tijdens, Kea G. and De Ruijter, Judith and De Ruijter, Esther},
	month = aug,
	year = {2012},
	pages = {751--763},
}

@inproceedings{hunter_case_2013,
	address = {Geneva},
	title = {The case to update or revise the  {International} {Standard} {Classification} of  {Occupations}, 2008 ({ISCO}-08)},
	language = {en},
	booktitle = {19th {International} {Conference} of {Labour} {Statisticians}},
	publisher = {International Labour Office},
	author = {Hunter, David},
	month = oct,
	year = {2013},
}

@inproceedings{appel_census_1983,
	title = {Census {Bureau} {Experience} with {Automated} {Industry} and {Occupation} {Coding}},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}},
	publisher = {American Statistical Association},
	author = {Appel, Martin V. and Hellerman, Eli},
	year = {1983},
	pages = {32--40},
}

@inproceedings{bushnell_evaluation_1998,
	address = {Southampton},
	title = {An evaluation of computer-assisted occupation coding},
	booktitle = {Proceedings of the {International} {Conference}},
	publisher = {Association for Survey Computing},
	author = {Bushnell, Diane},
	editor = {Westlake, Andrew and Martin, Jean and Rigg, Malcolm and Skinner, Chris},
	month = aug,
	year = {1998},
	pages = {pp. 23--36},
}

@inproceedings{cantor_evaluating_1992,
	title = {Evaluating {Interviewer} {Style} for {Collecting} {Industry} and {Occupation} {Information}},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}},
	publisher = {American Statistical Association},
	author = {Cantor, David and Esposito, James},
	year = {1992},
	pages = {661--666},
}

@misc{tijdens_self-identification_2016,
	address = {Amsterdam},
	title = {Self-identification of occupation in (web-) surveys: databases and experiences},
	author = {Tijdens, Kea},
	month = nov,
	year = {2016},
}

@misc{tijdens_design_2019,
	address = {Zagreb},
	title = {Design {Principles} of a {Multi}-{Country}, {Multilingual} {Database} of {Occupations}},
	author = {Tijdens, Kea and Belloni, Michele},
	month = jul,
	year = {2019},
}

@misc{noauthor_occupation_nodate,
	title = {Occupation measurement {\textbar} {SurveyCodings}},
	url = {https://www.surveycodings.org/occupation-measurement},
	urldate = {2021-10-28},
}

@article{tijdens_validating_2018,
	title = {Validating occupational coding indexes for use in multi-country surveys},
	url = {https://surveyinsights.org/?p=10422},
	doi = {10.13094/SMIF-2018-00007},
	urldate = {2021-10-28},
	author = {Tijdens, K.G. and Kaandorp, C.S.},
	year = {2018},
	note = {Publisher: Survey Insights, Methods from the Field (SMIF)},
}

@techreport{tijdens_database_2019,
	title = {Database of occupational titles, with  explanatory note},
	url = {www.seriss.eu/resources/deliverables},
	language = {en},
	institution = {SERISS},
	author = {Tijdens, Kea},
	month = jan,
	year = {2019},
	pages = {36},
}

@techreport{tijdens_eu-harmonised_2017,
	address = {Leuven},
	type = {Deliverable 21.1},
	title = {Eu-harmonised and comparative measurement of occupations and skills},
	language = {en},
	institution = {InGRID},
	author = {Tijdens, Kea and Visintin, Stefano},
	month = mar,
	year = {2017},
	pages = {59},
}

@techreport{tijdens_design_2015,
	address = {Leuven},
	title = {The design of a tool for the measurement of occupations in web surveys using a global index of occupations},
	url = {https://zenodo.org/record/3897472},
	abstract = {Occupation is a key variable in socio-economic research, used in a wide variety of studies, but the measurement of occupations is a major challenge. In case of open-ended survey questions, websurveys pose extra challenges, as respondents are more likely to key in odd text compared to other survey modes, particularly those with an interviewer. Websurveys however offer new opportunities for closed survey questions with self-identification of occupations, particularly when using semantic matching and a database with large numbers of coded occupational titles. This paper details the design requirements for such a database to facilitate surveys in multiple countries. First, this paper describes how the WISCO Database of Occupations has developed since its first use in 2000, when the database was applied in the WageIndicator websurvey on work and wages. By the end of 2015 the websurvey and the database had expanded to 85 countries. In these fifteen years, the number of occupations in the database had increased from 55 to 1,896 and the languages covered from 1 to 43. The paper then presents an overview of the ISCO occupational classification with four hierarchical levels and their coding. It also reviews how occupations can be measured in websurveys in open-ended or closed survey questions. For the closed question three approaches are detailed, notably scrolling, search trees and semantic matching. Given that any national labour market easily cover 10,000 or more occupational titles, the semantic matching is considered the best approach provided that the look-up tables include 5,000 or more occupational titles for each country. Finally, the paper details the design principles in the database used for semantic matching. It discusses the size of the source list in relation to the ISCO coding. It details the issue of male and female titles and the use of start or end years for occupations It specifies how the database copes with highly aggregated occupational titles such as clerk or manager, with abbreviations and organisation specific job titles, with synonyms, with skill levels within and across occupations, with occupations in the corporate hierarchy, with composite jobs, with handicraft workers, and with subsistence farmers and hunters. Next, it details how to cope with respondents indicating that their job title is not in the database. The paper ends with a future outlook for a programme to test the validity and reliability of occupational measurement.},
	language = {eng},
	urldate = {2021-10-28},
	author = {Tijdens, Kea},
	month = dec,
	year = {2015},
	pages = {25},
}

@article{de_matteis_occupational_2017,
	title = {Occupational self-coding and automatic recording ({OSCAR}): a novel web-based tool to collect and code lifetime job histories in large population-based studies},
	volume = {43},
	issn = {0355-3140},
	shorttitle = {Occupational self-coding and automatic recording ({OSCAR})},
	doi = {10.5271/sjweh.3613},
	number = {2},
	journal = {Scandinavian Journal of Work, Environment \& Health},
	author = {De Matteis, Sara and Jarvis, Deborah and Young, Heather and Young, Alan and Allen, Naomi and Potts, James and Darnton, Andrew and Rushton, Lesley and Cullinan, Paul},
	year = {2017},
	pages = {181--186},
}

@article{hoffmann_standard_2003,
	title = {Standard statistical classifications: {Basic} principles},
	volume = {19},
	issn = {01678000},
	shorttitle = {Standard statistical classifications},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SJU-2002-19401},
	doi = {10.3233/SJU-2002-19401},
	abstract = {This report describes best practices for the development, use, maintenance and revision of international standard statistical classifications (ISC); and the corresponding derived or related national (NSC) and multinational statistical classifications. Attention is drawn to the need to: (1) state goals and problems clearly; (2) identify the actors involved in the development and use of classifications (producers and users of statistics); (3) identify the injunctions which follow from legislation and government policies; (4) describe how the structure and details of the classification are used when producing and presenting statistics; (5) understand the use of statistics produced with the classification; (6) establish monitoring mechanisms for proper feedback from classifications users about problems in its use; (7) maintain a time table to draft, update or revise the classification; (8) coordinate the process with work on other classifications; and (9) set standards for dissemination of the classification and its related updates and revisions. The principles and standards of research methodology and statistics should be applied when the classifications are designed, tested, used, updated and revised. Clarity in terminology, concepts, definitions and structure are required for satisfactory results.},
	language = {en},
	number = {4},
	urldate = {2021-10-28},
	journal = {Statistical Journal of the United Nations Economic Commission for Europe},
	author = {Hoffmann, Eivind and Chamie, Mary},
	month = apr,
	year = {2003},
	pages = {223--241},
}

@misc{noauthor_readings_nodate,
	title = {Readings in {Nomenclatures}},
	url = {https://ec.europa.eu/eurostat/ramon/other_documents/index.cfm?TargetUrl=DSP_READINGS},
	urldate = {2021-10-28},
}

@misc{noauthor_your_nodate,
	title = {Your companion guide to  international statistical classifications},
	url = {https://ec.europa.eu/eurostat/ramon/miscellaneous/index.cfm?TargetUrl=DSP_GENINFO_CLASS},
	urldate = {2021-10-28},
}

@misc{noauthor_europa_nodate,
	title = {Europa - {RAMON} - {Classification} {List}},
	url = {https://ec.europa.eu/eurostat/ramon/nomenclatures/index.cfm?TargetUrl=LST_NOM&StrGroupCode=CLASSIFIC&StrLanguageCode=EN},
	urldate = {2021-10-28},
}

@misc{laughlin_6-digit_2019,
	address = {Denver},
	title = {From a 6-{Digit} to a 4-{Digit} {Code}: {How} the {Census} {Bureau} {Implements} the {Standard} {Occupation} {Classification} ({SOC}) {System}},
	url = {https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303022},
	urldate = {2021-10-28},
	author = {Laughlin, Lynda},
	month = jul,
	year = {2019},
}

@misc{salmon_overview_2019,
	address = {Denver},
	title = {An overview of the 2018 standard occupational classification ({SOC})},
	url = {https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303022},
	urldate = {2021-10-28},
	author = {Salmon, Laurie},
	month = jul,
	year = {2019},
}

@misc{fayer_2018_2019,
	address = {Denver},
	title = {The 2018 {Standard} {Occupational} {Classification} ({SOC}): {Options} for defining {STEM}},
	url = {https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303022},
	urldate = {2021-10-28},
	author = {Fayer, Stella},
	month = jul,
	year = {2019},
}

@article{amosse_revisiting_2013,
	title = {Revisiting the {History} of {Socio}-professional {Classification} in {France}},
	volume = {68},
	issn = {2398-5682, 2268-3763},
	url = {https://www.cambridge.org/core/product/identifier/S2398568200000157/type/journal_article},
	doi = {10.1017/S2398568200000157},
	abstract = {The result of a process begun in the nineteenth century, the French system of socio-professional classification (
              code des catégories socio-professionnelles
              ) was drawn up between 1951 and 1954 and has only been slightly modified since. With no strong theoretical framework and conceived according to a realist approach, it gave substance to social classes in the description of postwar society. During a period of “reworking” (1978-1981), it became an exciting topic of sociological exploration, furnishing a representation of Pierre Bourdieu’s two-dimensional social space and serving as a laboratory for the pragmatic sociology of Luc Boltanski and Laurent Thévenot. In a subsequent period of “updating” (1995-2001), administrative caution regarding changes contrasted with the evolution of categories used in labor law and the goal of analytical purity underpinned by econometrics. The history of this classification details the peculiar position of a statistical tool for representing the social world, ostensibly static amidst constant changes to the institution that managed it, the actors who used it, the social categories—everyday or legal—to which it referred, and, finally, the sociological theories that gave it a conceptual grounding.},
	language = {en},
	number = {04},
	urldate = {2021-10-28},
	journal = {Annales (English ed.)},
	author = {Amossé, Thomas},
	month = dec,
	year = {2013},
	pages = {697--732},
}

@misc{dumbacher_using_2019,
	address = {Denver},
	title = {Using {Machine} {Learning} to {Assign} {North} {American} {Industry} {Classification} {System} {Codes} to {Establishments} {Based} on {Business} {Description} {Write}-{Ins}},
	url = {https://www.census.gov/content/dam/Census/newsroom/press-kits/2019/jsm/Using%20Machine%20Learning%20to%20Assign%20North%20American%20Industry%20Classification%20System%20Codes%20to%20Establishments_Dumbacher.pdf},
	author = {Dumbacher, Brian and Russell, Anne},
	month = jul,
	year = {2019},
}

@article{becker_elements_1956,
	title = {The {Elements} of {Identification} with an {Occupation}},
	volume = {21},
	issn = {00031224},
	url = {http://www.jstor.org/stable/2089290?origin=crossref},
	doi = {10.2307/2089290},
	language = {en},
	number = {3},
	urldate = {2021-10-28},
	journal = {American Sociological Review},
	author = {Becker, Howard S. and Carper, James},
	month = jun,
	year = {1956},
	pages = {341},
}

@book{mcgovern_market_2007,
	address = {Oxford, New York},
	title = {Market, {Class}, and {Employment}},
	isbn = {978-0-19-921338-2},
	abstract = {Much of the received wisdom about the world of work emphasizes the marketization of the employment relationship; the decline of class-based forms of inequality, and the individualization of employment relations. Non-standard forms of employment, the delayering of organizational hierarchies, and the use of individual performance-based payment systems are all held up as examples of a new neo-liberal order in which employers and employees no longer feel a sense of obligation to each other. Drawing on a range of employee and employer surveys, including the authors own Working in Britain 2000 survey, this ambitious study presents a comprehensive examination of the conditions, attitudes, and experiences of British employees from the mid-1980s to the early years of this century. The authors' analyses provides a compelling critique of the received wisdom, while also providing an original, alternative account of recent developments in work and labour markets. Along the way, the book covers such topical issues as the changing nature of trade union membership, the consequences of Britain's 'long hours' culture', and the apparent inability of women to ask for pay rises. Significantly, the authors seek to reposition debates about the future of work by restoring the concepts of contracts and social class to the analysis of the employment relationship. Based on the ESRC funded Future of Work research programme this book is destined to shape our understanding of employment in Britain for the foreseeable future.},
	publisher = {Oxford University Press},
	author = {McGovern, Patrick and Hill, Stephen and Mills, Colin and White, Michael},
	month = dec,
	year = {2007},
}

@article{ebner_berufe_2020,
	title = {Berufe und soziale {Ungleichheit} – {Thematische} {Einführung} und {Inhalte} des {Sonderhefts}},
	volume = {72},
	issn = {1861-891X},
	url = {https://doi.org/10.1007/s11577-020-00675-2},
	doi = {10.1007/s11577-020-00675-2},
	abstract = {Die Untersuchung der Bedeutung des Berufs für die soziale Ungleichheit hat eine lange Tradition. In diesem einleitenden Beitrag werden die im hier vorliegenden Sonderheft der Kölner Zeitschrift für Soziologie und Sozialpsychologie gesammelten Aufsätze in den Kontext der bisherigen Berufsforschung gestellt. Der Anspruch des Sonderhefts besteht darin, Berufe nicht wie andernorts als „Blackbox“ zu betrachten, sondern explizit danach zu fragen, welche sozialen Mechanismen den Berufen ihre ungleichheitsrelevante Wirkung verleihen. Der erste Themenblock des Sonderhefts verschafft den Leserinnen und Lesern einen  Zugang zu theoretischen Grundlagen und Messkonzepten im Forschungsfeld „Berufe und soziale Ungleichheit“. In den anschließenden Themenblöcken wird anhand konkreter Forschungsfragen aufgezeigt, welches Potenzial in der hier vorgestellten Perspektive auf Mechanismen im Themenfeld Berufe und soziale Ungleichheit steckt. Die Themenblöcke befassen sich mit Fragen von Berufswahl und beruflichen Karrieren, mit dem Zusammenhang von Berufen und Geschlechterungleichheit, mit der Bedeutung von Berufen im Kontext der Debatten um Migration und Zuwanderung sowie mit dem Thema Berufe im Zeitalter der Digitalisierung und Globalisierung. Dabei handelt es sich explizit um theoretisch fundierte, empirische Beiträge, die gleichzeitig die Vielfalt der für diese Art von Analysen verwendbaren Datengrundlagen und methodischen Zugänge aufzeigt.},
	language = {de},
	number = {1},
	urldate = {2021-10-28},
	journal = {KZfSS Kölner Zeitschrift für Soziologie und Sozialpsychologie},
	author = {Ebner, Christian and Haupt, Andreas and Matthes, Britta},
	month = sep,
	year = {2020},
	pages = {1--17},
}

@article{christoph_occupation-based_2020,
	title = {Occupation-{Based} {Measures}—{An} {Overview} and {Discussion}},
	volume = {72},
	issn = {0023-2653, 1861-891X},
	url = {https://link.springer.com/10.1007/s11577-020-00673-4},
	doi = {10.1007/s11577-020-00673-4},
	abstract = {Occupational information is among the most versatile categories of information about a person available in quantitative data. The goal of this paper is to provide an overview of occupation-based measures in different topic areas. These include not only measures for analyzing social stratiﬁcation, such as prestige scales, socioeconomic indices and class schemes but also measures of workplace tasks, occupation-speciﬁc health risks, gender segregation, and occupational closure.},
	language = {en},
	number = {S1},
	urldate = {2021-10-28},
	journal = {KZfSS Kölner Zeitschrift für Soziologie und Sozialpsychologie},
	author = {Christoph, Bernhard and Matthes, Britta and Ebner, Christian},
	month = sep,
	year = {2020},
	pages = {41--78},
}

@article{connelly_review_2016,
	title = {A {Review} of occupation-based social classifications for social survey research},
	volume = {9},
	issn = {2059-7991, 2059-7991},
	url = {http://journals.sagepub.com/doi/10.1177/2059799116638003},
	doi = {10.1177/2059799116638003},
	abstract = {This article is a review of issues associated with measuring occupations and using occupation-based socio-economic classifications in social science research. The review is orientated towards researchers who undertake secondary analysis of large-scale micro-level social science datasets. This article begins with an outline of how to handle raw occupational information. This is followed by an introduction to the two main approaches to measuring occupations and a third lesser known but intellectually innovative approach. The three approaches are social class schemes, social stratification scales and the microclass approach. International comparisons are briefly described and a discussion of intersectionality with other key variables such as age and gender is provided. We are careful to emphasise that this article does not advocate the uncritical adoption of any one particular occupation-based socio-economic measure over and above other alternatives. Rather, we are advocating that researchers should choose from the portfolio of existing socio-economic measures in an informed and empirically defensible way, and we strongly advocate undertaking sensitivity analyses. We conclude that researchers should always use existing socio-economic measures that have agreed on and well-documented standards. We strongly advise researchers not to develop their own measures without strong justification nor to use existing measures in an un-prescribed or ad hoc manner.},
	language = {en},
	urldate = {2021-10-28},
	journal = {Methodological Innovations},
	author = {Connelly, Roxanne and Gayle, Vernon and Lambert, Paul S.},
	month = jan,
	year = {2016},
	pages = {205979911663800},
}

@book{schroder_scoring_2014,
	address = {Oldenburg},
	title = {Scoring im {Fokus}: ökonomische {Bedeutung} und rechtliche {Rahmenbedingungen} im internationalen {Vergleich}},
	isbn = {978-3-8142-2316-2},
	shorttitle = {Scoring im {Fokus}},
	abstract = {Unter dem Begriff Credit Scoring lassen sich mathematisch-statistische Verfahren zusammenfassen, anhand derer Informationen über Verbraucher oder Unternehmen zu quantitativen Punktwerten (Scores) verdichtet werden, um zu einer statistisch belastbaren Prognose der Erfüllungswahrscheinlichkeit von Geld- und Warenkrediten zu gelangen. Im Zuge neuer Vertriebs- und Zahlungswege bei Konsumgütern, aber auch im Bankensegment wächst aktuell die Nachfrage nach entsprechenden Prognosen sowie deren volkswirtschaftliche Relevanz. Wichtiger Treiber ist hier die Digitalisierung von Kauftransaktionen, die zu einer zunehmenden Anonymisierung und Kurzfristigkeit von Geschäftsbeziehungen führt. Dadurch sind persönliche Eindrücke und langfristige Erfahrungen seltener die Basis für Geschäfte auf Kredit. In der vorliegenden Studie wurden im ersten Teil die ökonomische Bedeutung und die methodischen Verfahrensansätze des Credit Scorings sowie institutionelle Rahmenbedingungen und Anbieterstrukturen in unterschiedlichen Ländern analysiert. Mit der Bewertung von Kreditscoring aus rechtlicher Sicht beschäftigt sich der zweite Teil der Studie. {\textless}dt.{\textgreater}},
	language = {ger},
	publisher = {BIS-Verl. der Carl von Ossietzky Univ. Oldenburg},
	editor = {Schröder, Michael and Taeger, Jürgen},
	year = {2014},
}

@article{hand_statistical_1997,
	title = {Statistical {Classification} {Methods} in {Consumer} {Credit} {Scoring}: a {Review}},
	volume = {160},
	issn = {1467-985X},
	shorttitle = {Statistical {Classification} {Methods} in {Consumer} {Credit} {Scoring}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-985X.1997.00078.x},
	doi = {10.1111/j.1467-985X.1997.00078.x},
	abstract = {Credit scoring is the term used to describe formal statistical methods used for classifying applicants for credit into ‘good’ and ‘bad’ risk classes. Such methods have become increasingly important with the dramatic growth in consumer credit in recent years. A wide range of statistical methods has been applied, though the literature available to the public is limited for reasons of commercial confidentiality. Particular problems arising in the credit scoring context are examined and the statistical methods which have been applied are reviewed.},
	language = {en},
	number = {3},
	urldate = {2021-10-23},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Hand, D. J. and Henley, W. E.},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-985X.1997.00078.x},
	pages = {523--541},
}

@article{mehrabi_survey_2019,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	url = {https://arxiv.org/abs/1908.09635v2},
	abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	language = {en},
	urldate = {2021-10-23},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = aug,
	year = {2019},
}

@misc{horwitz_focus_nodate,
	address = {Lisbon},
	title = {Do {Focus} {Group} {Participants} {Mean} {What} {They} {Say}? {A} {Field} {Test} of {Mailing} {Materials} {Updated} {Based} on {Qualitative} {Feedback}},
	author = {Horwitz, Rachel and Finamore, John},
}

@misc{seeman_generalized_2018,
	address = {Chicago},
	title = {Generalized {Reinforcement} {Learning} and {Applications} in {Public} {Policy}},
	language = {en},
	author = {Seeman, Jeremy},
	month = jul,
	year = {2018},
}

@misc{usher_big_2014,
	title = {Big {Data} and the {Social} {Sciences}},
	url = {https://de.slideshare.net/abeusher/big-data-and-the-social-sciences},
	urldate = {2021-10-23},
	author = {Usher, Abe},
	month = oct,
	year = {2014},
}

@misc{netscher_domain_2021,
	address = {virtual},
	title = {Domain {Data} {Protocols} for {Educational} {Research}. {Domain}-{Specific} {Guidance} and {Cross}-{Disciplinary} {Interoperability}},
	url = {https://zenodo.org/record/5121474},
	abstract = {Researchers are increasingly encouraged by different stakeholders to make research processes as transparent as possible, to enable reproducible research results and to share their (research) data FAIRly and openly with others. Such requirements can be challenging, as not all researchers are familiar with the concepts of FAIR and open data. At the same time, existing tools (and guidance) to foster the creation of FAIR data – such as templates for data management plans – vary to a great extent, rarely indicating what the best practice solution is. The project Domain Data Protocols for Educational Research in Germany aims to address this issue by developing a standardized tool to create data management plans. Funded by the German Federal Ministry of Education and Research, it brings together twelve German research institutions with diverse areas of expertise on educational research to develop so called Domain Data Protocols (in short DDPs). Based on a concept by Science Europe, DDPs are open, standardized, and referenceable data protocols, serving as a ‘model’ data management plan for a specific research domain, i.e. educational research in our case. DDPs are for the benefit of various stakeholders. First, they assist researchers in doing excellent data management, preparing project proposals and funding applications as well as offering support for data archiving and sharing. Second, they enable replication of results by the research community as well as the re-use of data by others in new (research) contexts. Third, DDPs simplify review processes on data management, reducing the efforts of examining funding applications and (periodical) reports on data management by implementing standardized procedures. Finally, DDPs foster data ingest in a data repository or archive, by fostering researchers in the creation of FAIR data. The development of DDPs is not without challenges, as their structure needs to be flexible enough to cover different types of data and methods and to enable researchers to reflect their individual project-specific requirements. DDPs therefore consist of different modules, e.g. in the context of data collection, documentation, legal issues, and data sharing. Each of these modules contains different elements defining a minimum set of requirements on what FAIR data look like and includes use cases, standards, relevant regulations as well as further resources on related data management practices.},
	language = {eng},
	urldate = {2021-10-23},
	author = {Netscher, Sebastian and Schwickerath, Anna},
	month = jul,
	year = {2021},
	doi = {10.5281/zenodo.5121474},
}

@misc{bender_datenschutz_2014,
	address = {Berlin},
	title = {Datenschutz, {Big} {Data} und das öffentliche {Gut}},
	url = {https://www.konsortswd.de/aktuelles/veranstaltungen/6kswd/},
	author = {Bender, Stefan and Lane, Julia and Stodden, Victoria and Holland, Mike},
	month = feb,
	year = {2014},
}

@misc{thompson_problem_2021,
	title = {The problem with the ‘gap in the literature’},
	url = {https://blogs.lse.ac.uk/impactofsocialsciences/2021/07/16/the-problem-with-the-gap-in-the-literature/},
	abstract = {In this cross-post Pat Thomson explores how an approach based around filling a gap in the research or literature can be problematic and how approaches based on different wording can align research …},
	language = {"en-US"},
	urldate = {2021-10-22},
	journal = {Impact of Social Sciences},
	author = {Thompson, Pat},
	month = jul,
	year = {2021},
}

@article{gopen_science_1990,
	title = {The {Science} of {Scientific} {Writing}},
	volume = {79},
	language = {en},
	journal = {American Scientist},
	author = {Gopen, George D and Swan, Judith A},
	year = {1990},
	pages = {17},
}

@article{plaxco_art_2010,
	title = {The art of writing science},
	volume = {19},
	issn = {09618368},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/pro.514},
	doi = {10.1002/pro.514},
	language = {en},
	number = {12},
	urldate = {2021-10-22},
	journal = {Protein Science},
	author = {Plaxco, Kevin W.},
	month = dec,
	year = {2010},
	pages = {2261--2266},
}

@book{sarnecka_writing_2019,
	title = {The {Writing} {Workshop}: {Write} {More}, {Write} {Better}, {Be} {Happier} in {Academia}},
	shorttitle = {The {Writing} {Workshop}},
	url = {https://osf.io/n8pc3/},
	abstract = {Self-published book on academic writing by Barbara W. Sarnecka. 
    Hosted on the Open Science Framework},
	language = {en},
	urldate = {2021-10-22},
	author = {Sarnecka, Barbara W.},
	month = may,
	year = {2019},
	note = {Publisher: OSF},
}

@misc{labaree_research_nodate,
	type = {Research {Guide}},
	title = {Research {Guides}: {Organizing} {Your} {Social} {Sciences} {Research} {Paper}: {Purpose} of {Guide}},
	copyright = {Copyright University of Southern California 2021},
	shorttitle = {Research {Guides}},
	url = {https://libguides.usc.edu/writingguide/purpose},
	abstract = {Offers detailed guidance on how to develop, organize, and write a college-level research paper in the social and behavioral sciences.},
	language = {en},
	urldate = {2021-10-22},
	author = {Labaree, Robert V.},
}

@article{selvin_sharpening_1984,
	title = {On {Sharpening} {Sociologists}' {Prose}},
	volume = {25},
	issn = {0038-0253},
	url = {https://doi.org/10.1111/j.1533-8525.1984.tb00183.x},
	doi = {10.1111/j.1533-8525.1984.tb00183.x},
	abstract = {Hopefully, the quality of sociologists' writing will improve irregardless of the temptation to talk jargon-wise.},
	number = {2},
	urldate = {2021-10-22},
	journal = {The Sociological Quarterly},
	author = {Selvin, Hanan C. and Wilson, Everett K.},
	month = mar,
	year = {1984},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1111/j.1533-8525.1984.tb00183.x},
	pages = {205--222},
}

@misc{the_economist_how_2021,
	title = {How to write well},
	url = {https://www.youtube.com/watch?v=YDErZqcEM7E},
	abstract = {Our journalists discuss how to communicate with clarity, style and precision},
	urldate = {2021-10-22},
	author = {{The Economist}},
	month = sep,
	year = {2021},
}

@misc{haas_pre-registration_2018,
	address = {Nürnberg},
	title = {Pre-registration of research projects},
	author = {Haas, Georg},
	year = {2018},
}

@misc{noauthor_bundesregierung_nodate,
	title = {Bundesregierung beschließt {Datenstrategie}},
	url = {https://www.bundesregierung.de/breg-de/aktuelles/datenstrategie-beschlossen-1842786},
	abstract = {Die Bundesregierung beschließt eine Datenstrategie mit über 240 Maßnahmen, um der gesellschaftlichen Bedeutung von Daten gerecht zu werden.},
	language = {de},
	urldate = {2021-10-22},
	journal = {Bundesregierung},
}

@misc{dietrich_emotional_2018,
	title = {Emotional {Arousal} {Predicts} {Voting} on the {U}.{S}. {Supreme} {Court}},
	url = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JFU71R},
	abstract = {Do judges telegraph their preferences during oral arguments? Using the U.S. Supreme Court as our example, we demonstrate that Justices implicitly r...},
	language = {en},
	urldate = {2021-10-22},
	publisher = {Harvard Dataverse},
	author = {Dietrich, Bryce J. and Enos, Ryan D. and Sen, Maya},
	month = jul,
	year = {2018},
	doi = {10.7910/DVN/JFU71R},
	note = {Type: dataset},
}

@book{european_commission_eurostat_european_2013,
	address = {LU},
	title = {European {Health} {Interview} {Survey} ({EHIS} wave 2): methodological manual : 2013 edition.},
	shorttitle = {European {Health} {Interview} {Survey} ({EHIS} wave 2)},
	url = {https://data.europa.eu/doi/10.2785/43280},
	language = {eng},
	urldate = {2021-10-22},
	publisher = {Publications Office},
	author = {{European Commission. Eurostat.}},
	year = {2013},
}

@inproceedings{iannacchione_response_1991,
	title = {Response {Probability} {Weight}  {Adjustments} {Using} {Logistic} {Regression}},
	url = {http://www.asasrms.org/Proceedings/y1991f.html},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	author = {Iannacchione, Vincent G. and Milne, Jennifer G. and Folsom, Ralph E.},
	year = {1991},
	pages = {637--642},
}

@inproceedings{lepkowski_weighting_1989,
	title = {Weighting {Adjustments} for {Partial} {Nonresponse} in the 1984 {SIPP} {Panel}},
	url = {http://www.asasrms.org/Proceedings/y1989f.html},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	author = {Lepkowski, James M. and Kalton, Graham and Kasprzyk, Daniel},
	year = {1989},
	pages = {296--301},
}

@article{lumley_fitting_2017,
	title = {Fitting {Regression} {Models} to {Survey} {Data}},
	volume = {32},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-2/Fitting-Regression-Models-to-Survey-Data/10.1214/16-STS605.full},
	doi = {10.1214/16-STS605},
	abstract = {Data from complex surveys are being used increasingly to build the same sort of explanatory and predictive models used in the rest of statistics. Although the assumptions underlying standard statistical methods are not even approximately valid for most survey data, analogues of most of the features of standard regression packages are now available for use with survey data. We review recent developments in the ﬁeld and illustrate their use on data from NHANES.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Statistical Science},
	author = {Lumley, Thomas and Scott, Alastair},
	month = may,
	year = {2017},
}

@article{winship_sampling_1994,
	title = {Sampling {Weights} and {Regression} {Analysis}},
	volume = {23},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124194023002004},
	doi = {10.1177/0049124194023002004},
	abstract = {Most major population surveys used by social scientists are based on complex sampling designs where sampling units have different probabilities of being selected. Although sampling weights must generally be used to derive unbiased estimates of univariate population characteristics, the decision about their use in regression analysis is more complicated. Where sampling weights are solely a function of independent variables included in the model, unweighted OLS estimates are preferred because they are unbiased, consistent, and have smaller standard errors than weighted OLS estimates. Where sampling weights are a function of the dependent variable (and thus of the error term), we recommend first attempting to respecify the model so that they are solely a function of the independent variables. If this can be accomplished, then unweighted OLS is again preferred. If the model cannot be respecified, then estimation of the model using sampling weights may be appropriate. In this case, however, the formula used by most computer programs for calculating standard errors will be incorrect. We recommend using the White heteroskedastic consistent estimator for the standard errors.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Sociological Methods \& Research},
	author = {Winship, Christopher and Radbill, Larry},
	month = nov,
	year = {1994},
	note = {Publisher: SAGE Publications Inc},
	pages = {230--257},
}

@misc{issp_demographic_methods_group_issp_nodate,
	title = {{ISSP}: {ISSP} {Background} {Variables}},
	url = {https://www.gesis.org/en/issp/home/issp-background-variables},
	urldate = {2021-10-22},
	author = {ISSP Demographic Methods Group},
}

@techreport{bicksler_sexual_2014,
	title = {Sexual {Assault} and {Sexual} {Harassment} in the {U}.{S}. {Military}: {Volume} 1. {Design} of the 2014 {RAND} {Military} {Workplace} {Study}},
	shorttitle = {Sexual {Assault} and {Sexual} {Harassment} in the {U}.{S}. {Military}},
	url = {https://www.rand.org/pubs/research_reports/RR870z1.html},
	abstract = {The Department of Defense Sexual Assault Prevention and Response Office asked the RAND Corporation to conduct an independent assessment of sexual assault and sexual harassment in the US military. This volume documents the methodology used in the resulting RAND Military Workplace Study, which invited close to 560,000 service members to participate in a survey fielded in August and September of 2014.},
	language = {en},
	urldate = {2021-10-22},
	institution = {RAND Corporation},
	author = {Bicksler, Barbara and Farris, Coreen and Ghosh-Dastidar, Bonnie and Jaycox, Lisa H. and Kilpatrick, Dean and Kistler, Steve and Street, Amy and Tanielian, Terri and Williams, Kayla M. and Morral, Andrew R. and Gore, Kristie L. and Schell, Terry L.},
	month = dec,
	year = {2014},
}

@article{bauer_is_2017,
	title = {Is the {Left}-{Right} {Scale} a {Valid} {Measure} of {Ideology}?: {Individual}-{Level} {Variation} in {Associations} with “{Left}” and “{Right}” and {Left}-{Right} {Self}-{Placement}},
	volume = {39},
	issn = {0190-9320, 1573-6687},
	shorttitle = {Is the {Left}-{Right} {Scale} a {Valid} {Measure} of {Ideology}?},
	url = {http://link.springer.com/10.1007/s11109-016-9368-2},
	doi = {10.1007/s11109-016-9368-2},
	abstract = {In order to measure ideology, political scientists heavily rely on the socalled left-right scale. Left and right are, however, abstract political concepts and may trigger different associations among respondents. If these associations vary systematically with other variables this may induce bias in the empirical study of ideology. We illustrate this problem using a unique survey that asked respondents open-ended questions regarding the meanings they attribute to the concepts ‘‘left’’ and ‘‘right’’. We assess and categorize this textual data using topic modeling techniques. Our analysis shows that variation in respondents’ associations is systematically related to their self-placement on the left-right scale and also to variables such as education and respondents’ cultural background (East vs. West Germany). Our ﬁndings indicate that the interpersonal comparability of the leftright scale across individuals is impaired. More generally, our study suggests that we need more research on how respondents interpret various abstract concepts that we regularly use in survey questions.},
	language = {en},
	number = {3},
	urldate = {2021-10-22},
	journal = {Political Behavior},
	author = {Bauer, Paul C. and Barberá, Pablo and Ackermann, Kathrin and Venetz, Aaron},
	month = sep,
	year = {2017},
	pages = {553--583},
}

@article{galesic_human_2021,
	title = {Human social sensing is an untapped resource for computational social science},
	volume = {595},
	copyright = {2021 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03649-2},
	doi = {10.1038/s41586-021-03649-2},
	abstract = {The ability to ‘sense’ the social environment and thereby to understand the thoughts and actions of others allows humans to fit into their social worlds, communicate and cooperate, and learn from others’ experiences. Here we argue that, through the lens of computational social science, this ability can be used to advance research into human sociality. When strategically selected to represent a specific population of interest, human social sensors can help to describe and predict societal trends. In addition, their reports of how they experience their social worlds can help to build models of social dynamics that are constrained by the empirical reality of human social systems.},
	language = {en},
	number = {7866},
	urldate = {2021-10-22},
	journal = {Nature},
	author = {Galesic, Mirta and Bruine de Bruin, Wändi and Dalege, Jonas and Feld, Scott L. and Kreuter, Frauke and Olsson, Henrik and Prelec, Drazen and Stein, Daniel L. and van der Does, Tamara},
	month = jul,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7866
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Human behaviour;Interdisciplinary studies;Sociology
Subject\_term\_id: human-behaviour;interdisciplinary-studies;sociology},
	pages = {214--222},
}

@article{stern_use_2008,
	title = {The {Use} of {Client}-side {Paradata} in {Analyzing} the {Effects} of {Visual} {Layout} on {Changing} {Responses} in {Web} {Surveys}},
	volume = {20},
	issn = {1525-822X, 1552-3969},
	url = {http://journals.sagepub.com/doi/10.1177/1525822X08320421},
	doi = {10.1177/1525822X08320421},
	abstract = {Recent research has shown that even minor changes in the visual layout of survey questions can affect the way in which respondents answer. Still, little is known about how these changes affect the actual answering process. The recent development of client-side paradata allows us to better understand the way respondents construct their answers by providing new types of data, including time taken to answer a question and whether respondents change their answers. Although a number of studies have used this technology to examine response latency, few have taken advantage of its potential to collect data on the types of changes respondents make while answering survey questions. This study uses client-side paradata to analyze changes in multiple versions of three types of Web survey questions. The results suggest that the visual layout of survey questions not only affects the number but also the types of changes respondents make.},
	language = {en},
	number = {4},
	urldate = {2021-10-22},
	journal = {Field Methods},
	author = {Stern, Michael J.},
	month = nov,
	year = {2008},
	pages = {377--398},
}

@article{herzing_investigation_2020,
	title = {Investigation of alternative interface designs for long-list questions–the case of a computer-assisted survey in {Germany}},
	volume = {23},
	issn = {1364-5579},
	url = {https://doi.org/10.1080/13645579.2020.1723201},
	doi = {10.1080/13645579.2020.1723201},
	abstract = {This study aims to address the questionnaire design challenges in cases wherein questions involve a large number of response options. Traditionally, these long-list questions are asked in open-ended or closed-ended formats. However, alternative interface design options are emerging in computer-assisted surveys that combine both interface designs. To investigate trade-offs of these alternative designs, a split-ballot experiment was conducted with a) a long list of radio buttons, b) a search tree (nested list of response options), and c) a combo box (combination of a text box and a drop-down box). Based on the question on the highest educational qualification attained from the Innovation Sample of the German Socio-Economic Panel, we investigated the interface design that facilitates respondents optimally and enhances the measurement quality. The findings indicate that combo boxes reduce the response burden and increase measurement details, whereas search trees and long lists reduce post-coding efforts.},
	number = {6},
	urldate = {2021-10-22},
	journal = {International Journal of Social Research Methodology},
	author = {Herzing, Jessica M. E.},
	month = nov,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13645579.2020.1723201},
	pages = {639--650},
}

@article{couper_designing_2011,
	title = {Designing {Input} {Fields} for {Non}-{Narrative} {Open}-{Ended} {Responses} in {Web} {Surveys}},
	volume = {27},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	language = {en},
	number = {1},
	journal = {Journal of Official Statistics},
	author = {Couper, Mick P. and Kennedy, Courtney and Conrad, Frederick G. and Tourangeau, Roger},
	year = {2011},
	pages = {65--85},
}

@article{lynn_longer_2014,
	title = {Longer {Interviews} {May} {Not} {Affect} {Subsequent} {Survey} {Participation} {Propensity}},
	volume = {78},
	issn = {0033-362X},
	url = {https://doi.org/10.1093/poq/nfu015},
	doi = {10.1093/poq/nfu015},
	abstract = {Survey researchers often assume that respondent burden is an important determinant of survey participation propensity and that interview length is a good indicator of burden. However, there is surprisingly little evidence of the effect of length of a completed interview on subsequent participation propensity, particularly in the case of face-to-face surveys. This article presents results from a large-scale randomized experiment in which respondents experienced interviews of different lengths at wave 1 of a panel survey. Subsequently, respondents were asked to complete a self-completion questionnaire and then to take part in further waves of the survey. For each of these subsequent tasks, the study compares completion rates between those administered the shorter and those administered the longer version of the wave 1 interview. No evidence is found that wave 1 interview length affects subsequent participation propensity.},
	number = {2},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Lynn, Peter},
	month = jan,
	year = {2014},
	pages = {500--509},
}

@article{herzog_effects_1981,
	title = {Effects of {Questionnaire} {Length} on {Response} {Quality}},
	volume = {45},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/2748903},
	abstract = {A response tendency resulting from the length of a group-administered questionnaire instrument is described. Respondents answering items that are included in large sets toward the later parts of a long questionnaire are more likely to give identical answers to most or all of the items, compared with those responding to items in smaller sets or in shorter questionnaires. While means and intercorrelations among items within the same set are affected by this "straight-line" response pattern, intercorrelations between items from different sets are much less affected by it. These investigations are based on comparisons between a long questionnaire, administered to 1,050 high school seniors in nine high schools across the nation in 1978, and five shorter questionnaires administered to large national samples of high school seniors.},
	number = {4},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Herzog, A. Regula and Bachman, Jerald G.},
	year = {1981},
	note = {Publisher: [Oxford University Press, American Association for Public Opinion Research]},
	pages = {549--559},
}

@inproceedings{bogen_effect_1996,
	address = {Salt Lake City},
	title = {The {Effect} of {Questionnaire} {Length} on {Response} {Rates} - {A} {Review} of the {Literature}},
	url = {http://www.asasrms.org/Proceedings/y1996f.html},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	author = {Bogen, Karen},
	year = {1996},
	pages = {1020--1025},
}

@article{west_can_2018,
	title = {Can conversational interviewing improve survey response quality without increasing interviewer effects?},
	volume = {181},
	issn = {1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12255},
	doi = {10.1111/rssa.12255},
	abstract = {Several studies have shown that conversational interviewing (CI) reduces response bias for complex survey questions relative to standardized interviewing. However, no studies have addressed concerns about whether CI increases intra-interviewer correlations (IICs) in the responses collected, which could negatively impact the overall quality of survey estimates. The paper reports the results of an experimental investigation addressing this question in a national face-to-face survey. We find that CI improves response quality, as in previous studies, without substantially or frequently increasing IICs. Furthermore, any slight increases in the IICs do not offset the reduced bias in survey estimates engendered by CI.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {West, Brady T. and Conrad, Frederick G. and Kreuter, Frauke and Mittereder, Felicitas},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12255},
	pages = {181--203},
}

@article{schwarz_self-reports_1999,
	title = {Self-reports: {How} the questions shape the answers},
	volume = {54},
	issn = {1935-990X},
	shorttitle = {Self-reports},
	doi = {10.1037/0003-066X.54.2.93},
	abstract = {Self-reports of behaviors and attitudes are strongly influenced by features of the research instrument, including question wording, format, and context. Recent research has addressed the underlying cognitive and communicative processes, which are systematic and increasingly well-understood. The author reviews what has been learned, focusing on issues of question comprehension, behavioral frequency reports, and the emergence of context effect in attitude measurement. The accumulating knowledge about the processes underlying self-reports promises to improve the questionnaire design and data quality. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {American Psychologist},
	author = {Schwarz, Norbert},
	year = {1999},
	note = {Place: US
Publisher: American Psychological Association},
	pages = {93--105},
}

@misc{schnell_methodische_2020,
	address = {Berlin},
	title = {Methodische {Probleme} und {Lösungen} für eine epidemiologische {Corona}-{Forschung}},
	language = {de},
	author = {Schnell, Rainer and Smid, Menno},
	month = apr,
	year = {2020},
}

@article{schnaudt_european_2014,
	title = {The {European} {Social} {Survey}: {Contents}, {Design}, and {Research} {Potential}},
	volume = {134},
	issn = {1439-121X},
	shorttitle = {The {European} {Social} {Survey}},
	url = {http://ejournals.duncker-humblot.de/doi/abs/10.3790/schm.134.4.487},
	doi = {10.3790/schm.134.4.487},
	language = {en},
	number = {4},
	urldate = {2021-10-22},
	journal = {Schmollers Jahrbuch},
	author = {Schnaudt,, Christian and Weinhardt,, Michael and Fitzgerald,, Rory and Liebig, Stefan},
	month = dec,
	year = {2014},
	pages = {487--506},
}

@article{saris_correction_2016,
	title = {Correction for {Measurement} {Errors} in {Survey} {Research}: {Necessary} and {Possible}},
	volume = {127},
	issn = {0303-8300, 1573-0921},
	shorttitle = {Correction for {Measurement} {Errors} in {Survey} {Research}},
	url = {http://link.springer.com/10.1007/s11205-015-1002-x},
	doi = {10.1007/s11205-015-1002-x},
	abstract = {Survey research is the most frequently used data collection method in many disciplines. Nearly, everybody agrees that such data contain serious measurement errors. However, only few researchers try to correct for them. If the measurement errors in the variables vary, the comparison of the sizes of effects of these variables on each other will be wrong. If the sizes of the measurement errors are different across countries, crossnational comparisons of relationships between variables cannot be made. There is ample evidence for these differences in measurement errors across variables, methods and countries (Saris and Gallhofer in Design, evaluation and analysis of questionnaires for survey. Wiley, Hoboken, 2007; Oberski in Measurement errors in comparative surveys. PhD thesis, University of Tilburg, 2011). Therefore, correction for measurement errors is essential for the social sciences. The correction for measurement errors can be made in a simple way, but it requires that the sizes of the error variances are known for all observed variables. Many experiments are carried out to determine the quality of questions. The relationship between the quality and the characteristics of the questions has been studied. Because this relationship is rather strong, one can also predict the quality of new questions. A program SQP has been developed to predict the quality of questions. Using this program, the quality of the questions (complement of error variance) can be obtained for nearly all questions measuring subjective concepts. For objective variables, other research needs to be used (e.g., Alwin in Margins of error: a study of reliability in survey measurement. Wiley, Hoboken, 2007). Using these two sources of information, making correction for measurement error in survey research is possible. We illustrate here that correction for measurement errors can and should be performed.},
	language = {en},
	number = {3},
	urldate = {2021-10-22},
	journal = {Social Indicators Research},
	author = {Saris, Willem E. and Revilla, Melanie},
	month = jul,
	year = {2016},
	pages = {1005--1020},
}

@article{sengupta_simple_2021,
	title = {Simple {Surveys}: {Response} {Retrieval} {Inspired} by {Recommendation} {Systems}},
	volume = {39},
	issn = {0894-4393},
	shorttitle = {Simple {Surveys}},
	url = {https://doi.org/10.1177/0894439319848374},
	doi = {10.1177/0894439319848374},
	abstract = {In the last decade, the use of simple rating and comparison surveys has proliferated on social and digital media platforms to fuel recommendations. These simple surveys and their extrapolation with machine learning algorithms such as matrix factorization shed light on user preferences over large and growing pools of items such as movies, songs, and ads. Social scientists also have a long history of measuring perceptions, preferences, and opinions, typically often over smaller, discrete item sets with exhaustive rating or ranking surveys. This article introduces simple surveys for social science application. We ran experiments to compare the predictive accuracy of both individual and aggregate comparative assessments using four types of simple surveys—pairwise comparisons (PCs) and ratings on 2, 5, and continuous point scales in three contexts—perceived safety of Google Street View images, likability of artwork, and hilarity of animal GIFs. Across contexts, we find that continuous scale ratings best predict individual assessments but consume the most time and cognitive effort. Binary choice surveys are quick and best predict aggregate assessments, useful for collective decision tasks, but poorly predict personalized preferences, for which they are currently used by Netflix to recommend movies. PCs, by contrast, successfully predict personal assessments but poorly predict aggregate assessments despite being widely used to crowdsource ideas and collective preferences. We also demonstrate how findings from these surveys can be visualized in a low-dimensional space to reveal distinct respondent interpretations of questions asked in each context. We conclude by reflecting on differences between sparse, incomplete “simple surveys” and their traditional survey counterparts in terms of efficiency, information elicited, and settings in which knowing less about more may be critical for social science.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Social Science Computer Review},
	author = {Sengupta, Nandana and Srebro, Nati and Evans, James},
	month = feb,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {105--129},
}

@article{presser_methods_2004-1,
	title = {Methods for {Testing} and {Evaluating} {Survey} {Questions}},
	volume = {68},
	issn = {0033-362X},
	url = {https://doi.org/10.1093/poq/nfh008},
	doi = {10.1093/poq/nfh008},
	abstract = {An examination of survey pretesting reveals a paradox. On the one hand, pretesting is the only way to evaluate in advance whether a questionnaire causes problems for interviewers or respondents. Consequently, both elementary textbooks and experienced researchers declare pretesting indispensable. On the other hand, most textbooks offer minimal, if any, guidance about pretesting methods, and published survey reports usually provide no information about whether questionnaires were pretested and, if so, how, and with what results. Moreover, until recently there was relatively little methodological research on pretesting. Thus pretesting’s universally acknowledged importance has been honored more in the breach than in the practice, and not a great deal is known about many aspects of pretesting, including the extent to which pretests serve their intended purpose and lead to improved questionnaires.},
	number = {1},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Presser, Stanley and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Rothgeb, Jennifer M. and Singer, Eleanor},
	month = mar,
	year = {2004},
	pages = {109--130},
}

@article{pforr_are_2015,
	title = {Are {Incentive} {Effects} on {Response} {Rates} and {Nonresponse} {Bias} in {Large}-scale, {Face}-to-face {Surveys} {Generalizable} to {Germany}? {Evidence} from {Ten} {Experiments}},
	volume = {79},
	issn = {0033-362X, 1537-5331},
	shorttitle = {Are {Incentive} {Effects} on {Response} {Rates} and {Nonresponse} {Bias} in {Large}-scale, {Face}-to-face {Surveys} {Generalizable} to {Germany}?},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfv014},
	doi = {10.1093/poq/nfv014},
	abstract = {In survey research, a consensus has grown regarding the effectiveness of incentives encouraging survey participation across different survey modes and target populations. Most of this research has been based on surveys from the United States, whereas few studies have provided evidence that these results can be generalized to other contexts. This paper is the first to present comprehensive information concerning the effects of incentives on response rates and nonresponse bias across large-scale surveys in Germany. The context could be viewed as a critical test for incentive effects because Germany’s population is among the most survey-critical in the world, with very low response rates. Our results suggest positive incentive effects on response rates and patterns of effects that are similar to those in previous research: The effect increased with the monetary value of the incentive; cash incentives affected response propensity more strongly than lottery tickets do; and prepaid incentives could be more cost effective than conditional incentives. We found mixed results for the effects of incentives on nonresponse bias. Regarding large-scale panel surveys, we could not unequivocally confirm that incentives increased response rates in later panel waves.},
	language = {en},
	number = {3},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Pforr, Klaus and Blohm, Michael and Blom, Annelies G. and Erdel, Barbara and Felderer, Barbara and Fräßdorf, Mathis and Hajek, Kristin and Helmschrott, Susanne and Kleinert, Corinna and Koch, Achim and Krieger, Ulrich and Kroh, Martin and Martin, Silke and Saßenroth, Denise and Schmiedeberg, Claudia and Trüdinger, Eva-Maria and Rammstedt, Beatrice},
	year = {2015},
	pages = {740--768},
}

@article{ongena_methods_2006,
	title = {Methods of {Behavior} {Coding} of {Survey} {Interviews}},
	volume = {22},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	language = {en},
	number = {3},
	journal = {Journal of Official Statistics},
	author = {Ongena, Yfke P. and Dijkstra, Wil},
	year = {2006},
	pages = {419--451},
}

@article{meyer_household_2015,
	title = {Household {Surveys} in {Crisis}},
	volume = {29},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.29.4.199},
	doi = {10.1257/jep.29.4.199},
	abstract = {Household surveys, one of the main innovations in social science 
research of the last century, are threatened by declining accuracy due 
to reduced cooperation of respondents. While many indicators of 
survey quality have steadily declined in recent decades, the literature 
has largely emphasized rising nonresponse rates rather than other 
potentially more important dimensions to the problem. We divide the 
problem into rising rates of nonresponse, imputation, and 
measurement error, documenting the rise in each of these threats to 
survey quality over the past three decades. A fundamental problem in 
assessing biases due to these problems in surveys is the lack of a 
benchmark or measure of truth, leading us to focus on the accuracy of 
the reporting of government transfers. We provide evidence from 
aggregate measures of transfer reporting as well as linked microdata. 
We discuss the relative importance of misreporting of program receipt 
and conditional amounts of benefits received, as well as some of the 
conjectured reasons for declining cooperation and for survey errors. 
We end by discussing ways to reduce the impact of the problem 
including the increased use of administrative data and the possibilities 
for combining administrative and survey data.},
	language = {en},
	number = {4},
	urldate = {2021-10-22},
	journal = {Journal of Economic Perspectives},
	author = {Meyer, Bruce D. and Mok, Wallace K. C. and Sullivan, James X.},
	month = nov,
	year = {2015},
	pages = {199--226},
}

@article{mathiowetz_respondent_1998,
	title = {Respondent {Expressions} of {Uncertainty}: {Data} {Source} for {Imputation}},
	volume = {62},
	issn = {0033362X},
	shorttitle = {Respondent {Expressions} of {Uncertainty}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1086/297830},
	doi = {10.1086/297830},
	abstract = {As participants in conversation, we regard remarks such as ‘‘I’m not sure’’ or ‘‘I think’’ as indications that the speaker is unsure of the information he or she is providing. Subsequent discussions or actions based on the conversation often take these qualifying remarks into account. However, in most survey interviews, qualifying remarks are not recorded as part of the response. Replacement of missing data rarely takes into account the fact that the behavioral experience, by the very nature that it was not or could not be reported by the respondent, may differ signiﬁcantly from the behavioral experience of those who could provide a response. This article examines the relationship between response behavior, that is, indications of uncertainty expressed by the respondent, and true behavioral experience. The feasibility of using response behavior as a covariate in imputation models is explored. The ﬁndings suggest that with respect to reporting health care utilization, response behavior is indicative of actual behavior, in this case, the number of outpatient visits. Restricting the pool of donors for imputation models to those respondents who exhibited response behavior similar to those individuals who could not provide a response resulted in an improvement in the estimates over imputation models based solely on substantive characteristics.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Mathiowetz, Nancy A.},
	year = {1998},
	pages = {47},
}

@article{kuha_effect_2018,
	title = {The {Effect} of {Probing} “{Don}’t {Know}” {Responses} on {Measurement} {Quality} and {Nonresponse} in {Surveys}},
	volume = {113},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1323640},
	doi = {10.1080/01621459.2017.1323640},
	abstract = {In survey interviews, “Don’t know” (DK) responses are commonly treated as missing data. One way to reduce the rate of such responses is to probe initial DK answers with a follow-up question designed to encourage respondents to give substantive, non-DK responses. However, such probing can also reduce data quality by introducing additional or differential measurement error. We propose a latent variable model for analyzing the effects of probing on responses to survey questions. The model makes it possible to separate measurement effects of probing from true differences between respondents who do and do not require probing. We analyze new data from an experiment, which compared responses to two multi-item batteries of questions with and without probing. In this study, probing reduced the rate of DK responses by around a half. However, it also had substantial measurement effects, in that probed answers were often weaker measures of constructs of interest than were unprobed answers. These effects were larger for questions on attitudes than for pseudo-knowledge questions on perceptions of external facts. The results provide evidence against the use of probing of “Don’t know” responses, at least for the kinds of items and respondents considered in this study. Supplementary materials for this article are available online.},
	number = {521},
	urldate = {2021-10-22},
	journal = {Journal of the American Statistical Association},
	author = {Kuha, Jouni and Butt, Sarah and Katsikatsou, Myrsini and Skinner, Chris J.},
	month = jan,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2017.1323640},
	pages = {26--40},
}

@article{krosnick_survey_1999,
	title = {Survey {Research}},
	volume = {50},
	url = {https://doi.org/10.1146/annurev.psych.50.1.537},
	doi = {10.1146/annurev.psych.50.1.537},
	abstract = {For the first time in decades, conventional wisdom about survey methodology is being challenged on many fronts. The insights gained can not only help psychologists do their research better but also provide useful insights into the basics of social interaction and cognition. This chapter reviews some of the many recent advances in the literature, including the following: New findings challenge a long-standing prejudice against studies with low response rates; innovative techniques for pretesting questionnaires offer opportunities for improving measurement validity; surprising effects of the verbal labels put on rating scale points have been identified, suggesting optimal approaches to scale labeling; respondents interpret questions on the basis of the norms of everyday conversation, so violations of those conventions introduce error; some measurement error thought to have been attributable to social desirability response bias now appears to be due to other factors instead, thus encouraging different approaches to fixing such problems; and a new theory of satisficing in questionnaire responding offers parsimonious explanations for a range of response patterns long recognized by psycholo-gists and survey researchers but previously not well understood.},
	number = {1},
	urldate = {2021-10-22},
	journal = {Annual Review of Psychology},
	author = {Krosnick, Jon A.},
	year = {1999},
	pmid = {15012463},
	note = {\_eprint: https://doi.org/10.1146/annurev.psych.50.1.537},
	pages = {537--567},
}

@article{kohler_surveys_2007,
	title = {Surveys from inside: {An} assessment of unit nonresponse bias with internal criteria},
	volume = {1},
	copyright = {Copyright (c) 2015 Survey Research Methods},
	issn = {1864-3361},
	shorttitle = {Surveys from inside},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/75},
	doi = {10.18148/srm/2007.v1i2.75},
	abstract = {The article uses the so called “internal criteria of representativeness” to assess the unit nonresponse
bias in five European comparative survey projects. It then goes on investigating several
ideas why unit nonresponse bias might vary between surveys and countries. It is proposed that
unit nonresponse bias is either caused by country characteristics or survey methodology. The
empirical evidence presented speaks more in favour of the latter than of the former. Among
the survey characteristics the features that strengthen the leverage to control interviewers’ behaviour
have top priority},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Survey Research Methods},
	author = {Kohler, Ulrich},
	year = {2007},
	note = {Number: 2},
	pages = {55--67},
}

@article{kalton_developments_2019,
	title = {Developments in {Survey} {Research} over the {Past} 60 {Years}: {A} {Personal} {Perspective}},
	volume = {87},
	issn = {0306-7734, 1751-5823},
	shorttitle = {Developments in {Survey} {Research} over the {Past} 60 {Years}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12287},
	doi = {10.1111/insr.12287},
	abstract = {Many developments have occurred in the practice of survey sampling and survey methodology in the past 60 years or so. These developments have been partly driven by the emergence of computers and the continuous growth in computer power over the years and partly by the increasingly sophisticated demands from the users of survey data. The paper reviews these developments with a main emphasis on survey sampling issues for the design and analysis of social surveys. Design-based inference based on probability samples was the predominant approach in the early years, but over time, that predominance has been eroded by the need to employ model-dependent methods to deal with missing data and to satisfy analysts’ demands for survey estimates that cannot be met with design-based methods. With the continuous decline in response rates that has occurred in recent years, much current research has focused on the use of non-probability samples and data collected from administrative records and web surveys.},
	language = {en},
	number = {S1},
	urldate = {2021-10-22},
	journal = {International Statistical Review},
	author = {Kalton, Graham},
	month = may,
	year = {2019},
	pages = {S10--S30},
}

@article{groves_total_2010,
	title = {Total {Survey} {Error}: {Past}, {Present}, and {Future}},
	volume = {74},
	issn = {0033-362X, 1537-5331},
	shorttitle = {Total {Survey} {Error}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfq065},
	doi = {10.1093/poq/nfq065},
	abstract = {Total survey error’’ is a conceptual framework describing statistical error properties of sample survey statistics. Early in the history of sample surveys, it arose as a tool to focus on implications of various gaps between the conditions under which probability samples yielded unbiased estimates of ﬁnite population parameters and practical situations in implementing survey design. While the framework permits design-based estimates of various error components, many of the design burdens to produce those estimates are large, and in practice most surveys do not implement them. Further, the framework does not incorporate other, nonstatistical, dimensions of quality that are commonly utilized in evaluating statistical information. The importation of new modeling tools brings new promise to measuring total survey error components, but also new challenges. A lasting value of the total survey error framework is at the design stage of a survey, to attempt a balance of costs and various errors. Indeed, this framework is the central organizing structure of the ﬁeld of survey methodology.},
	language = {en},
	number = {5},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Groves, R. M. and Lyberg, L.},
	month = jan,
	year = {2010},
	pages = {849--879},
}

@phdthesis{bach_unintended_2018,
	address = {Mannheim},
	type = {{PhD} {Thesis}},
	title = {Unintended behavioral consequences due to repeated measurement},
	url = {https://madoc.bib.uni-mannheim.de/44314/},
	abstract = {This dissertation deals with the various ways how retrieving information from respondents in surveys may affect their future reporting behavior and/or actual behavior. Such effects, called respondent conditioning, can take various forms. For example, respondents of a panel survey may recall having answered the same questions before and report differently in future waves of the same survey. Similarly, responding to questions about certain behaviors may work as a stimulus that encourages some respondents to change their behavior. I provide a framework for the analysis of such conditioning effects and study their various forms in several social science surveys.},
	language = {Englisch},
	author = {Bach, Ruben L.},
	year = {2018},
}

@article{duan_survey_2007,
	title = {Survey {Conditioning} in {Self}-{Reported} {Mental} {Health} {Service} {Use}: {Randomized} {Comparison} of {Alternative} {Instrument} {Formats}},
	volume = {42},
	issn = {1475-6773},
	shorttitle = {Survey {Conditioning} in {Self}-{Reported} {Mental} {Health} {Service} {Use}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-6773.2006.00618.x},
	doi = {10.1111/j.1475-6773.2006.00618.x},
	abstract = {Objective. To test the effect of survey conditioning (whether observed survey responses are affected by previous experience in the same survey or similar surveys) in a survey instrument used to assess mental health service use. Data Sources. Primary data collected in the National Latino and Asian American Study, a cross-sectional household survey of Latinos and Asian Americans residing in the United States. Study Design. Study participants are randomly assigned to a Traditional Instrument with an interleafed format placing service use questions after detailed questions on disorders, or a Modified Instrument with an ensemble format screening for service use near the beginning of the survey. We hypothesize the ensemble format to be less susceptible to survey conditioning than the interleafed format. We compare self-reported mental health services use measures (overall, aggregate categories, and specific categories) between recipients of the two instruments, using 2 × 2 χ2 tests and logistic regressions that control for key covariates. Data Collection. In-person computer-assisted interviews, conducted in respondent's preferred language (English, Spanish, Mandarin Chinese, Tagalog, or Vietnamese). Principal Findings. Higher service use rates are reported with the Modified Instrument than with the Traditional Instrument for all service use measures; odds ratios range from 1.41 to 3.10, all p-values {\textless}.001. Results are similar across ethnic groups and insensitive to model specification. Conclusions. Survey conditioning biases downward reported mental health service use when the instrument follows an interleafed format. An ensemble format should be used when it is feasible for measures that are susceptible to survey conditioning.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Health Services Research},
	author = {Duan, Naihua and Alegria, Margarita and Canino, Glorisa and McGuire, Thomas G. and Takeuchi, David},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-6773.2006.00618.x},
	pages = {890--907},
}

@article{bais_can_2019,
	title = {Can {Survey} {Item} {Characteristics} {Relevant} to {Measurement} {Error} {Be} {Coded} {Reliably}? {A} {Case} {Study} on 11 {Dutch} {General} {Population} {Surveys}},
	volume = {48},
	issn = {0049-1241},
	shorttitle = {Can {Survey} {Item} {Characteristics} {Relevant} to {Measurement} {Error} {Be} {Coded} {Reliably}?},
	url = {https://doi.org/10.1177/0049124117729692},
	doi = {10.1177/0049124117729692},
	abstract = {Item characteristics can have a significant effect on survey data quality and may be associated with measurement error. Literature on data quality and measurement error is often inconclusive. This could be because item characteristics used for detecting measurement error are not coded unambiguously. In our study, we use a systematic coding procedure with multiple coders to investigate the extent to which the coding of item characteristics could be done reliably. For this purpose, we constructed an item characteristics scheme that is based on typologies of characteristics. High intercoder reliability indicates a clear relation between item characteristic, item content, and measurement error. Our results show that intercoder reliability is often low, especially for item characteristics that are hard to code due to subjectivity. Low intercoder reliability complicates comparisons between studies about item characteristics and measurement error. We give suggestions for coping with low intercoder reliability.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Sociological Methods \& Research},
	author = {Bais, Frank and Schouten, Barry and Lugtig, Peter and Toepoel, Vera and Arends-Tòth, Judit and Douhou, Salima and Kieruj, Natalia and Morren, Mattijn and Vis, Corrie},
	month = may,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {263--295},
}

@techreport{american_economic_association_principles_nodate,
	address = {Nashville},
	title = {Principles of {Economic} {Measurement}},
	url = {https://www.aeaweb.org/content/file?id=6847},
	urldate = {2021-10-22},
	institution = {American Economic Association},
	author = {American Economic Association},
	pages = {2},
}

@article{amaya_total_2020,
	title = {Total {Error} in a {Big} {Data} {World}: {Adapting} the {TSE} {Framework} to {Big} {Data}},
	volume = {8},
	issn = {2325-0984, 2325-0992},
	shorttitle = {Total {Error} in a {Big} {Data} {World}},
	url = {https://academic.oup.com/jssam/article/8/1/89/5728725},
	doi = {10.1093/jssam/smz056},
	abstract = {Abstract
            While Big Data offers a potentially less expensive, less burdensome, and more timely alternative to survey data for producing a variety of statistics, it is not without error. The AAPOR Task Force on Big Data and others have called for researchers to evaluate the quality of Big Data using an approach similar to the total survey error (TSE) framework. However, differences in the construction of, access to, and overall data structure between survey data and Big Data make application of TSE difficult. In this article, we seek to develop the Total Error Framework (TEF), an extension of the TSE framework, to be (1) more inclusive and applicable to many types of Big Data, (2) comprehensive in that it considers “total” error, and (3) unified in that it allows researchers to compare errors in Big Data to errors in survey data. After outlining this framework, we then illustrate an application of TEF by comparing error in housing unit area (square footage) estimates collected in a survey (the 2015 Residential Energy Consumption Survey [RECS]) to those estimates found in three Big Data databases (Zillow.com, Acxiom, and CoreLogic).},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Amaya, Ashley and Biemer, Paul P and Kinyon, David},
	month = feb,
	year = {2020},
	pages = {89--119},
}

@book{smith_meeting_2020,
	title = {A {Meeting} {Place} and {More}…: {A} {History} of the {American} {Association} for {Public}  {Opinion} {Research}},
	shorttitle = {{AAPOR} history book},
	url = {https://www.aapor.org/About-Us/History/A-Meeting-Place-and-More.aspx},
	publisher = {AAPOR},
	editor = {Smith, Tom W.},
	year = {2020},
}

@techreport{federal_committee_on_statistical_methodology_evaluating_2016,
	address = {Washington, D.C.},
	type = {Statistical {Policy} {Working} {Paper}},
	title = {Evaluating survey questions: {An} inventory of methods},
	url = {https://nces.ed.gov/FCSM/policy_wp.asp},
	language = {en},
	number = {47},
	institution = {Office of Management and Budget},
	author = {Federal Committee on Statistical Methodology},
	collaborator = {Subcommittee on Questionnaire Evaluation Methods},
	month = jan,
	year = {2016},
	pages = {51},
}

@misc{noauthor_fcsm_nodate,
	title = {{FCSM} {\textbar} {Federal} {Committee} on {Statistical} {Methodology} {Statistical} {Standards} and {Guidelines}},
	url = {https://nces.ed.gov/FCSM/policies.asp},
	urldate = {2021-10-22},
}

@techreport{ratswd_quality_standards_working_group_quality_2015,
	address = {Berlin},
	type = {{RatSWD} {Working} {Papers}},
	title = {Quality {Standards} for the {Development}, {Application}, and {Evaluation} of {Measurement} {Instruments} in {Social} {Science} {Survey} {Research}},
	url = {https://www.konsortswd.de/en/latest/publication/wp245-2015/},
	abstract = {Since its establishment in 2004, the German Data Forum (RatSWD), which is funded by the German Federal Ministry of Science and Research (BMBF), has been advising the federal government and the governments of the federal states (Länder) on issues relating to the
expansion and improvement of the research infrastructure for the empirical social, behavioral, and economic sciences (SBE). In late 2010, the RatSWD addressed the question of how the quality of survey instruments in the social and economic sciences, and especially in social and economic survey research, could be controlled and assured. At a meeting on November 9, 2012, the RatSWD therefore decided to set up a Quality Assurance of Survey Instruments Working Group under the leadership of Professor Beatrice Rammstedt. The establishment of the working group was prompted in particular by the desire to define quality standards in order to assure and optimize the quality of survey instruments. Hence, the working group made the formulation of these standards its primary objective. They are presented in this paper.},
	language = {en},
	number = {245/2015},
	institution = {RatSWD},
	author = {RatSWD Quality Standards Working Group},
	month = feb,
	year = {2015},
	pages = {37},
}

@article{smit_suggestive_1997,
	title = {Suggestive {Interviewer} {Behaviour} in {Surveys}: {An} {Experimental} {Study}},
	volume = {13},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	language = {en},
	number = {1},
	journal = {Journal of Official Statistics},
	author = {Smit, Johannes H. and Dijkstra, Wil and Zouwen},
	year = {1997},
	pages = {19--28},
}

@article{schober_does_1997,
	title = {Does {Conversational} {Interviewing} {Reduce} {Survey} {Measurement} {Error}?},
	volume = {61},
	issn = {0033-362X},
	url = {https://doi.org/10.1086/297818},
	doi = {10.1086/297818},
	number = {4},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Schober, Michael F. and Conrad, Frederick G.},
	month = feb,
	year = {1997},
	pages = {576--602},
}

@incollection{gubrium_standardization_2001,
	address = {2455 Teller Road, Thousand Oaks California 91320 United States of America},
	title = {Standardization and {Interaction} in the {Survey} {Interview}},
	isbn = {978-0-7619-1951-3 978-1-4129-7358-8},
	url = {http://methods.sagepub.com/book/handbook-of-interview-research/d34.xml},
	urldate = {2021-10-22},
	booktitle = {Handbook of {Interview} {Research}},
	publisher = {SAGE Publications, Inc.},
	author = {Schaeffer, Nora Cate and Maynard, Douglas W.},
	collaborator = {Gubrium, Jaber and Holstein, James},
	year = {2001},
	doi = {10.4135/9781412973588.n34},
	pages = {576--601},
}

@phdthesis{moerman_probing_2010,
	address = {Amsterdam},
	title = {Probing behaviour in open interviews: a field experiment on the effects of probing tactics on quality and content of the received information},
	shorttitle = {Probing behaviour in open interviews},
	language = {English},
	school = {Vrije Universiteit Amsterdam},
	author = {Moerman, Gerben},
	year = {2010},
	note = {ISBN: 9789086594436
OCLC: 645943278},
}

@article{mangione_question_1992,
	title = {Question {Characteristics} and {Interviewer} {Effects}},
	volume = {8},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	language = {en},
	number = {3},
	journal = {Journal of Official Statistics},
	author = {Mangione, Thomas W. and Fowler, Floyd J. and Louis, Thomas A.},
	year = {1992},
	pages = {293--307},
}

@article{wager_estimation_2018,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	volume = {113},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839},
	doi = {10.1080/01621459.2017.1319839},
	abstract = {Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman’s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	language = {en},
	number = {523},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {Wager, Stefan and Athey, Susan},
	month = jul,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2017.1319839},
	pages = {1228--1242},
}

@article{kim_you_2020,
	title = {Do you know what you do for a living? {Occupational} coding mismatches between coders in the {Korean} {General} {Social} {Survey}},
	volume = {70},
	issn = {0276-5624},
	shorttitle = {Do you know what you do for a living?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0276562419302434},
	doi = {10.1016/j.rssm.2019.100467},
	abstract = {Occupation has been a workhorse variable in stratification and mobility studies in sociology. Despite the widespread usage of the occupation variable, its measurement issues and the related theoretical and practical implications have been little studied in the sociology community. In this study, we review the previous studies on the measurement issue of occupation and examine the match rates of occupational codes between independent coders using the 2010∼2018 Korean General Social Survey. Our results show that the reliability of occupational classification is not as high as previously assumed in sociology. The mismatch rates between two independent coders ranged from 31.3 \% for the broad 1-digit occupational groupings to 50.9 \% for the detailed 4-digit occupational groupings. We argue that this relatively low reliability is inherently tied to the socially constructive aspects of occupation. The factors associated with the mismatch are also explored. The implications of these findings for sociological research in stratification and mobility are discussed.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Research in Social Stratification and Mobility},
	author = {Kim, ChangHwan and Kim, Jibum and Ban, Mihee},
	month = dec,
	year = {2020},
	pages = {100467},
}

@article{holland_influence_2009,
	title = {The {Influence} of {Topic} {Interest} and {Interactive} {Probing} on {Responses} to {Open}-{Ended} {Questions} in {Web} {Surveys}},
	volume = {27},
	issn = {0894-4393},
	url = {https://doi.org/10.1177/0894439308327481},
	doi = {10.1177/0894439308327481},
	abstract = {Web surveys offer new opportunities for achieving high-quality responses to open-ended questions because the interactive nature of the web allows questions to be tailored to individual respondents. This article explores how respondents' level of interest in the topic of the question can influence whether they provide a response and the quality of their answers. In addition, we examine whether an interactive follow-up probe, asked after people submit their initial response to the open-ended question, can improve the quality of responses. We find that respondents' interest in the question topic significantly affects the responses to open-ended questions, and interactively probing responses to open-ended questions in web surveys can improve the quality of responses for some respondents, particularly for those very interested in the question topic. Nonresponse remains a significant problem for open-ended questions; we found high item nonresponse rates for the initial question and even higher nonresponse to the probe, especially for those less interested in the topic of the question. Consequently, interactive probing should only be used for a few key open-ended questions within a survey where high-quality responses are essential and may be more effective for respondents who are already motivated to provide a response.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Social Science Computer Review},
	author = {Holland, Jennifer L. and Christian, Leah Melani},
	year = {2009},
	note = {Publisher: SAGE Publications Inc},
	pages = {196--212},
}

@article{grimmer_text_2013,
	title = {Text as data: {The} promise and pitfalls of automatic content analysis methods for political texts},
	volume = {21},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Text as {Data}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700013401/type/journal_article},
	doi = {10.1093/pan/mps028},
	abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Stewart, Brandon M.},
	year = {2013},
	pages = {267--297},
}

@book{presser_methods_2004,
	address = {Hoboken, NJ},
	series = {Wiley series in survey methodology},
	title = {Methods for testing and evaluating survey questionnaires},
	isbn = {978-0-471-45841-8},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	year = {2004},
}

@incollection{presser_response_2004,
	address = {Hoboken, NJ, USA},
	title = {Response {Latency} and ({Para}){Linguistic} {Expressions} as {Indicators} of {Response} {Error}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch7},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Draisma, Stasja and Dijkstra, Wil},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch7},
	pages = {131--147},
}

@article{conrad_promoting_2005,
	title = {Promoting {Uniform} {Question} {Understanding} in {Today}'s and {Tomorrow}'s {Surveys}},
	volume = {21},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	language = {en},
	number = {2},
	journal = {Journal of Official Statistics},
	author = {Conrad, Frederick G. and Schober, Michael F.},
	year = {2005},
	pages = {215--231},
}

@article{billiet_improvement_1988,
	title = {Improvement of the quality of responses to factual survey questions by interviewer training},
	volume = {52},
	issn = {0033-362X},
	url = {https://doi.org/10.1086/269094},
	doi = {10.1086/269094},
	abstract = {In this study we analyze the effects of interviewer training on the quality of responses. Data from a field experiment reveal a number of significant differences between trained and untrained interviewers in terms of nonresponse and the amount of information obtained. For some questions, an interaction effect occurred between interviewer training, the use of tape recorders, and the kind of responses. The effects of training appear to be dependent on the structure of the questions. Since such effects occur primarily with questions that assume a great deal of interviewer activity, it is suggested that they are indeed due to the application of the techniques acquired by training, namely giving instructions, probing, and feedback. A brief analysis of the interviewer-respondent interaction supports this interpretation.},
	number = {2},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Billiet, Jacques and Loosveldt, Geert},
	month = jan,
	year = {1988},
	pages = {190--211},
}

@article{houtkoop-steenstra_probing_1996,
	title = {Probing behaviour of interviewers in the standardised semi-open research interview},
	volume = {30},
	issn = {1573-7845},
	url = {https://doi.org/10.1007/BF00153988},
	doi = {10.1007/BF00153988},
	abstract = {In the semi-open research interview questions are presented to the respondent without mentioning the answer categories to be used for coding. So the respondents are free to formulate the answers in their own way, but interviewers are supposed to record the respondents' answers in the precoded answer categories. This presupposes that respondents will give answers that match these categories. However, the formulation of interview questions does not always make clear which type of answer is expected from the respondent. When the question is met with an inadequate answer the interviewer uses certain interactional devices in order to generate an adequate answer. This may easily result into answers the validity of which may be doubted.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Quality and Quantity},
	author = {Houtkoop-Steenstra, Hanneke},
	month = may,
	year = {1996},
	pages = {205--230},
}

@article{dillman_survey_2005,
	title = {Survey {Mode} as a {Source} of {Instability} in {Responses} across {Surveys}},
	volume = {17},
	issn = {1525-822X},
	url = {https://doi.org/10.1177/1525822X04269550},
	doi = {10.1177/1525822X04269550},
	abstract = {Changes in survey mode for conducting panel surveys may contribute significantly to survey error. This article explores the causes and consequences of such changes in survey mode. The authors describe how and why the choice of survey mode often causes changes to be made to the wording of questions, as well as the reasons that identically worded questions often produce different answers when administered through different modes. The authors provide evidence that answers may changeas a result of different visual layouts for otherwise identical questions and suggest ways to keep measurement the same despite changes in survey mode.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Field Methods},
	author = {Dillman, Don A. and Christian, Leah Melani},
	month = feb,
	year = {2005},
	note = {Publisher: SAGE Publications Inc},
	pages = {30--52},
}

@article{conrad_clarifying_2000,
	title = {Clarifying {Question} {Meaning} in a {Household} {Telephone} {Survey}},
	volume = {64},
	issn = {0033-362X},
	url = {https://doi.org/10.1086/316757},
	doi = {10.1086/316757},
	abstract = {This study contrasts two interviewing techniques that reflect different tacit assumptions about communication. In one, strictly standardized interviewing, interviewers leave the interpretation of questions up to respondents. In the other, conversational interviewing, interviewers say whatever it takes to make sure that questions are interpreted uniformly and as intended. Respondents from a national sample were interviewed twice. Each time they were asked the same factual questions from ongoing government surveys, five about housing and five about recent purchases. The first interview was strictly standardized; the second was standardized for half the respondents and conversational for the others. Respondents in a second conversational interview answered differently than in the first interview more often, and for reasons that conformed more closely to official definitions, than respondents in a second standardized interview. This suggests that conversational interviewing improved comprehension, although it also lengthened interviews. We conclude that respondents in a national sample may misinterpret certain questions frequently enough to compromise data quality and that such misunderstandings cannot easily be eliminated by pretesting and rewording questions alone. More standardized comprehension may require less standardized interviewer behavior.},
	number = {1},
	urldate = {2021-10-22},
	journal = {Public Opinion Quarterly},
	author = {Conrad, Frederick G. and Schober, Michael F.},
	month = mar,
	year = {2000},
	pages = {1--28},
}

@article{beatty_understanding_1995,
	title = {Understanding the {Standardized}/{Non}-{Standardized} {Interviewing} {Controversy}},
	volume = {11},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	language = {en},
	number = {2},
	journal = {Journal of Official Statistics},
	author = {Beatty, Paul},
	year = {1995},
	pages = {147--160},
}

@article{suchman_interactional_1990,
	title = {Interactional {Troubles} in {Face}-to-{Face} {Survey} {Interviews}},
	volume = {85},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10475331},
	doi = {10.1080/01621459.1990.10475331},
	language = {en},
	number = {409},
	urldate = {2021-10-22},
	journal = {Journal of the American Statistical Association},
	author = {Suchman, Lucy and Jordan, Brigitte},
	month = mar,
	year = {1990},
	pages = {232--241},
}

@article{yan_fast_2008,
	title = {Fast times and easy questions: the effects of age, experience and question complexity on web survey response times},
	volume = {22},
	issn = {08884080, 10990720},
	shorttitle = {Fast times and easy questions},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/acp.1331},
	doi = {10.1002/acp.1331},
	abstract = {This paper examines response times (RT) to survey questions. Cognitive psychologists have long relied on response times to study cognitive processes but response time data have only recently received attention from survey researchers. To date, most of the studies on response times in surveys have treated response times either as a predictor or as a proxy measure for some other variable (e.g. attitude accessibility) of greater interest. As a result, response times have not been the main focus of the research. Focusing on the nature and determinants of response times, this paper examines variables that affect how long it takes respondents to answer questions in web surveys. Using the survey response model proposed by Tourangeau, Rips, and Rasinski (2000), we include both item-level characteristics and respondent-level characteristics thought to affect response times in a two-level cross-classiﬁed model. Much of the time spent on processing the questions involves reading and interpreting them. The results from the cross-classiﬁed models indicate that response times are affected by question characteristics such as the total number of clauses and the number of words per clause that probably reﬂect reading times. In addition, response times are also affected by the number and type of answer categories, and the location of the question within the questionnaire, as well as respondent characteristics such as age, education and experience with the Internet and with completing web surveys. Aside from their ﬁxed effects on response times, respondent-level characteristics (such as age) are shown to vary randomly over questions and effects of question-level characteristics (such as types of questions and response scales) vary randomly over respondents. Copyright \# 2007 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2021-10-22},
	journal = {Applied Cognitive Psychology},
	author = {Yan, Ting and Tourangeau, Roger},
	month = jan,
	year = {2008},
	pages = {51--68},
}

@article{breiman_stacked_1996,
	title = {Stacked regressions},
	volume = {24},
	issn = {1573-0565},
	url = {http://link.springer.com/10.1007/BF00117832},
	doi = {10.1023/A:1018046112532},
	abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes and in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992).},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {1996},
	pages = {49--64},
}

@techreport{hedlin_developing_2005,
	title = {Developing methods for assessing perceived response burden : a joint report of {Statistics} {Sweden}, {Statistics} {Norway} and the {UK} {Office} for {National} {Statistics}},
	shorttitle = {Developing methods for assessing perceived response burden},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:su:diva-85789},
	abstract = {Developing methods for assessing perceived response burden : a joint report of Statistics Sweden, Statistics Norway and the UK Office for National Statistics},
	language = {eng},
	urldate = {2021-10-22},
	author = {Hedlin, Dan and Dale, Trine and Haraldsen, Gustav and Jones, Jaqui},
	month = feb,
	year = {2005},
	pages = {210},
}

@article{helgeson_role_1994,
	title = {The role of affective and cognitive decision-making processes during questionnaire completion},
	volume = {11},
	issn = {1520-6793},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mar.4220110506},
	doi = {10.1002/mar.4220110506},
	abstract = {The use of affective and cognitive decision-making processes during questionnaire completion is examined through protocol analysis. Characteristics of the questionnaire, such as length and extremity of the scale anchor cues, resulted in more affective processing. Respondent-related variables, such as product familiarity, attitude toward survey research, and sex, had little influence on decision making. Finally, affective decision making influenced the outcomes of the questionnaire response analysis by being associated with more within-item variance, less decision time, and lower ratings. The implications of the results to applied and theoretical research are discussed. © 1994 John Wiley \& Sons, Inc.},
	language = {en},
	number = {5},
	urldate = {2021-10-22},
	journal = {Psychology \& Marketing},
	author = {Helgeson, James G. and Ursic, Michael L.},
	year = {1994},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.4220110506},
	pages = {493--510},
}

@inproceedings{jones_response_2012,
	address = {Montreal},
	title = {Response {Burden}: {Introductory} {Overview} {Lecture}},
	url = {https://ww2.amstat.org/meetings/ices/2012/proceedings/ICESIV_TOC.pdf},
	abstract = {Internationally each year survey organisations send out millions of questionnaires to businesses selected to participate in business surveys. Some businesses may only receive one survey questionnaire a year whilst others receive numerous questionnaires for different surveys and for the different periods that the surveys are conducted e.g. monthly, quarterly and annual. From the perspective of the businesses these survey requests are an irritant that incur them costs and from which they often perceive receiving no benefits – this can impact on how or if they respond. From the survey organisations perspective survey responses are vital to the quality of the final statistical outputs. A trade off therefore exists between the survey demands being placed on businesses and the need for quality statistics. Added complications come from the importance of large businesses in terms of their representation in numerous surveys and their importance (by weight) to the final statistical outputs; and increasing demands for data.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Establishment} {Surveys}},
	publisher = {American Statistical Association},
	author = {Jones, Jacqui},
	year = {2012},
	pages = {19},
}

@incollection{yan_response_2020,
	title = {Response {Burden}: {What} {Is} {It} and {What} {Predicts} {It}?},
	isbn = {978-1-119-26368-5},
	shorttitle = {Response {Burden}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119263685.ch8},
	abstract = {Survey researchers and practitioners have long expressed concerns about the burden that surveys place on respondents. A review of the response burden literature reveals that burden is loosely defined and measured in many different ways. Too often researchers employ, as a proxy measure of burden, properties of surveys or tasks that are believed to impose response burden (e.g. interview length), respondents' attitudes and beliefs toward surveys (such as interest in survey), and respondents' behaviors such as willingness to be interviewed again. These very different measurements of response burden reflect the lack of and the need for a well-developed conceptual framework for burden and lead to equivocal findings from empirical research. In this chapter, we posit a path model that explicitly looks into the direct and indirect effects on burden of survey features, respondent characteristics, and respondents' perceptions of the survey , in hopes of shedding light on which factors (or combination of factors) are most likely to result in response burden. We found that low motivation, difficult recall tasks, challenging survey effort, and negative perception of the survey all directly contribute to respondents' perception of burden. In addition, low motivation contributes to response burden through its effect on respondents' negative perception of the survey. By contrast, the direct effects of challenging survey effort are canceled out by the indirect effects through respondents' negative perception of the survey. We also found that, even though parts of the measurement models are not equivalent across modes of data collection, the structural relationships are invariant by mode. Implications of these findings are discussed.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Advances in {Questionnaire} {Design}, {Development}, {Evaluation} and {Testing}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Yan, Ting and Fricker, Scott and Tsai, Shirley},
	year = {2020},
	doi = {10.1002/9781119263685.ch8},
	note = {Section: 8
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119263685.ch8},
	pages = {193--212},
}

@misc{mackeben_modus-experiment_2021,
	address = {Nürnberg},
	type = {{IAB} survey meetup},
	title = {Das {Modus}-{Experiment} im {LPP}: {Hintergründe} und {Ergebnisse} des {Modus}-{Experimentes} in der 4. {Erhebungswelle} der {LPP}-{Beschäftigtenbefragung}},
	author = {Mackeben, Jan},
	month = apr,
	year = {2021},
}

@article{toomet_sample_2008,
	title = {Sample {Selection} {Models} in {R}: {Package} {sampleSelection}},
	volume = {27},
	copyright = {Copyright (c) 2007 Ott Toomet, Arne Henningsen},
	issn = {1548-7660},
	shorttitle = {Sample {Selection} {Models} in {R}},
	url = {https://doi.org/10.18637/jss.v027.i07},
	doi = {10.18637/jss.v027.i07},
	abstract = {This paper describes the implementation of Heckman-type sample selection models in R. We discuss the sample selection problem as well as the Heckman solution to it, and argue that although modern econometrics has non- and semiparametric estimation methods in its toolbox, Heckman models are an integral part of the modern applied analysis and econometrics syllabus. We describe the implementation of these models in the package sampleSelection and illustrate the usage of the package on several simulation and real data examples. Our examples demonstrate the effect of exclusion restrictions, identification at infinity and misspecification. We argue that the package can be used both in applied research and teaching.},
	language = {en},
	urldate = {2021-10-22},
	journal = {Journal of Statistical Software},
	author = {Toomet, Ott and Henningsen, Arne},
	month = jul,
	year = {2008},
	pages = {1--23},
}

@incollection{heckman_common_1976,
	title = {The {Common} {Structure} of {Statistical} {Models} of {Truncation}, {Sample} {Selection} and {Limited} {Dependent} {Variables} and a {Simple} {Estimator} for {Such} {Models}},
	url = {http://www.nber.org/chapters/c10491},
	booktitle = {Annals of {Economic} and {Social} {Measurement}, {Volume} 5, number 4},
	publisher = {NBER},
	author = {Heckman, James J.},
	month = oct,
	year = {1976},
	pages = {475--492},
}

@misc{zins_maximum_2020,
	title = {Maximum {Likelihood} und {Stichprobentheorie}},
	author = {Zins, Stefan},
	month = oct,
	year = {2020},
}

@article{little_missing-data_1988,
	title = {Missing-{Data} {Adjustments} in {Large} {Surveys}},
	volume = {6},
	issn = {0735-0015},
	url = {https://www.jstor.org/stable/1391878},
	doi = {10.2307/1391878},
	abstract = {Useful properties of a general-purpose imputation method for numerical data are suggested and discussed in the context of several large government surveys. Imputation based on predictive mean matching is proposed as a useful extension of methods in existing practice, and versions of the method are presented for unit nonresponse and item nonresponse with a general pattern of missingness. Extensions of the method to provide multiple imputations are also considered. Pros and cons of weighting adjustments are discussed, and weighting-based analogs to predictive mean matching are outlined.},
	number = {3},
	urldate = {2021-10-22},
	journal = {Journal of Business \& Economic Statistics},
	author = {Little, Roderick J. A.},
	year = {1988},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {287--296},
}

@article{little_survey_1986,
	title = {Survey {Nonresponse} {Adjustments} for {Estimates} of {Means}},
	volume = {54},
	issn = {03067734},
	url = {https://www.jstor.org/stable/1403140?origin=crossref},
	doi = {10.2307/1403140},
	abstract = {Theoretical properties of nonresponse adjustments based on adjustment cells are studied, for estimates of means for the whole population and in subclasses that cut across adjustment cells. Three forms of adjustment are considered: weighting by the inverse response rate within cells, post-stratification on known population cell counts, and mean imputation within adjustment cells. Two dimensions of covariate information x are distinguished as particularly useful for reducing nonresponse bias: the response propensity f(x) and the conditional mean {\textasciicircum}(x) of the outcome variable y given x. Weighting within adjustment cells based on {\textasciicircum}f(x) controls bias, but not necessarily variance. Imputation within adjustment cells based on {\textasciicircum}(x) controls bias and variance.},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {International Statistical Review},
	author = {Little, Roderick J. A.},
	month = aug,
	year = {1986},
	pages = {139--157},
}

@article{kalton_weighting_2003,
	title = {Weighting methods},
	volume = {19},
	url = {https://www.scb.se/dokumentation/statistiska-metoder/JOS-archive/},
	abstract = {Calibration; generalised regression estimation; poststratification; raking; trimming weights.},
	language = {en},
	number = {2},
	journal = {Journal of Official Statistics},
	author = {Kalton, Graham and Flores-Cervantes, Ismael},
	year = {2003},
	pages = {81--97},
}

@phdthesis{speidel_imputation_2018,
	type = {Text.{PhDThesis}},
	title = {Imputation for hierarchical datasets and responses in intervals},
	url = {https://edoc.ub.uni-muenchen.de/23251/},
	language = {de},
	urldate = {2021-10-22},
	school = {Ludwig-Maximilians-Universität München},
	author = {Speidel, Matthias},
	month = nov,
	year = {2018},
}

@inproceedings{wisniewski_dear_2016,
	address = {San Jose California USA},
	title = {Dear {Diary}: {Teens} {Reflect} on {Their} {Weekly} {Online} {Risk} {Experiences}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Dear {Diary}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858317},
	doi = {10.1145/2858036.2858317},
	abstract = {In our study, 68 teens spend two months reflecting on their weekly online experiences and report 207 separate risk events involving information breaches, online harassment, sexual solicitations, and exposure to explicit content. We conduct a structured, qualitative analysis to characterize the salient dimensions of their risk experiences, such as severity, level of agency, coping strategies, and whether the teens felt like the situation had been resolved. Overall, we found that teens can potentially benefit from lower risk online situations, which allow them to develop crucial interpersonal skills, such as boundary setting, conflict resolution, and empathy. We can also use the dimensions of risk described in this paper to identify potentially harmful risk trajectories before they become high-risk situations. Our end goal is to find a way to empower and protect teens so that they can benefit from online engagement.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wisniewski, Pamela and Xu, Heng and Rosson, Mary Beth and Perkins, Daniel F. and Carroll, John M.},
	month = may,
	year = {2016},
	pages = {3919--3930},
}

@inproceedings{moser_technology_2016,
	address = {San Jose California USA},
	title = {Technology at the {Table}: {Attitudes} about {Mobile} {Phone} {Use} at {Mealtimes}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Technology at the {Table}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858357},
	doi = {10.1145/2858036.2858357},
	abstract = {Mealtimes are a cherished part of everyday life around the world. Often centered on family, friends, or special occasions, sharing meals is a practice embedded with traditions and values. However, as mobile phone adoption becomes increasingly pervasive, tensions emerge about how appropriate it is to use personal devices while sharing a meal with others. Furthermore, while personal devices have been designed to support awareness for the individual user (e.g., notifications), little is known about how to support shared awareness in acceptability in social settings such as meals. In order to understand attitudes about mobile phone use during shared mealtimes, we conducted an online survey with 1,163 English-speaking participants. We find that attitudes about mobile phone use at meals differ depending on the particular phone activity and on who at the meal is engaged in that activity, children versus adults. We also show that three major factors impact participants’ attitudes: 1) their own mobile phone use; 2) their age; and 3) whether a child is present at the meal. We discuss the potential for incorporating social awareness features into mobile phone systems to ease tensions around conflicting mealtime behaviors and attitudes.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Moser, Carol and Schoenebeck, Sarita Y. and Reinecke, Katharina},
	month = may,
	year = {2016},
	pages = {1881--1892},
}

@inproceedings{mitra_comparing_2015,
	address = {Seoul Republic of Korea},
	title = {Comparing {Person}- and {Process}-centric {Strategies} for {Obtaining} {Quality} {Data} on {Amazon} {Mechanical} {Turk}},
	isbn = {978-1-4503-3145-6},
	url = {https://dl.acm.org/doi/10.1145/2702123.2702553},
	doi = {10.1145/2702123.2702553},
	abstract = {In the past half-decade, Amazon Mechanical Turk has radically changed the way many scholars do research. The availability of a massive, distributed, anonymous crowd of individuals willing to perform general human-intelligence micro-tasks for micro-payments is a valuable resource for researchers and practitioners. This paper addresses the challenges of obtaining quality annotations for subjective judgment oriented tasks of varying difficulty. We design and conduct a large, controlled experiment (N=68,000) to measure the efficacy of selected strategies for obtaining high quality data annotations from non-experts. Our results point to the advantages of person-oriented strategies over process-oriented strategies. Specifically, we find that screening workers for requisite cognitive aptitudes and providing training in qualitative coding techniques is quite effective, significantly outperforming control and baseline conditions. Interestingly, such strategies can improve coder annotation accuracy above and beyond common benchmark strategies such as Bayesian Truth Serum (BTS).},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Mitra, Tanushree and Hutto, C.J. and Gilbert, Eric},
	month = apr,
	year = {2015},
	pages = {1345--1354},
}

@inproceedings{lyu_measuring_2015,
	address = {Seoul Republic of Korea},
	title = {Measuring {Photoplethysmogram}-{Based} {Stress}-{Induced} {Vascular} {Response} {Index} to {Assess} {Cognitive} {Load} and {Stress}},
	isbn = {978-1-4503-3145-6},
	url = {https://dl.acm.org/doi/10.1145/2702123.2702399},
	doi = {10.1145/2702123.2702399},
	abstract = {Quantitative assessment for cognitive load and mental stress is very important in optimizing human-computer system designs to improve performance and efficiency. Traditional physiological measures, such as heart rate variation (HRV), blood pressure and electrodermal activity (EDA), are widely used but still have limitations in sensitivity, reliability and usability. In this study, we propose a novel photoplethysmogram-based stress induced vascular index (sVRI) to measure cognitive load and stress. We also provide the basic methodology and detailed algorithm framework. We employed a classic experiment with three levels of task difficulty and three stages of testing period to verify the new measure. Compared with the blood pressure, heart rate and HRV components recorded simultaneously, the sVRI reached the same level of significance on the effect of task difficulty/period as the most significant other measure. Our findings showed sVRI’s potential as a sensitive, reliable and usable parameter.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lyu, Yongqiang and Luo, Xiaomin and Zhou, Jun and Yu, Chun and Miao, Congcong and Wang, Tong and Shi, Yuanchun and Kameyama, Ken-ichi},
	month = apr,
	year = {2015},
	pages = {857--866},
}

@inproceedings{kay_unequal_2015,
	address = {Seoul Republic of Korea},
	title = {Unequal {Representation} and {Gender} {Stereotypes} in {Image} {Search} {Results} for {Occupations}},
	isbn = {978-1-4503-3145-6},
	url = {https://dl.acm.org/doi/10.1145/2702123.2702520},
	doi = {10.1145/2702123.2702520},
	abstract = {Information environments have the power to affect people’s perceptions and behaviors. In this paper, we present the results of studies in which we characterize the gender bias present in image search results for a variety of occupations. We experimentally evaluate the effects of bias in image search results on the images people choose to represent those careers and on people’s perceptions of the prevalence of men and women in each occupation. We find evidence for both stereotype exaggeration and systematic underrepresentation of women in search results. We also find that people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people’s perceptions about real-world distributions. We also discuss tensions between desires for high-quality results and broader societal goals for equality of representation in this space.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
	month = apr,
	year = {2015},
	pages = {3819--3828},
}

@inproceedings{hamidi_gender_2018,
	address = {Montreal QC Canada},
	title = {Gender {Recognition} or {Gender} {Reductionism}?: {The} {Social} {Implications} of {Embedded} {Gender} {Recognition} {Systems}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {Gender {Recognition} or {Gender} {Reductionism}?},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173582},
	doi = {10.1145/3173574.3173582},
	abstract = {Automatic Gender Recognition (AGR) refers to various computational methods that aim to identify an individual’s gender by extracting and analyzing features from images, video, and/or audio. Applications of AGR are increasingly being explored in domains such as security, marketing, and social robotics. However, little is known about stakeholders’ perceptions and attitudes towards AGR and how this technology might disproportionately affect vulnerable communities. To begin to address these gaps, we interviewed 13 transgender individuals, including three transgender technology designers, about their perceptions and attitudes towards AGR. We found that transgender individuals have overwhelmingly negative attitudes towards AGR and fundamentally question whether it can accurately recognize such a subjective aspect of their identity. They raised concerns about privacy and potential harms that can result from being incorrectly gendered, or misgendered, by technology. We present a series of recommendations on how to accommodate gender diversity when designing new digital systems.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hamidi, Foad and Scheuerman, Morgan Klaus and Branham, Stacy M.},
	month = apr,
	year = {2018},
	pages = {1--13},
}

@misc{noauthor_understanding_2020,
	title = {Understanding {People}: {Theory}, {Concepts}, {Methods}},
	url = {https://chi2020.acm.org/authors/papers/selecting-a-subcommittee/#Understanding-People-Theory-Concepts-Methods},
	urldate = {2021-10-22},
	year = {2020},
}

@article{hollenbach_zusammenhange_2014,
	title = {Zusammenhänge zwischen {Pflanzenartenzusammen}-setzung, {Phytodiversität} und {Bodenvariablen} in {Nieder}-mooren und verwandten {Habitattypen} {Luxemburgs}},
	copyright = {Creative Commons License - CC BY-NC-ND 3.0 DE},
	url = {https://www.zobodat.at/pdf/Tuexenia_NS_34_0163-0186.pdf},
	doi = {10.14471/2014.34.018},
	abstract = {In Luxembourg, as a consequence of land use intensification, there are only few relicts of nutrient-poor wetland habitats with a high conservation value left. Up to now, a small-scale spatial analysis of the relationships between soil factors and vegetation has not been published for these relicts. For this reason, we assessed vegetation composition, vegetation structure (90\% percentile of the vegetation height, litter cover), soil variables relevant for plant nutrition (pH value, contents of phosphorous, potassium, calcium, magnesium) and Ellenberg indicator values (EIV-N, EIV-F) in five fen complexes in Luxembourg. The central questions were: Which environmental gradients explain the obvious zonation patterns (core and fringe zones) and which habitat qualities correlate with the number of threatened plant species? Therefore, in total, 74 small-sized quadrats were established in both core and fringe zones and vegetation and soil samples were analyzed. The quadrats differed clearly in soil pH value across study areas (pH range: 3.9–6.5). There were floristic, structural and soil chemical differences between the zones. In most areas, zones differed in diversity (higher in core zones) and productivity (in terms of 90\% percentile and EIV-N higher in fringe zones). Soil chemical variables showed only in single areas differences between zones. The two areas with the lowest and highest pH values, respectively, showed the highest numbers of significantly differing variables between zones. In all areas, EIV-F was similar in the two zones. The diversity of threatened species correlated most frequently with the contents of magnesium in soil (mostly positively) and the 90\% percentile (always negatively). We conclude that different factors might cause the zonation in fen complexes. Options for the management of the threatened areas are discussed.},
	language = {de},
	urldate = {2021-10-22},
	author = {Hollenbach, Hannes and Schneider, Simone and Eichberg, Carsten},
	year = {2014},
	note = {Medium: PDF
Publisher: Floristisch-soziologische Arbeitsgemeinschaft e. V. (FlorSoz)},
}

@article{bercik_intestinal_2011,
	title = {The {Intestinal} {Microbiota} {Affect} {Central} {Levels} of {Brain}-{Derived} {Neurotropic} {Factor} and {Behavior} in {Mice}},
	volume = {141},
	issn = {00165085},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S001650851100607X},
	doi = {10.1053/j.gastro.2011.04.052},
	language = {en},
	number = {2},
	urldate = {2021-10-22},
	journal = {Gastroenterology},
	author = {Bercik, Premysl and Denou, Emmanuel and Collins, Josh and Jackson, Wendy and Lu, Jun and Jury, Jennifer and Deng, Yikang and Blennerhassett, Patricia and Macri, Joseph and McCoy, Kathy D. and Verdu, Elena F. and Collins, Stephen M.},
	month = aug,
	year = {2011},
	pages = {599--609.e3},
}

@article{penaloza_mechanics_2019,
	title = {The {Mechanics} of {Treatment}-effect {Estimate} {Bias} for {Nonexperimental} {Data}},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124119852375},
	doi = {10.1177/0049124119852375},
	abstract = {To measure “treatment” effects, social science researchers typically rely on nonexperimental data. In education, school and teacher effects on students are often measured through value-added models (VAMs) that are not fully understood. We propose a framework that relates to the education production function in its most flexible form and connects with the basic VAMs without using untenable assumptions. We illustrate how, due to measurement error (ME), cross-group imbalances created by nonrandom group assignment cause correlations that drive the models’ treatment-effect estimate bias. We derive formulas to calculate bias and rank the models and show that no model is better in all situations. The framework and formulas’ workings are verified and illustrated via simulation. We also evaluate the performance of latent variable/errors-in-variables models that handle ME and study the role of extra covariates including lags of the outcome.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Sociological Methods \& Research},
	author = {Penaloza, Roberto V. and Berends, Mark},
	month = jun,
	year = {2019},
	pages = {004912411985237},
}

@article{klatzky_canonical_1971,
	title = {A {Canonical} {Correlation} {Analysis} of {Occupational} {Mobility}},
	volume = {66},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482212},
	doi = {10.1080/01621459.1971.10482212},
	abstract = {The method of canonical correlation between sets of dummy variables is used to assign weights to the categories of occupational mobility tables, in order to test two assumptions: 1. The socioeconomic distance between occupations determines the correlation across individuals in a mobility table. 2. The relative status of occupations has remained constant over time. The weights which yield the maximum correlation are found to correlate very highly with measures of socioeconomic status, thus validating the first assumption. Since the maximum correlation between fathers' and sons' occupations is obtained by assigning similar weights to both sets, the second assumption is also validated.},
	number = {333},
	urldate = {2021-10-21},
	journal = {Journal of the American Statistical Association},
	author = {Klatzky, Sheila R. and Hodge, Robert W.},
	month = mar,
	year = {1971},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1971.10482212},
	pages = {16--22},
}

@article{clifford_political_1993,
	title = {The {Political} {Consequences} of {Social} {Mobility}},
	volume = {156},
	issn = {09641998},
	url = {https://www.jstor.org/stable/2982860?origin=crossref},
	doi = {10.2307/2982860},
	abstract = {This paper examines the effect of social mobility on voting behaviour by using diagonal reference models. The EM algorithm is used to fit the models. Asymmetrical mobility effects are found on voting for the Labour party, the downwardly mobile from the salariat being more likely to retain the voting patterns of their class of origin than are the upwardly mobile into the salariat. It is suggested that this asymmetry can be explained by countermobility.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Clifford, P. and Heath, A. F.},
	year = {1993},
	pages = {51},
}

@article{rosch_basic_1976,
	title = {Basic objects in natural categories},
	volume = {8},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/001002857690013X},
	doi = {10.1016/0010-0285(76)90013-X},
	abstract = {Categorizations which humans make of the concrete world are not arbitrary but highly determined. In taxonomies of concrete objects, there is one level of abstraction at which the most basic category cuts are made. Basic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. The four experiments of Part I define basic objects by demonstrating that in taxonomies of common concrete nouns in English based on class inclusion, basic objects are the most inclusive categories whose members: (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. The eight experiments of Part II explore implications of the structure of categories. Basic objects are shown to be the most inclusive categories for which a concrete image of the category as a whole can be formed, to be the first categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Cognitive Psychology},
	author = {Rosch, Eleanor and Mervis, Carolyn B and Gray, Wayne D and Johnson, David M and Boyes-Braem, Penny},
	month = jul,
	year = {1976},
	pages = {382--439},
}

@incollection{rosch_principles_1978,
	address = {Hillsdale, NJ},
	title = {Principles of {Categorization}},
	booktitle = {Cognition and {Categorization}},
	publisher = {Erlbaum},
	author = {Rosch, Eleanor},
	editor = {Rosch, Eleanor and Lloyd, B. B.},
	year = {1978},
	keywords = {nn},
	pages = {27--48},
}

@incollection{bartha_analogy_2019,
	edition = {Spring 2019},
	title = {Analogy and {Analogical} {Reasoning}},
	url = {https://plato.stanford.edu/archives/spr2019/entries/reasoning-analogy/},
	abstract = {An analogy is a comparison between two objects, or systems ofobjects, that highlights respects in which they are thought to besimilar. Analogical reasoning is any type of thinking thatrelies upon an analogy. An analogicalargument is an explicit representation of a form ofanalogical reasoning that cites accepted similarities between twosystems to support the conclusion that some further similarityexists. In general (but not always), such arguments belong in thecategory of ampliative reasoning, since their conclusions do notfollow with certainty but are only supported with varying degrees ofstrength. However, the proper characterization of analogical argumentsis subject to debate (see §2.2). , Analogical reasoning is fundamental to human thought and, arguably,to some nonhuman animals as well. Historically, analogicalreasoning has played an important, but sometimes mysterious, role in awide range of problem-solving contexts. The explicit use ofanalogical arguments, since antiquity, has been a distinctive featureof scientific, philosophical and legal reasoning. This articlefocuses primarily on the nature, evaluation and justification ofanalogical arguments. Related topics include metaphor, models in science,and precedent and analogy in legal reasoning.},
	urldate = {2021-10-21},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Bartha, Paul},
	editor = {Zalta, Edward N.},
	year = {2019},
}

@misc{dawid_prequential_2007,
	title = {Prequential {Statistics}},
	url = {http://videolectures.net/mlcued08_dawid_prstat/},
	language = {en},
	urldate = {2021-10-21},
	author = {Dawid, Philip},
	year = {2007},
}

@incollection{dawid_prequential_2006,
	title = {Prequential {Analysis}},
	isbn = {978-0-471-66719-3},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471667196.ess0335.pub2},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Encyclopedia of {Statistical} {Sciences}},
	publisher = {American Cancer Society},
	author = {Dawid, A. P.},
	year = {2006},
	doi = {10.1002/0471667196.ess0335.pub2},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471667196.ess0335.pub2},
}

@article{dawid_conditional_1979,
	title = {Conditional {Independence} in {Statistical} {Theory}},
	volume = {41},
	url = {http://www.jstor.org/stable/2984718},
	abstract = {Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the r6le of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building.},
	language = {en},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dawid, A. P.},
	year = {1979},
	pages = {1--31},
}

@article{sunstein_ethics_2015,
	title = {The {Ethics} of {Nudging}},
	volume = {32},
	issn = {0741-9457},
	url = {https://digitalcommons.law.yale.edu/yjreg/vol32/iss2/6},
	number = {2},
	journal = {Yale Journal on Regulation},
	author = {Sunstein, Cass},
	month = jan,
	year = {2015},
}

@article{manski_communicating_2015,
	title = {Communicating {Uncertainty} in {Official} {Economic} {Statistics}: {An} {Appraisal} {Fifty} {Years} after {Morgenstern}},
	volume = {53},
	issn = {0022-0515},
	shorttitle = {Communicating {Uncertainty} in {Official} {Economic} {Statistics}},
	url = {https://www.aeaweb.org/articles?id=10.1257/jel.53.3.631},
	doi = {10.1257/jel.53.3.631},
	abstract = {Federal statistical agencies in the United States and analogous agencies elsewhere commonly report official economic statistics as point estimates, without accompanying measures of error. Users of the statistics may incorrectly view them as error free or may incorrectly conjecture error magnitudes. This paper discusses strategies to mitigate misinterpretation of official statistics by communicating uncertainty to the public. Sampling error can be measured using established statistical principles. The challenge is to satisfactorily measure the various forms of nonsampling error. I find it useful to distinguish transitory statistical uncertainty, permanent statistical uncertainty, and conceptual uncertainty. I illustrate how each arises as the Bureau of Economic Analysis periodically revises GDP estimates, the Census Bureau generates household income statistics from surveys with nonresponse, and the Bureau of Labor Statistics seasonally adjusts employment statistics. I anchor my discussion of communication of uncertainty in the contribution of Oskar Morgenstern (1963a), who argued forcefully for agency publication of error estimates for official economic statistics. (JEL B22, C82, E23)},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Journal of Economic Literature},
	author = {Manski, Charles F.},
	month = sep,
	year = {2015},
	pages = {631--653},
}

@article{tukey_future_1962,
	title = {The {Future} of {Data} {Analysis}},
	volume = {33},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full},
	doi = {10.1214/aoms/1177704711},
	abstract = {The Annals of Mathematical Statistics},
	number = {1},
	urldate = {2021-10-21},
	journal = {The Annals of Mathematical Statistics},
	author = {Tukey, John W.},
	month = mar,
	year = {1962},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1--67},
}

@article{west_how_2016,
	title = {How {Big} of a {Problem} is {Analytic} {Error} in {Secondary} {Analyses} of {Survey} {Data}?},
	volume = {11},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0158120},
	doi = {10.1371/journal.pone.0158120},
	abstract = {Secondary analyses of survey data collected from large probability samples of persons or establishments further scientific progress in many fields. The complex design features of these samples improve data collection efficiency, but also require analysts to account for these features when conducting analysis. Unfortunately, many secondary analysts from fields outside of statistics, biostatistics, and survey methodology do not have adequate training in this area, and as a result may apply incorrect statistical methods when analyzing these survey data sets. This in turn could lead to the publication of incorrect inferences based on the survey data that effectively negate the resources dedicated to these surveys. In this article, we build on the results of a preliminary meta-analysis of 100 peer-reviewed journal articles presenting analyses of data from a variety of national health surveys, which suggested that analytic errors may be extremely prevalent in these types of investigations. We first perform a meta-analysis of a stratified random sample of 145 additional research products analyzing survey data from the Scientists and Engineers Statistical Data System (SESTAT), which describes features of the U.S. Science and Engineering workforce, and examine trends in the prevalence of analytic error across the decades used to stratify the sample. We once again find that analytic errors appear to be quite prevalent in these studies. Next, we present several example analyses of real SESTAT data, and demonstrate that a failure to perform these analyses correctly can result in substantially biased estimates with standard errors that do not adequately reflect complex sample design features. Collectively, the results of this investigation suggest that reviewers of this type of research need to pay much closer attention to the analytic methods employed by researchers attempting to publish or present secondary analyses of survey data.},
	language = {en},
	number = {6},
	urldate = {2021-10-21},
	journal = {PLOS ONE},
	author = {West, Brady T. and Sakshaug, Joseph W. and Aurelien, Guy Alain S.},
	editor = {Waldorp, Lourens J},
	month = jun,
	year = {2016},
	pages = {e0158120},
}

@incollection{parikh_query_2013,
	title = {Query {Suggestion} with {Large} {Scale} {Data}},
	volume = {31},
	isbn = {978-0-444-53859-8},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444538598000205},
	abstract = {Explosive growth of information has created a challenge for search engines in various domains to handle large scale data. It is still difﬁcult for search engines to fully understand user intent in many scenarios. To address this, most search engines provide assistive features to the user which help the users in managing their information need. Query Suggestion (Related Searches) is one such feature which is an integral part of all search engines. It helps steer users toward queries which are more likely to help them succeed in their search missions. There has been extensive research in this ﬁeld. In this chapter, we discuss state-of-theart techniques to build a Query Suggestion system. Speciﬁcally, we describe the strengths and limitations of different approaches. We also describe salient characteristics of large scale data sets like query corpora and click-stream logs. We walk the reader through the design, implementation, and evaluation of large scale Query Suggestion systems in practice. We show how challenges related to sparsity in the long tail, biases in user data, and speed of algorithms can be tackled at industry scale.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Parikh, Nish and Singh, Gyanit and Sundaresan, Neel},
	year = {2013},
	doi = {10.1016/B978-0-444-53859-8.00020-5},
	pages = {493--518},
}

@incollection{patel_dictionary-based_2013,
	title = {Dictionary-{Based} {Methods} for {Object} {Recognition}∗},
	volume = {31},
	isbn = {978-0-444-53859-8},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444538598000084},
	abstract = {In recent years, Sparse Representation (SR) and Dictionary Learning (DL) have emerged as powerful tools for efﬁcient processing image and video data in nontraditional ways. An area of promise for these theories is object recognition. In this chapter, we review the role of algorithms based on SR and DL for object recognition.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Patel, Vishal M. and Chellappa, Rama},
	year = {2013},
	doi = {10.1016/B978-0-444-53859-8.00008-4},
	pages = {203--225},
}

@article{casella_introduction_1985,
	title = {An {Introduction} to {Empirical} {Bayes} {Data} {Analysis}},
	volume = {39},
	issn = {00031305},
	url = {https://www.jstor.org/stable/2682801?origin=crossref},
	doi = {10.2307/2682801},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {The American Statistician},
	author = {Casella, George},
	month = may,
	year = {1985},
	pages = {83},
}

@article{keuschnigg_analytical_2018,
	title = {Analytical sociology and computational social science},
	volume = {1},
	issn = {2432-2717, 2432-2725},
	url = {http://link.springer.com/10.1007/s42001-017-0006-5},
	doi = {10.1007/s42001-017-0006-5},
	abstract = {Analytical sociology focuses on social interactions among individuals and the hard-to-predict aggregate outcomes they bring about. It seeks to identify generalizable mechanisms giving rise to emergent properties of social systems which, in turn, feed back on individual decision-making. This research program beneﬁts from computational tools such as agent-based simulations, machine learning, and large-scale web experiments, and has considerable overlap with the nascent ﬁeld of computational social science. By providing relevant analytical tools to rigorously address sociology’s core questions, computational social science has the potential to advance sociology in a similar way that the introduction of econometrics advanced economics during the last half century. Computational social scientists from computer science and physics often see as their main task to establish empirical regularities which they view as ‘‘social laws.’’ From the perspective of the social sciences, references to social laws appear unfounded and misplaced, however, and in this article we outline how analytical sociology, with its theory-grounded approach to computational social science, can help to move the ﬁeld forward from mere descriptions and predictions to the explanation of social phenomena.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Journal of Computational Social Science},
	author = {Keuschnigg, Marc and Lovsjö, Niclas and Hedström, Peter},
	month = jan,
	year = {2018},
	pages = {3--14},
}

@article{freedman_statistical_1991,
	title = {Statistical {Models} and {Shoe} {Leather}},
	volume = {21},
	issn = {00811750},
	url = {https://www.jstor.org/stable/270939?origin=crossref},
	doi = {10.2307/270939},
	language = {en},
	urldate = {2021-10-21},
	journal = {Sociological Methodology},
	author = {Freedman, David A.},
	year = {1991},
	pages = {291},
}

@article{chatfield_model_1995,
	title = {Model {Uncertainty}, {Data} {Mining} and {Statistical} {Inference}},
	volume = {158},
	issn = {09641998},
	url = {https://www.jstor.org/stable/10.2307/2983440?origin=crossref},
	doi = {10.2307/2983440},
	abstract = {This paper takes a broad, pragmatic view of statistical inference to include all aspects of model formulation. The estimation of model parameters traditionally assumes that a model has a prespecified known form and takes no account of possible uncertainty regarding the model structure. This implicitly assumes the existence of a 'true' model, which many would regard as a fiction. In practice model uncertainty is a fact of life and likely to be more serious than other sources of uncertainty which have received far more attention from statisticians. This is true whether the model is specified on subject-matter grounds or, as is increasingly the case, when a model is formulated, fitted and checked on the same data set in an iterative, interactive way. Modern computing power allows a large number of models to be considered and data-dependent specification searches have become the norm in many areas of statistics. The term data mining may be used in this context when the analyst goes to great lengths to obtain a good fit. This paper reviews the effects of model uncertainty, such as too narrow prediction intervals, and the non-trivial biases in parameter estimates which can follow data-based modelling. Ways of assessing and overcoming the effects of model uncertainty are discussed, including the use of simulation and resampling methods, a Bayesian model averaging approach and collecting additional data wherever possible. Perhaps the main aim of the paper is to ensure that statisticians are aware of the problems and start addressing the issues even if there is no simple, general theoretical fix.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Chatfield, Chris},
	year = {1995},
	pages = {419},
}

@techreport{bitkom-arbeitskreis_big_data_big_2012,
	address = {Berlin},
	title = {Big {Data} im {Praxiseinsatz} - {Szenarien}, {Beispiele}, {Effekte}},
	url = {https://www.bitkom.org/Bitkom/Publikationen/Leitfaden-Big-Data-im-Praxiseinsatz-Szenarien-Beispiele-Effekte.html},
	language = {de},
	urldate = {2021-10-21},
	institution = {Bundesverband Informationswirtschaft, Telekommunikation und neue Medien e.V.},
	author = {BITKOM-Arbeitskreis Big Data},
	year = {2012},
	pages = {108},
}

@article{athey_economists_2019,
	title = {Economists (and {Economics}) in {Tech} {Companies}},
	volume = {33},
	issn = {0895-3309},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.33.1.209},
	doi = {10.1257/jep.33.1.209},
	abstract = {As technology platforms have created new markets and new ways of acquiring information, economists have come to play an increasingly central role in tech companies—tackling problems such as platform design, strategy, pricing, and policy. Over the past five years, hundreds of PhD economists have accepted positions in the technology sector. In this paper, we explore the skills that PhD economists apply in tech companies, the companies that hire them, the types of problems that economists are currently working on, and the areas of academic research that have emerged in relation to these problems.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Journal of Economic Perspectives},
	author = {Athey, Susan and Luca, Michael},
	month = feb,
	year = {2019},
	pages = {209--230},
}

@misc{salganik_computational_2014,
	title = {Computational {Social} {Science} ({Fall} 2014)},
	url = {http://www.princeton.edu/~mjs3/soc596_fa14.shtml},
	urldate = {2021-10-21},
	author = {Salganik, Matthew J},
	year = {2014},
}

@article{snee_we_2019,
	title = {We {Stand} on the {Shoulders} of {Giants}—{Pioneers} of {Statistics} in {Industry}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1543140},
	doi = {10.1080/00031305.2018.1543140},
	abstract = {Industrial statistics has a rich and proud heritage. The field was initiated in the 1920s and picked up steam in the 1950s with the establishment of industrial statistics groups in several companies including American Cyanamid, DuPont, General Electric, Kodak, Western Electric, Procter and Gamble, General Foods, and 3M. It can be argued that we are in the third generation of the development of the profession. Indeed we are standing on the shoulders of giants. Several pioneering industrial statistics organizations are profiled in this article. The focus is on the roots of the organizations, the people involved and their contributions to their employers, advancements in the field and the development of the profession. Synthesis of this information provides some unique insights into who we are, what we have accomplished, and the needs and opportunities of the future.},
	number = {4},
	urldate = {2021-10-21},
	journal = {The American Statistician},
	author = {Snee, Ronald D.},
	month = oct,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1543140},
	pages = {400--407},
}

@article{friedrich_is_2021,
	title = {Is there a role for statistics in artificial intelligence?},
	issn = {1862-5355},
	url = {https://doi.org/10.1007/s11634-021-00455-6},
	doi = {10.1007/s11634-021-00455-6},
	abstract = {The research on and application of artificial intelligence (AI) has triggered a comprehensive scientific, economic, social and political discussion. Here we argue that statistics, as an interdisciplinary scientific field, plays a substantial role both for the theoretical and practical understanding of AI and for its future development. Statistics might even be considered a core element of AI. With its specialist knowledge of data evaluation, starting with the precise formulation of the research question and passing through a study design stage on to analysis and interpretation of the results, statistics is a natural partner for other disciplines in teaching, research and practice. This paper aims at highlighting the relevance of statistical methodology in the context of AI development. In particular, we discuss contributions of statistics to the field of artificial intelligence concerning methodological development, planning and design of studies, assessment of data quality and data collection, differentiation of causality and associations and assessment of uncertainty in results. Moreover, the paper also discusses the equally necessary and meaningful extensions of curricula in schools and universities to integrate statistical aspects into AI teaching.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Advances in Data Analysis and Classification},
	author = {Friedrich, Sarah and Antes, Gerd and Behr, Sigrid and Binder, Harald and Brannath, Werner and Dumpert, Florian and Ickstadt, Katja and Kestler, Hans A. and Lederer, Johannes and Leitgöb, Heinz and Pauly, Markus and Steland, Ansgar and Wilhelm, Adalbert and Friede, Tim},
	month = aug,
	year = {2021},
}

@article{jarmin_evolving_2019,
	title = {Evolving {Measurement} for an {Evolving} {Economy}: {Thoughts} on 21st {Century} {US} {Economic} {Statistics}},
	volume = {33},
	issn = {0895-3309},
	shorttitle = {Evolving {Measurement} for an {Evolving} {Economy}},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.33.1.165},
	doi = {10.1257/jep.33.1.165},
	abstract = {The system of federal economic statistics developed in the 20th century has served the country well, but the current methods for collecting and disseminating these data products are unsustainable. These statistics are heavily reliant on sample surveys. Recently, however, response rates for both household and business surveys have declined, increasing costs and threatening quality. Existing statistical measures, many developed decades ago, may also miss important aspects of our rapidly evolving economy; moreover, they may not be sufficiently accurate, timely, or granular to meet the increasingly complex needs of data users. Meanwhile, the rapid proliferation of online data and more powerful computation make privacy and confidentiality protections more challenging. There is broad agreement on the need to transform government statistical agencies from the 20th century survey-centric model to a 21st century model that blends structured survey data with administrative and unstructured alternative digital data sources. In this essay, I describe some work underway that hints at what 21st century official economic measurement will look like and offer some preliminary comments on what is needed to get there.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Journal of Economic Perspectives},
	author = {Jarmin, Ron S.},
	month = feb,
	year = {2019},
	pages = {165--184},
}

@article{wright_ranger_2017,
	title = {ranger: {A} {Fast} {Implementation} of {Random} {Forests} for {High} {Dimensional} {Data} in {C}++ and {R}},
	volume = {77},
	copyright = {Copyright (c) 2017 Marvin N. Wright, Andreas Ziegler},
	issn = {1548-7660},
	shorttitle = {ranger},
	url = {https://doi.org/10.18637/jss.v077.i01},
	doi = {10.18637/jss.v077.i01},
	abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Journal of Statistical Software},
	author = {Wright, Marvin N. and Ziegler, Andreas},
	month = mar,
	year = {2017},
	pages = {1--17},
}

@misc{noauthor_christopher_nodate,
	title = {Christopher {Flynn}, {PhD}},
	url = {https://flynn.gg/blog/skgrf-python-bindings-for-grf/},
	urldate = {2021-10-21},
}

@misc{noauthor_honest_nodate,
	title = {Honest ranger},
	url = {https://giters.com/imbs-hl/ranger/issues/547?amp=1},
	abstract = {I was wondering if there is an implementation for an honest ranger? I know there is an honest regression forest in the grf package, but their package does not have support for ordinal/categorical variables that exists in ranger (i.e., re...},
	language = {en},
	urldate = {2021-10-21},
	journal = {Giters},
}

@article{athey_generalized_2019,
	title = {Generalized random forests},
	volume = {47},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full},
	doi = {10.1214/18-AOS1709},
	abstract = {We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
	number = {2},
	urldate = {2021-10-21},
	journal = {The Annals of Statistics},
	author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
	month = apr,
	year = {2019},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1148--1178},
}

@book{groves_innovations_2017,
	address = {Washington, D.C.},
	title = {Innovations in {Federal} {Statistics}: {Combining} {Data} {Sources} {While} {Protecting} {Privacy}},
	isbn = {978-0-309-45428-5},
	shorttitle = {Innovations in {Federal} {Statistics}},
	url = {https://www.nap.edu/catalog/24652},
	urldate = {2021-10-21},
	publisher = {National Academies Press},
	author = {{National Academies of Sciences, Engineering, and Medicine}},
	editor = {Groves, Robert M. and Harris-Kojetin, Brian A.},
	month = mar,
	year = {2017},
	doi = {10.17226/24652},
	note = {Pages: 24652},
}

@article{roberts_structural_2014,
	title = {Structural {Topic} {Models} for {Open}-{Ended} {Survey} {Responses}},
	volume = {58},
	issn = {00925853},
	shorttitle = {Structural {Topic} {Models} for {Open}-{Ended} {Survey} {Responses}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ajps.12103},
	doi = {10.1111/ajps.12103},
	language = {en},
	number = {4},
	urldate = {2021-10-21},
	journal = {American Journal of Political Science},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	month = oct,
	year = {2014},
	pages = {1064--1082},
}

@article{watts_common_2014,
	title = {Common {Sense} and {Sociological} {Explanations}},
	volume = {120},
	issn = {0002-9602, 1537-5390},
	url = {https://www.journals.uchicago.edu/doi/10.1086/678271},
	doi = {10.1086/678271},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {American Journal of Sociology},
	author = {Watts, Duncan J.},
	month = sep,
	year = {2014},
	pages = {313--351},
}

@misc{mount_why_2017,
	title = {Why do {Decision} {Trees} {Work}? – {Win} {Vector} {LLC}},
	url = {https://win-vector.com/2017/01/05/why-do-decision-trees-work/#GordonOlshen1978},
	urldate = {2021-10-21},
	author = {Mount, John},
	month = jan,
	year = {2017},
}

@article{wright_primer_2019,
	title = {A {Primer} on {Visualizations} for {Comparing} {Populations}, {Including} the {Issue} of {Overlapping} {Confidence} {Intervals}},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1392359},
	doi = {10.1080/00031305.2017.1392359},
	abstract = {In comparing a collection of K populations, it is common practice to display in one visualization confidence intervals for the corresponding population parameters θ1, θ2, . . . , θK . For a pair of confidence intervals that do (or do not) overlap, viewers of the visualization are cognitively compelled to declare that there is not (or there is) a statistically significant difference between the two corresponding population parameters. It is generally well known that the method of examining overlap of pairs of confidence intervals should not be used for formal hypothesis testing. However, use of a single visualization with overlapping and nonoverlapping confidence intervals leads many to draw such conclusions, despite the best efforts of statisticians toward preventing users from reaching such conclusions. In this article, we summarize some alternative visualizations from the literature that can be used to properly test equality between a pair of population parameters. We recommend that these visualizations be used with caution to avoid incorrect statistical inference. The methods presented require only that we have K sample estimates and their associated standard errors. We also assume that the sample estimators are independent, unbiased, and normally distributed.},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {The American Statistician},
	author = {Wright, Tommy and Klein, Martin and Wieczorek, Jerzy},
	month = apr,
	year = {2019},
	pages = {165--178},
}

@article{xu_generalized_2017,
	title = {Generalized {Synthetic} {Control} {Method}: {Causal} {Inference} with {Interactive} {Fixed} {Effects} {Models}},
	volume = {25},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Generalized {Synthetic} {Control} {Method}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198716000024/type/journal_article},
	doi = {10.1017/pan.2016.2},
	abstract = {Di erence-in-di erences (DID) is commonly used for causal inference in time-series cross-sectional data. It requires the assumption that the average outcomes of treated and control units would have followed parallel paths in the absence of treatment. In this paper, we propose a method that not only relaxes this o en-violated assumption, but also unifies the synthetic control method (Abadie, Diamond, and Hainmueller ) with linear fixed e ects models under a simple framework, of which DID is a special case. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed e ects model that incorporates unit-specific intercepts interacted with time-varying coe icients. This method has several advantages. First, it allows the treatment to be correlated with unobserved unit and time heterogeneities under reasonable modeling assumptions. Second, it generalizes the synthetic control method to the case of multiple treated units and variable treatment periods, and improves e iciency and interpretability. Third, with a built-in cross-validation procedure, it avoids specification searches and thus is easy to implement. An empirical example of Election Day Registration and voter turnout in the United States is provided.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Political Analysis},
	author = {Xu, Yiqing},
	month = jan,
	year = {2017},
	pages = {57--76},
}

@article{horton_labor_2015,
	title = {Labor {Economists} {Get} {Their} {Microscope}: {Big} {Data} and {Labor} {Market} {Analysis}},
	volume = {3},
	issn = {2167-6461, 2167-647X},
	shorttitle = {Labor {Economists} {Get} {Their} {Microscope}},
	url = {http://www.liebertpub.com/doi/10.1089/big.2015.0017},
	doi = {10.1089/big.2015.0017},
	abstract = {This article describes how the ﬁne-grained data being collected by Internet labor market intermediaries, such as employment websites, online labor markets, and knowledge discussion boards, are providing new research opportunities and directions for the empirical analysis of labor market activity. After discussing these data sources, we examine some of the research opportunities they have created, highlight some examples of existing work that already use these new data sources, and enumerate the challenges associated with the use of these corporate data sources.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Big Data},
	author = {Horton, John J. and Tambe, Prasanna},
	month = sep,
	year = {2015},
	pages = {130--137},
}

@techreport{engstrom_government_2020,
	type = {Report submitted to the {Administrative} {Conference} of the {United} {States}},
	title = {Government by {Algorithm}: {Artificial} {Intelligence} in {Federal} {Administrative} {Agencies}},
	shorttitle = {Government by {Algorithm}},
	url = {https://www.acus.gov/report/government-algorithm-artificial-intelligence-federal-administrative-agencies},
	language = {en},
	number = {Statement \#20, Agency Use of Artificial Intelligence},
	urldate = {2021-10-21},
	institution = {Administrative Conference of the United States},
	author = {Engstrom, David Freeman and Ho, Daniel E. and Sharkey, Catherine M. and Cuéllar, Mariano-Florentino},
	month = feb,
	year = {2020},
	pages = {122},
}

@article{ruczinski_logic_2003,
	title = {Logic {Regression}},
	volume = {12},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/1061860032238},
	doi = {10.1198/1061860032238},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Ruczinski, Ingo and Kooperberg, Charles and LeBlanc, Michael},
	month = sep,
	year = {2003},
	pages = {475--511},
}

@article{shlens_tutorial_2014,
	title = {A {Tutorial} on {Principal} {Component} {Analysis}},
	url = {http://arxiv.org/abs/1404.1100},
	abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:1404.1100 [cs, stat]},
	author = {Shlens, Jonathon},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.1100},
}

@article{li_statistical_2020,
	title = {Statistical {Inference} for {Average} {Treatment} {Effects} {Estimated} by {Synthetic} {Control} {Methods}},
	volume = {115},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2019.1686986},
	doi = {10.1080/01621459.2019.1686986},
	abstract = {The synthetic control (SC) method, a powerful tool for estimating average treatment effects (ATE), is increasingly popular in fields such as statistics, economics, political science, and marketing. The SC is particularly suitable for estimating ATE with a single (or a few) treated unit(s), a fixed number of control units, and large pre and post-treatment periods (which we refer as “long panels”). To date, there has been no formal inference theory for SC ATE estimator with long panels under general conditions. Existing work mostly use placebo tests for inference or some permutation methods when the post-treatment period is small. In this article, we derive the asymptotic distribution of the SC and modified synthetic control (MSC) ATE estimators using projection theory. We show that a properly designed subsampling method can be used to obtain confidence intervals and conduct inference whereas the standard bootstrap cannot. Simulations and an empirical application that examines the effect of opening a physical showroom by an e-tailer demonstrate the usefulness of the MSC method in applications. Supplementary materials for this article are available online.},
	language = {en},
	number = {532},
	urldate = {2021-10-21},
	journal = {Journal of the American Statistical Association},
	author = {Li, Kathleen T.},
	month = oct,
	year = {2020},
	pages = {2068--2083},
}

@article{mustillo_effects_2012,
	title = {The {Effects} of {Auxiliary} {Variables} on {Coefficient} {Bias} and {Efficiency} in {Multiple} {Imputation}},
	volume = {41},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124112452392},
	doi = {10.1177/0049124112452392},
	abstract = {Current research on multiple imputation suggests that including auxiliary variables in the imputation model may increase the accuracy and efficiency of coefficient estimation, yet few studies have actually tested this principle for regression analysis. This article uses data from the 2008 General Social Survey to present results from simulations that vary in three respects: (a) three types of auxiliary variables (variables related to the mechanism of missingness, variables related to the variable/varaibles being imputed, and extraneous variables); (b) three levels of missing data (10 percent, 20 percent, and 30 percent missing); and (c) two assumptions of missing (missing completely at random and missing at random). Results show that the inclusion of any type of auxiliary variable does not appreciably impact the coefficient bias or efficiency in this simulation, regardless of the amount of missing data or the assumption of missing. Hence, the inclusion of auxiliary variables may not be necessary in many analytic situations.},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {Sociological Methods \& Research},
	author = {Mustillo, Sarah},
	month = may,
	year = {2012},
	pages = {335--361},
}

@article{efron_automatic_2020,
	title = {The {Automatic} {Construction} of {Bootstrap} {Confidence} {Intervals}},
	volume = {29},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2020.1714633},
	doi = {10.1080/10618600.2020.1714633},
	abstract = {The standard intervals, for example, θ̂±1.96σ̂ for nominal 95\% two-sided coverage, are familiar and easy to use, but can be of dubious accuracy in regular practice. Bootstrap confidence intervals offer an order of magnitude improvement—from first order to second order accuracy. This article introduces a new set of algorithms that automate the construction of bootstrap intervals, substituting computer power for the need to individually program particular applications. The algorithms are described in terms of the underlying theory that motivates them, along with examples of their application. They are implemented in the R package bcaboot. Supplementary materials for this article are available online.},
	number = {3},
	urldate = {2021-10-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Efron, Bradley and Narasimhan, Balasubramanian},
	month = jul,
	year = {2020},
	pmid = {33727780},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2020.1714633},
	pages = {608--619},
}

@article{neyman_outline_1937,
	title = {Outline of a {Theory} of {Statistical} {Estimation} {Based} on the {Classical} {Theory} of {Probability}},
	volume = {236},
	url = {http://www.jstor.org/stable/91337},
	language = {en},
	number = {767},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences},
	author = {Neyman, J.},
	year = {1937},
	pages = {333--380},
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/63.3.581},
	doi = {10.1093/biomet/63.3.581},
	abstract = {When making sampling distribution inferences about the parameter of the data, 0, it is appropriate to ignore the process that causes missing data if the missing data are 'missing at random' and the observed data are 'observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making directlikelihood or Bayesian inferences about 0, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is 'distinct' from 0. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Biometrika},
	author = {Rubin, Donald B.},
	year = {1976},
	pages = {581--592},
}

@article{stone_cross-validation_1974,
	title = {Cross-validation and multinomial prediction},
	volume = {61},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/61.3.509},
	doi = {10.1093/biomet/61.3.509},
	abstract = {The method of cross-validatorychoice and assessment is applied to prediction of a multinomialindicator. The resultingpredictoris compared with analogous expressions due to Good and Fienberg \& Holland. Some numericalcomparisonsare made.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Biometrika},
	author = {Stone, M.},
	year = {1974},
	pages = {509--515},
}

@phdthesis{schenck_lasso_2013,
	address = {München},
	type = {Seminararbeit},
	title = {Lasso in {LMs} und {GLMs}},
	school = {Ludwig-Maximilians-Universität München},
	author = {Schenck, Patrick},
	month = mar,
	year = {2013},
}

@book{mazarr_emerging_2019,
	title = {The {Emerging} {Risk} of {Virtual} {Societal} {Warfare}: {Social} {Manipulation} in a {Changing} {Information} {Environment}},
	isbn = {978-1-977402-72-1},
	shorttitle = {The {Emerging} {Risk} of {Virtual} {Societal} {Warfare}},
	url = {https://www.rand.org/pubs/research_reports/RR2714.html},
	language = {en},
	urldate = {2021-10-21},
	publisher = {RAND Corporation},
	author = {Mazarr, Michael and Bauer, Ryan and Casey, Abigail and Heintz, Sarah and Matthews, Luke},
	year = {2019},
	doi = {10.7249/RR2714},
}

@inproceedings{pallett_look_2003,
	title = {A look at {NIST}'{S} benchmark {ASR} tests: past, present, and future},
	shorttitle = {A look at {NIST}'{S} benchmark {ASR} tests},
	doi = {10.1109/ASRU.2003.1318488},
	abstract = {This paper discusses the role that the NIST Speech Group has played by coordinating and implementing objective benchmark tests in the automatic speech recognition research community. From 1987, when the first tests were implemented, to the present time, at which the rich transcription concept has been developed, the tests have served to document the state-of-the art. Testing has involved a number of different, but complementary, domains. These test results document the progress of the technology - ever-lower WER with continued attention to the task domain of the era, and several changes of focus to address ever-more challenging tasks.},
	booktitle = {2003 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} ({IEEE} {Cat}. {No}.{03EX721})},
	author = {Pallett, D.S.},
	month = nov,
	year = {2003},
	pages = {483--488},
}

@article{rao_sample_2017,
	title = {Sample survey theory and methods: {Past}, present, and future directions (with discussion)},
	volume = {43},
	issn = {1492-0921},
	url = {http://www.statcan.gc.ca/pub/12-001-x/2017002/article/54888-eng.htm},
	abstract = {We discuss developments in sample survey theory and methods covering the past 100 years. Neyman’s 1934 landmark paper laid the theoretical foundations for the probability sampling approach to inference from survey samples. Classical sampling books by Cochran, Deming, Hansen, Hurwitz and Madow, Sukhatme, and Yates, which appeared in the early 1950s, expanded and elaborated the theory of probability sampling, emphasizing unbiasedness, model free features, and designs that minimize variance for a fixed cost. During the period 1960-1970, theoretical foundations of inference from survey data received attention, with the model-dependent approach generating considerable discussion. Introduction of general purpose statistical software led to the use of such software with survey data, which led to the design of methods specifically for complex survey data. At the same time, weighting methods, such as regression estimation and calibration, became practical and design consistency replaced unbiasedness as the requirement for standard estimators. A bit later, computer-intensive resampling methods also became practical for large scale survey samples. Improved computer power led to more sophisticated imputation for missing data, use of more auxiliary data, some treatment of measurement errors in estimation, and more complex estimation procedures. A notable use of models was in the expanded use of small area estimation. Future directions in research and methods will be influenced by budgets, response rates, timeliness, improved data collection devices, and availability of auxiliary data, some of which will come from “Big Data”. Survey taking will be impacted by changing cultural behavior and by a changing physical-technical environment.},
	language = {en},
	number = {2},
	journal = {Survey Methodology},
	author = {Rao, J.N.K. and Fuller, Wayne A.},
	month = dec,
	year = {2017},
	pages = {145--182},
}

@article{rao_making_2021,
	title = {On {Making} {Valid} {Inferences} by {Integrating} {Data} from {Surveys} and {Other} {Sources}},
	volume = {83},
	url = {https://ideas.repec.org/a/spr/sankhb/v83y2021i1d10.1007_s13571-020-00227-w.html},
	abstract = {Survey samplers have long been using probability samples from one or more sources in conjunction with census and administrative data to make valid and efficient inferences on finite population parameters. This topic has received a lot of attention more recently in the context of data from non-probability samples such as transaction data, web surveys and social media data. In this paper, I will provide a brief overview of probability sampling methods first and then discuss some recent methods, based on models for the non-probability samples, which could lead to useful inferences from a non-probability sample by itself or when combined with a probability sample. I will also explain how big data may be used as predictors in small area estimation, a topic of current interest because of the growing demand for reliable local area statistics.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Sankhya B: The Indian Journal of Statistics},
	author = {Rao, J. N. K.},
	year = {2021},
	note = {Publisher: Springer \& Indian Statistical Institute},
	pages = {242--272},
}

@article{murray_multiple_2018,
	title = {Multiple {Imputation}: {A} {Review} of {Practical} and {Theoretical} {Findings}},
	volume = {33},
	issn = {0883-4237},
	shorttitle = {Multiple {Imputation}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-33/issue-2/Multiple-Imputation-A-Review-of-Practical-and-Theoretical-Findings/10.1214/18-STS644.full},
	doi = {10.1214/18-STS644},
	abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in ﬂexible joint modeling and sequential regression/chained equations/fully conditional speciﬁcation approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {Statistical Science},
	author = {Murray, Jared S.},
	month = may,
	year = {2018},
}

@article{meng_statistical_2018,
	title = {Statistical paradises and paradoxes in big data ({I}): {Law} of large populations, big data paradox, and the 2016 {US} presidential election},
	volume = {12},
	issn = {1932-6157},
	shorttitle = {Statistical paradises and paradoxes in big data ({I})},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-12/issue-2/Statistical-paradises-and-paradoxes-in-big-data-I--Law/10.1214/18-AOAS1161SF.full},
	doi = {10.1214/18-AOAS1161SF},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {The Annals of Applied Statistics},
	author = {Meng, Xiao-Li},
	month = jun,
	year = {2018},
}

@article{kindel_improving_2019,
	title = {Improving {Metadata} {Infrastructure} for {Complex} {Surveys}: {Insights} from the {Fragile} {Families} {Challenge}},
	volume = {5},
	issn = {2378-0231},
	shorttitle = {Improving {Metadata} {Infrastructure} for {Complex} {Surveys}},
	url = {https://doi.org/10.1177/2378023118817378},
	doi = {10.1177/2378023118817378},
	abstract = {Researchers rely on metadata systems to prepare data for analysis. As the complexity of data sets increases and the breadth of data analysis practices grow, existing metadata systems can limit the efficiency and quality of data preparation. This article describes the redesign of a metadata system supporting the Fragile Families and Child Wellbeing Study on the basis of the experiences of participants in the Fragile Families Challenge. The authors demonstrate how treating metadata as data (i.e., releasing comprehensive information about variables in a format amenable to both automated and manual processing) can make the task of data preparation less arduous and less error prone for all types of data analysis. The authors hope that their work will facilitate new applications of machine-learning methods to longitudinal surveys and inspire research on data preparation in the social sciences. The authors have open-sourced the tools they created so that others can use and improve them.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Socius},
	author = {Kindel, Alexander T. and Bansal, Vineet and Catena, Kristin D. and Hartshorne, Thomas H. and Jaeger, Kate and Koffman, Dawn and McLanahan, Sara and Phillips, Maya and Rouhani, Shiva and Vinh, Ryan and Salganik, Matthew J.},
	month = jan,
	year = {2019},
	note = {Publisher: SAGE Publications},
	pages = {2378023118817378},
}

@article{keiding_web-based_2018,
	title = {Web-{Based} {Enrollment} and {Other} {Types} of {Self}-{Selection} in {Surveys} and {Studies}: {Consequences} for {Generalizability}},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	shorttitle = {Web-{Based} {Enrollment} and {Other} {Types} of {Self}-{Selection} in {Surveys} and {Studies}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100127},
	doi = {10.1146/annurev-statistics-031017-100127},
	abstract = {Web-based enrollment in surveys and studies is increasingly attractive as the internet is approaching near-universal coverage and the attitude of respondents toward participation in classical modes of study deteriorates. Followup is also facilitated by the web-based approach. However, the consequent self-selection raises the question of the importance of representativity when attempting to generalize the results of a study beyond the context in which they were obtained, particularly under effect heterogeneity. Our review is divided into three main components: ﬁrst, sample surveys or prevalence studies, assessing the frequency or prevalence of some attitude or disease condition in a population from its frequency in a sample from this population; second, generalization of the results from randomized trials to the population in which they were performed and to other populations; and third, generalization of results from observational studies.},
	language = {en},
	number = {1},
	urldate = {2021-10-21},
	journal = {Annual Review of Statistics and Its Application},
	author = {Keiding, Niels and Louis, Thomas A.},
	month = mar,
	year = {2018},
	pages = {25--47},
}

@article{keiding_perils_2016,
	title = {Perils and potentials of self-selected entry to epidemiological studies and surveys (with discussion)},
	volume = {179},
	issn = {09641998},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssa.12136},
	doi = {10.1111/rssa.12136},
	abstract = {Low front-end cost and rapid accrual make Web-based surveys and enrolment in studies attractive, but participants are often self-selected with little reference to a well-deﬁned study base. Of course, high quality studies must be internally valid (validity of inferences for the sample at hand), but Web-based enrolment reactivates discussion of external validity (generalization of within-study inferences to a target population or context) in epidemiology and clinical trials. Survey research relies on a representative sample produced by a sampling frame, prespeciﬁed sampling process and weighting that maps results to an intended population. In contrast, recent analytical epidemiology has shifted the focus away from survey-type representativity to internal validity in the sample. Against this background, it is a good time for statisticians to take stock of our role and position regarding surveys, observational research in epidemiology and clinical studies. The central issue is whether conditional effects in the sample (the study population) may be transported to desired target populations. Success depends on compatibility of causal structures in study and target populations, and will require subject matter considerations in each concrete case. Statisticians, epidemiologists and survey researchers should work together to increase understanding of these challenges and to develop improved tools to handle them.},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Keiding, Niels and Louis, Thomas A.},
	month = feb,
	year = {2016},
	pages = {319--376},
}

@article{hand_statistical_2018,
	title = {Statistical challenges of administrative and transaction data},
	volume = {181},
	issn = {0964-1998, 1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssa.12315},
	doi = {10.1111/rssa.12315},
	abstract = {Administrative data are becoming increasingly important.They are typically the side effect of some operational exercise and are often seen as having signiﬁcant advantages over alternative sources of data. Although it is true that such data have merits, statisticians should approach the analysis of such data with the same cautious and critical eye as they approach the analysis of data from any other source. The paper identiﬁes some statistical challenges, with the aim of stimulating debate about and improving the analysis of administrative data, and encouraging methodology researchers to explore some of the important statistical problems which arise with such data.},
	language = {en},
	number = {3},
	urldate = {2021-10-21},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Hand, David J.},
	month = jun,
	year = {2018},
	pages = {555--605},
}

@article{ferwerda_kernel-based_2017,
	title = {Kernel-{Based} {Regularized} {Least} {Squares} in {R} ({KRLS}) and {Stata} (krls)},
	volume = {79},
	copyright = {Copyright (c) 2017 Jeremy Ferwerda, Jens Hainmueller, Chad J. Hazlett},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v079.i03},
	doi = {10.18637/jss.v079.i03},
	abstract = {The Stata package krls as well as the R package KRLS implement kernel-based regularized least squares (KRLS), a machine learning method described in Hainmueller and Hazlett (2014) that allows users to tackle regression and classification problems without strong functional form assumptions or a specification search. The flexible KRLS estimator learns the functional form from the data, thereby protecting inferences against misspecification bias. Yet it nevertheless allows for interpretability and inference in ways similar to ordinary regression models. In particular, KRLS provides closed-form estimates for the predicted values, variances, and the pointwise partial derivatives that characterize the marginal effects of each independent variable at each data point in the covariate space. The method is thus a convenient and powerful alternative to ordinary least squares and other generalized linear models for regression-based analyses.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Journal of Statistical Software},
	author = {Ferwerda, Jeremy and Hainmueller, Jens and Hazlett, Chad J.},
	month = jul,
	year = {2017},
	pages = {1--26},
}

@article{hainmueller_kernel_2014,
	title = {Kernel {Regularized} {Least} {Squares}: {Reducing} {Misspecification} {Bias} with a {Flexible} and {Interpretable} {Machine} {Learning} {Approach}},
	volume = {22},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Kernel {Regularized} {Least} {Squares}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700013668/type/journal_article},
	doi = {10.1093/pan/mpt019},
	abstract = {We propose the use of Kernel Regularized Least Squares (KRLS) for social science modeling and inference problems. KRLS borrows from machine learning methods designed to solve regression and classification problems without relying on linearity or additivity assumptions. The method constructs a flexible hypothesis space that uses kernels as radial basis functions and finds the best-fitting surface in this space by minimizing a complexity-penalized least squares problem. We argue that the method is well-suited for social science inquiry because it avoids strong parametric assumptions, yet allows interpretation in ways analogous to generalized linear models while also permitting more complex interpretation to examine nonlinearities, interactions, and heterogeneous effects. We also extend the method in several directions to make it more effective for social inquiry, by (1) deriving estimators for the pointwise marginal effects and their variances, (2) establishing unbiasedness, consistency, and asymptotic normality of the KRLS estimator under fairly general conditions, (3) proposing a simple automated rule for choosing the kernel bandwidth, and (4) providing companion software. We illustrate the use of the method through simulations and empirical examples.},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {Political Analysis},
	author = {Hainmueller, Jens and Hazlett, Chad},
	year = {2014},
	pages = {143--168},
}

@incollection{gudivada_big_2015,
	title = {Big {Data} {Driven} {Natural} {Language} {Processing} {Research} and {Applications}},
	volume = {33},
	isbn = {978-0-444-63492-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444634924000095},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {Gudivada, Venkat N. and Rao, Dhana and Raghavan, Vijay V.},
	year = {2015},
	doi = {10.1016/B978-0-444-63492-4.00009-5},
	pages = {203--238},
}

@article{lenzen_trend_2020,
	title = {Trend in {Wissenschaft}: {Leben} wir in einer unordentlichen {Welt}?},
	issn = {0174-4909},
	shorttitle = {Trend in {Wissenschaft}},
	url = {https://www.faz.net/aktuell/karriere-hochschule/nutzen-von-theorien-leben-wir-in-einer-unordentlichen-welt-16723885.html},
	abstract = {Ohne Theorie würde kein Computer laufen, doch die Wissenschaften rechnen heute lieber mit den Beständen. Was eine Zeit entdeckt, die der großen Theorien müde ist.},
	language = {de},
	urldate = {2021-10-21},
	journal = {FAZ.NET},
	author = {Lenzen, Manuela},
	month = apr,
	year = {2020},
}

@article{donoho_50_2017,
	title = {50 {Years} of {Data} {Science}},
	volume = {26},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1384734},
	doi = {10.1080/10618600.2017.1384734},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science”programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts fieldby-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
	language = {en},
	number = {4},
	urldate = {2021-10-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Donoho, David},
	month = oct,
	year = {2017},
	pages = {745--766},
}

@article{dawes_robust_1979,
	title = {The robust beauty of improper linear models in decision making},
	volume = {34},
	issn = {1935-990X},
	doi = {10.1037/0003-066X.34.7.571},
	abstract = {Proper linear models are those in which predictor variables are given weights such that the resulting linear composite optimally predicts some criterion of interest; examples of proper linear models are standard regression analysis, discriminant function analysis, and ridge regression analysis. Research summarized in P. Meehl's (1954) book on clinical vs statistical prediction and research stimulated in part by that book indicate that when a numerical criterion variable (e.g., graduate GPA) is to be predicted from numerical predictor variables, proper linear models outperform clinical intuition. Improper linear models are those in which the weights of the predictor variables are obtained by some nonoptimal method. The present article presents evidence that even such improper linear models are superior to clinical intuition when predicting a numerical criterion from numerical predictors. In fact, unit (i.e., equal) weighting is quite robust for making such predictions. The application of unit weights to decide what bullet the Denver Police Department should use is described; some technical, psychological, and ethical resistances to using linear models in making social decisions are considered; and arguments that could weaken these resistances are presented. (50 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {7},
	journal = {American Psychologist},
	author = {Dawes, Robyn M.},
	year = {1979},
	note = {Place: US
Publisher: American Psychological Association},
	pages = {571--582},
}

@incollection{wolf_non-probability_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Non-probability {Sampling}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i2461.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Vehovar, Vasja and Toepoel, Vera and Steinmetz, Stephanie},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n22},
	pages = {329--345},
}

@incollection{wolf_designing_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Designing a {Mixed}-{Mode} {Survey}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i1964.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Dillman, Don A. and Edwards, Michelle L.},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n18},
	pages = {255--268},
}

@incollection{wolf_creating_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Creating a {Good} {Question}: {How} to {Use} {Cumulative} {Experience}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	shorttitle = {Creating a {Good} {Question}},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i1838.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Revilla, Melanie and Zavala-Rojas, Diana and Saris, Willem},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n17},
	pages = {236--254},
}

@incollection{wolf_designing_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Designing {Questions} and {Questionnaires}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i1718.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Smyth, Jolene D.},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n16},
	pages = {218--235},
}

@incollection{wolf_when_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {When {Translation} is not {Enough}: {Background} {Variables} in {Comparative} {Surveys}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	shorttitle = {When {Translation} is not {Enough}},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i2141.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Schneider, Silke L. and Joye, Dominique and Wolf, Christof},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n20},
	pages = {288--308},
}

@incollection{wolf_survey_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Survey {Standards}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i502.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Smith, Tom W.},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n2},
	pages = {16--26},
}

@incollection{wolf_ethical_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {The {Ethical} {Issues} of {Survey} and {Market} {Research}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i867.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Joe, Kathy and Raben, Finn and Phillips, Adam},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n7},
	pages = {77--86},
}

@incollection{wolf_observations_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Observations on the {Historical} {Development} of {Polling}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i914.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Frankovic, Kathleen A.},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n8},
	pages = {87--102},
}

@incollection{wolf_record_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Record {Linkage}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i4447.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Schnell, Rainer},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n41},
	pages = {662--669},
}

@incollection{wolf_survey_2016-1,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Survey {Mode} or {Survey} {Modes}?},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i1196.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Leeuw, Edith de and Berzelak, Nejc},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n11},
	pages = {142--156},
}

@incollection{wolf_dealing_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Dealing with {Missing} {Values}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i4087.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Spiess, Martin},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n37},
	pages = {597--612},
}

@incollection{wolf_response_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Response {Styles} in {Surveys}: {Understanding} their {Causes} and {Mitigating} their {Impact} on {Data} {Quality}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	shorttitle = {Response {Styles} in {Surveys}},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i4000.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Roberts, Caroline},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n36},
	pages = {579--596},
}

@incollection{wolf_weighting_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Weighting: {Principles} and {Practicalities}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	shorttitle = {Weighting},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i3223.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Lavalle, Pierre and Beaumont, Jean-Franois},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n30},
	pages = {460--476},
}

@incollection{wolf_analysis_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {Analysis of {Data} from {Stratified} and {Clustered} {Surveys}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology/i3343.xml},
	urldate = {2021-10-20},
	booktitle = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	publisher = {SAGE Publications Ltd},
	author = {Eckman, Stephanie and West, Brady T.},
	collaborator = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893.n31},
	pages = {477--487},
}

@misc{financial_times_chart-doctorvisual-vocabulary_2017,
	title = {chart-doctor/visual-vocabulary at main · {Financial}-{Times}/chart-doctor},
	url = {https://github.com/Financial-Times/chart-doctor/tree/main/visual-vocabulary},
	urldate = {2021-10-20},
	author = {Financial Times},
	year = {2017},
}

@book{berret_teaching_2016,
	title = {Teaching data and computational journalism},
	isbn = {978-0-692-63745-6},
	language = {en},
	author = {Berret, Charles and Phillips, Cheryl and Coll, Steve},
	year = {2016},
	note = {OCLC: 1050870083},
}

@article{tamhane_inference_1978,
	title = {Inference based on regression estimator in double sampling},
	volume = {65},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/65.2.419},
	doi = {10.1093/biomet/65.2.419},
	abstract = {The problem of hypothesistestingusing the regressionestimatorin double sampling is considered.Test proceduresare provided when the covariance matrixbetween the primary and the auxiliary variables is eitherpartiallyknownor completelyunknown.For the latter case a new 'studentized' versionof the regressionestimatoris proposed as a test statistic. The exact null distributionofthis statisticis derivedin a special case. An approximationto the null distributionis derivedin the generalcase and studied by means of the Monte Carlo method. The problem of choosingbetween the double sample regressionestimatorand the singlesample mean estimatoris also discussed.},
	language = {en},
	number = {2},
	urldate = {2021-10-20},
	journal = {Biometrika},
	author = {Tamhane, A. C.},
	month = aug,
	year = {1978},
	pages = {419--427},
}

@article{buche_qualitative_2015,
	title = {Qualitative {Comparative} {Analysis} ({QCA}) in der {Soziologie} – {Perspektiven}, {Potentiale} und {Anwendungsbereiche} / {Qualitative} {Comparative} {Analysis} ({QCA}) and {Sociology} – {Perspectives}, {Potential}, and {Areas} of {Application}},
	volume = {44},
	issn = {2366-0325, 0340-1804},
	url = {https://www.degruyter.com/document/doi/10.1515/zfsoz-2015-0602/html},
	doi = {10.1515/zfsoz-2015-0602},
	abstract = {The sociologist Charles C. Ragin originally introduced Qualitative Comparative Analysis (QCA), which combines ideas of case-oriented research, configurational thinking, and set-theoretical logic. Since then – as further developed by Ragin and others – QCA has become an established tool within social science methodology which uses a set-theoretical approach to analyze social phenomena. This paper aims at highlighting perspectives and potentials which QCA as a (relatively) new method is able to offer to address sociological research questions. In order to do so, seventyseven publications in the field of sociology are reviewed. On this basis a broad overview is given regarding applications, trends, and developments of QCA, on the one hand. On the other, crucial steps within QCA are examined and potential pitfalls discussed by exemplifying practiced practices and best practices.},
	language = {de},
	number = {6},
	urldate = {2021-10-20},
	journal = {Zeitschrift für Soziologie},
	author = {Buche, Jonas and Siewert, Markus B.},
	month = dec,
	year = {2015},
	pages = {386--406},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	issn = {0883-4237},
	shorttitle = {Statistical {Modeling}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full},
	doi = {10.1214/ss/1009213726},
	number = {3},
	urldate = {2021-10-20},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
}

@article{breidt_model-assisted_2017,
	title = {Model-{Assisted} {Survey} {Estimation} with {Modern} {Prediction} {Techniques}},
	volume = {32},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-2/Model-Assisted-Survey-Estimation-with-Modern-Prediction-Techniques/10.1214/16-STS589.full},
	doi = {10.1214/16-STS589},
	abstract = {This paper reviews the design-based, model-assisted approach to using data from a complex survey together with auxiliary information to estimate ﬁnite population parameters. A general recipe for deriving modelassisted estimators is presented and design-based asymptotic analysis for such estimators is reviewed. The recipe allows for a very broad class of prediction methods, with examples from the literature including linear models, linear mixed models, nonparametric regression and machine learning techniques.},
	language = {en},
	number = {2},
	urldate = {2021-10-20},
	journal = {Statistical Science},
	author = {Breidt, F. Jay and Opsomer, Jean D.},
	month = may,
	year = {2017},
}

@article{boll_geschlechtsspezifische_2015,
	title = {Die geschlechtsspezifische {Lohnlücke} in {Deutschland}: {Umfang}, {Ursachen} und {Interpretation}},
	volume = {95},
	issn = {0043-6275, 1613-978X},
	shorttitle = {Die geschlechtsspezifische {Lohnlücke} in {Deutschland}},
	url = {http://link.springer.com/10.1007/s10273-015-1814-y},
	doi = {10.1007/s10273-015-1814-y},
	language = {de},
	number = {4},
	urldate = {2021-10-20},
	journal = {Wirtschaftsdienst},
	author = {Boll, Christina and Leppin, Julian S.},
	month = apr,
	year = {2015},
	pages = {249--254},
}

@incollection{bick_sozialwissenschaftliche_1984,
	address = {Stuttgart},
	series = {Historisch-sozialwissenschaftliche {Forschungen}},
	title = {Sozialwissenschaftliche {Datenkunde} für prozeß-produzierte {Daten}: {Entstehungsbedingungen} und {Indikatorenqualität}},
	isbn = {978-3-608-91127-5},
	url = {http://nbn-resolving.de/urn:nbn:de:0168-ssoar-330744},
	language = {de},
	number = {Bd. 17},
	booktitle = {Sozialforschung und {Verwaltungsdaten}},
	publisher = {Klett-Cotta},
	author = {Bick, Wolfgang and Müller, Paul J.},
	editor = {Bick, Wolfgang and Mann, Reinhard and Müller, Paul J.},
	year = {1984},
	pages = {123--159},
}

@article{berk_assumption_2021,
	title = {Assumption {Lean} {Regression}},
	volume = {75},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2019.1592781},
	doi = {10.1080/00031305.2019.1592781},
	abstract = {It is well known that with observational data, models used in conventional regression analyses are commonly misspecified. Yet in practice, one tends to proceed with interpretations and inferences that rely on correct specification. Even those who invoke Box’s maxim that all models are wrong proceed as if results were generally useful. Misspecification, however, has implications that affect practice. Regression models are approximations to a true response surface and should be treated as such. Accordingly, regression parameters should be interpreted as statistical functionals. Importantly, the regressor distribution affects targets of estimation and regressor randomness affects the sampling variability of estimates. As a consequence, inference should be based on sandwich estimators or the pairs (x–y) bootstrap. Traditional prediction intervals lose their pointwise coverage guarantees, but empirically calibrated intervals can be justified for future populations. We illustrate the key concepts with an empirical application.},
	number = {1},
	urldate = {2021-10-20},
	journal = {The American Statistician},
	author = {Berk, Richard and Buja, Andreas and Brown, Lawrence and George, Edward and Kuchibhotla, Arun Kumar and Su, Weijie and Zhao, Linda},
	month = jan,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2019.1592781},
	pages = {76--84},
}

@article{bauer_probing_2005,
	title = {Probing {Interactions} in {Fixed} and {Multilevel} {Regression}: {Inferential} and {Graphical} {Techniques}},
	volume = {40},
	issn = {0027-3171, 1532-7906},
	shorttitle = {Probing {Interactions} in {Fixed} and {Multilevel} {Regression}},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15327906mbr4003_5},
	doi = {10.1207/s15327906mbr4003_5},
	language = {en},
	number = {3},
	urldate = {2021-10-20},
	journal = {Multivariate Behavioral Research},
	author = {Bauer, Daniel J. and Curran, Patrick J.},
	month = jul,
	year = {2005},
	pages = {373--400},
}

@misc{noauthor_agent-based_nodate,
	title = {Agent-{Based} {Modeling}},
	url = {http://bactra.org/notebooks/agent-based-modeling.html},
	urldate = {2021-10-20},
}

@article{banks_statistical_2021,
	title = {Statistical {Challenges} in {Agent}-{Based} {Modeling}},
	volume = {75},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2021.1900914},
	doi = {10.1080/00031305.2021.1900914},
	abstract = {Agent-based models (ABMs) are popular in many research communities, but few statisticians have contributed to their theoretical development. They are models like any other models we study, but in general, we are still learning how to fit ABMs to data and how to make quantified statements of uncertainty about the outputs of an ABM. ABM validation is also an underdeveloped area that is ripe for new statistical developments. In what follows, we lay out the research space and encourage statisticians to address the many research issues in the ABM ambit.},
	language = {en},
	number = {3},
	urldate = {2021-10-20},
	journal = {The American Statistician},
	author = {Banks, David L. and Hooten, Mevin B.},
	month = jul,
	year = {2021},
	pages = {235--242},
}

@incollection{banbura_now-casting_2013,
	title = {Now-{Casting} and the {Real}-{Time} {Data} {Flow}},
	volume = {2},
	isbn = {978-0-444-53683-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444536839000049},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {Handbook of {Economic} {Forecasting}},
	publisher = {Elsevier},
	author = {Bańbura, Marta and Giannone, Domenico and Modugno, Michele and Reichlin, Lucrezia},
	year = {2013},
	doi = {10.1016/B978-0-444-53683-9.00004-9},
	pages = {195--237},
}

@article{aldrich_keynes_2008,
	title = {Keynes among the {Statisticians}},
	volume = {40},
	issn = {0018-2702, 1527-1919},
	url = {https://read.dukeupress.edu/hope/article/40/2/265-316/38396},
	doi = {10.1215/00182702-2008-003},
	language = {en},
	number = {2},
	urldate = {2021-10-20},
	journal = {History of Political Economy},
	author = {Aldrich, J.},
	month = jan,
	year = {2008},
	pages = {265--316},
}

@article{helland_why_2019,
	title = {Why {Are} the {Prices} {So} {Damn} {High}? {Health}, {Education}, and the {Baumol} {Effect}},
	url = {https://www.mercatus.org/system/files/helland-tabarrok_why-are-the-prices-so-damn-high_v1.pdf},
	language = {en},
	urldate = {2021-10-13},
	journal = {Mercatus Center at George Mason University},
	author = {Helland, Eric and Tabarrok, Alex},
	year = {2019},
	pages = {90},
}

@article{abowd_high_1999,
	title = {High {Wage} {Workers} and {High} {Wage} {Firms}},
	volume = {67},
	issn = {0012-9682, 1468-0262},
	url = {http://doi.wiley.com/10.1111/1468-0262.00020},
	doi = {10.1111/1468-0262.00020},
	abstract = {We study a longitudinal sample of over one million French workers from more than ﬁve hundred thousand employing ﬁrms. We decompose real total annual compensation per worker into components related to observable employee characteristics, personal heterogeneity, ﬁrm heterogeneity, and residual variation. Except for the residual, all components may be correlated in an arbitrary fashion. At the level of the individual, we ﬁnd that person effects, especially those not related to observables like education, are a very important source of wage variation in France. Firm effects, while important, are not as important as person effects. At the level of ﬁrms, we ﬁnd that enterprises that hire high-wage workers are more productive but not more proﬁtable. They are also more capital and high-skilled employee intensive. Enterprises that pay higher wages, controlling for person effects, are more productive and more proﬁtable. They are also more capital intensive but are not more high-skilled labor intensive. We ﬁnd that person effects explain about 90\% of inter-industry wage differentials and about 75\% of the ﬁrm-size wage effect while ﬁrm effects explain relatively little of either differential.},
	language = {en},
	number = {2},
	urldate = {2021-10-20},
	journal = {Econometrica},
	author = {Abowd, John M. and Kramarz, Francis and Margolis, David N.},
	month = mar,
	year = {1999},
	pages = {251--333},
}

@techreport{ng_cs_2014,
	title = {{CS} 229 {Lecture} {Notes}: {Generative} {Learning} algorithms},
	author = {Ng, Andrew},
	year = {2014},
}

@article{arel_deep_2010,
	title = {Deep {Machine} {Learning} - {A} {New} {Frontier} in {Artificial} {Intelligence} {Research} [{Research} {Frontier}]},
	volume = {5},
	issn = {1556-603X},
	url = {http://ieeexplore.ieee.org/document/5605630/},
	doi = {10.1109/MCI.2010.938364},
	language = {en},
	number = {4},
	urldate = {2021-10-20},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Arel, I and Rose, D C and Karnowski, T P},
	month = nov,
	year = {2010},
	pages = {13--18},
}

@misc{noauthor_probability_nodate,
	title = {Probability {Theory}},
	url = {http://bactra.org/notebooks/probability.html},
	urldate = {2021-10-20},
}

@misc{noauthor_sociology_nodate,
	title = {Sociology},
	url = {http://bactra.org/notebooks/sociology.html},
	urldate = {2021-10-20},
}

@misc{noauthor_cross-validation_nodate,
	title = {Cross-{Validation}},
	url = {http://bactra.org/notebooks/cross-validation.html},
	urldate = {2021-10-20},
}

@misc{noauthor_artificial_nodate,
	title = {Artificial {Intelligence}},
	url = {http://bactra.org/notebooks/ai.html},
	urldate = {2021-10-20},
}

@misc{noauthor_ethical_nodate,
	title = {Ethical and {Political} {Issues} in {Data} {Mining}, {Especially} {Unfairness} in {Automated} {Decision} {Making}},
	url = {http://bactra.org/notebooks/ethics-politics-data-mining.html},
	urldate = {2021-10-20},
}

@misc{noauthor_causal_nodate,
	title = {Causal {Discovery} {Algorithms}},
	url = {http://bactra.org/notebooks/causal-discovery-algorithms.html},
	urldate = {2021-10-20},
}

@misc{noauthor_causal_nodate-1,
	title = {Causal {Inference}},
	url = {http://bactra.org/notebooks/causal-inference.html},
	urldate = {2021-10-20},
}

@misc{noauthor_economics_nodate,
	title = {Economics},
	url = {http://bactra.org/notebooks/economics.html},
	urldate = {2021-10-20},
}

@article{shmueli_predictive_2011,
	title = {Predictive {Analytics} in {Information} {Systems} {Research}},
	volume = {35},
	issn = {0276-7783},
	url = {https://www.jstor.org/stable/23042796},
	doi = {10.2307/23042796},
	abstract = {This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.},
	number = {3},
	urldate = {2021-10-20},
	journal = {MIS Quarterly},
	author = {Shmueli, Galit and Koppius, Otto R.},
	year = {2011},
	note = {Publisher: Management Information Systems Research Center, University of Minnesota},
	pages = {553--572},
}

@article{speed_correlation_2011,
	title = {A {Correlation} for the 21st {Century}},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1215894},
	doi = {10.1126/science.1215894},
	language = {en},
	number = {6062},
	urldate = {2021-10-20},
	journal = {Science},
	author = {Speed, Terry},
	month = dec,
	year = {2011},
	pages = {1502--1503},
}

@article{reshef_detecting_2011,
	title = {Detecting {Novel} {Associations} in {Large} {Data} {Sets}},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1205438},
	doi = {10.1126/science.1205438},
	language = {en},
	number = {6062},
	urldate = {2021-10-20},
	journal = {Science},
	author = {Reshef, David N. and Reshef, Yakir A. and Finucane, Hilary K. and Grossman, Sharon R. and McVean, Gilean and Turnbaugh, Peter J. and Lander, Eric S. and Mitzenmacher, Michael and Sabeti, Pardis C.},
	month = dec,
	year = {2011},
	pages = {1518--1524},
}

@article{kinney_equitability_2014,
	title = {Equitability, mutual information, and the maximal information coefficient},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1309933111},
	doi = {10.1073/pnas.1309933111},
	language = {en},
	number = {9},
	urldate = {2021-10-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kinney, Justin B. and Atwal, Gurinder S.},
	month = mar,
	year = {2014},
	pages = {3354--3359},
}

@article{borsboom_attack_2006,
	title = {The attack of the psychometricians},
	volume = {71},
	issn = {0033-3123, 1860-0980},
	url = {https://link.springer.com/10.1007/s11336-006-1447-6},
	doi = {10.1007/s11336-006-1447-6},
	abstract = {This paper analyzes the theoretical, pragmatic, and substantive factors that have hampered the integration between psychology and psychometrics. Theoretical factors include the operationalist mode of thinking which is common throughout psychology, the dominance of classical test theory, and the use of “construct validity” as a catch-all category for a range of challenging psychometric problems. Pragmatic factors include the lack of interest in mathematically precise thinking in psychology, inadequate representation of psychometric modeling in major statistics programs, and insufﬁcient mathematical training in the psychological curriculum. Substantive factors relate to the absence of psychological theories that are sufﬁciently strong to motivate the structure of psychometric models. Following the identiﬁcation of these problems, a number of promising recent developments are discussed, and suggestions are made to further the integration of psychology and psychometrics.},
	language = {en},
	number = {3},
	urldate = {2021-10-20},
	journal = {Psychometrika},
	author = {Borsboom, Denny},
	month = sep,
	year = {2006},
	pages = {425},
}

@article{borsboom_psychometric_2008,
	title = {Psychometric perspectives on diagnostic systems},
	volume = {64},
	issn = {00219762, 10974679},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jclp.20503},
	doi = {10.1002/jclp.20503},
	language = {en},
	number = {9},
	urldate = {2021-10-20},
	journal = {Journal of Clinical Psychology},
	author = {Borsboom, Denny},
	month = sep,
	year = {2008},
	pages = {1089--1108},
}

@techreport{hunermund_causal_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Causal {Machine} {Learning} and {Business} {Decision} {Making}},
	url = {https://papers.ssrn.com/abstract=3867326},
	abstract = {Causal knowledge is critical for strategic and organizational decision-making. By contrast, standard machine learning approaches remain purely correlational and prediction-based, rendering them unsuitable for addressing a wide variety of managerial decision problems. Taking a mixed-methods approach, which relies on multiple sources, including semi-structured interviews with data scientists and senior decision-makers, as well as quantitative survey data, this study argues that causality is a critical boundary condition for the application of machine learning in a business analytical context. It highlights the crucial role of theory in causal inference and offers a new perspective on human-machine interaction for data-augmented decision making.},
	language = {en},
	number = {ID 3867326},
	urldate = {2021-10-20},
	institution = {Social Science Research Network},
	author = {Hünermund, Paul and Kaminski, Jermain and Schmitt, Carla},
	month = may,
	year = {2021},
	doi = {10.2139/ssrn.3867326},
}

@article{hothorn_unbiased_2006,
	title = {Unbiased {Recursive} {Partitioning}: {A} {Conditional} {Inference} {Framework}},
	volume = {15},
	issn = {1061-8600, 1537-2715},
	shorttitle = {Unbiased {Recursive} {Partitioning}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X133933},
	doi = {10.1198/106186006X133933},
	abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to ﬁt such models have been known for a long time: overﬁtting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overﬁtting problem, the variable selection bias still seriously aﬀects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a uniﬁed framework for recursive partitioning which embeds tree-structured regression models into a well deﬁned theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally diﬀerent, conﬁrming the need for an unbiased variable selection. Moreover, it is shown that the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classiﬁcation, node positive breast cancer survival and mammography experience are re-analyzed.},
	language = {en},
	number = {3},
	urldate = {2021-10-16},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
	month = sep,
	year = {2006},
	pages = {651--674},
}

@article{westreich_propensity_2010,
	title = {Propensity score estimation: neural networks, support vector machines, decision trees ({CART}), and meta-classifiers as alternatives to logistic regression},
	volume = {63},
	issn = {1878-5921},
	shorttitle = {Propensity score estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435610001022},
	doi = {10.1016/j.jclinepi.2009.11.020},
	abstract = {OBJECTIVE: Propensity scores for the analysis of observational data are typically estimated using logistic regression. Our objective in this review was to assess machine learning alternatives to logistic regression, which may accomplish the same goals but with fewer assumptions or greater accuracy.
STUDY DESIGN AND SETTING: We identified alternative methods for propensity score estimation and/or classification from the public health, biostatistics, discrete mathematics, and computer science literature, and evaluated these algorithms for applicability to the problem of propensity score estimation, potential advantages over logistic regression, and ease of use.
RESULTS: We identified four techniques as alternatives to logistic regression: neural networks, support vector machines, decision trees (classification and regression trees [CART]), and meta-classifiers (in particular, boosting).
CONCLUSION: Although the assumptions of logistic regression are well understood, those assumptions are frequently ignored. All four alternatives have advantages and disadvantages compared with logistic regression. Boosting (meta-classifiers) and, to a lesser extent, decision trees (particularly CART), appear to be most promising for use in the context of propensity score analysis, but extensive simulation studies are needed to establish their utility in practice.},
	language = {eng},
	number = {8},
	journal = {Journal of Clinical Epidemiology},
	author = {Westreich, Daniel and Lessler, Justin and Funk, Michele Jonsson},
	month = aug,
	year = {2010},
	pmid = {20630332},
	pages = {826--833},
}

@article{pirracchio_improving_2015,
	title = {Improving propensity score estimators' robustness to model misspecification using super learner},
	volume = {181},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwu253},
	doi = {10.1093/aje/kwu253},
	abstract = {The consistency of propensity score (PS) estimators relies on correct specification of the PS model. The PS is frequently estimated using main-effects logistic regression. However, the underlying model assumptions may not hold. Machine learning methods provide an alternative nonparametric approach to PS estimation. In this simulation study, we evaluated the benefit of using Super Learner (SL) for PS estimation. We created 1,000 simulated data sets (n = 500) under 4 different scenarios characterized by various degrees of deviance from the usual main-term logistic regression model for the true PS. We estimated the average treatment effect using PS matching and inverse probability of treatment weighting. The estimators' performance was evaluated in terms of PS prediction accuracy, covariate balance achieved, bias, standard error, coverage, and mean squared error. All methods exhibited adequate overall balancing properties, but in the case of model misspecification, SL performed better for highly unbalanced variables. The SL-based estimators were associated with the smallest bias in cases of severe model misspecification. Our results suggest that use of SL to estimate the PS can improve covariate balance and reduce bias in a meaningful manner in cases of serious model misspecification for treatment assignment.},
	language = {eng},
	number = {2},
	urldate = {2021-10-19},
	journal = {American Journal of Epidemiology},
	author = {Pirracchio, Romain and Petersen, Maya L. and van der Laan, Mark},
	month = jan,
	year = {2015},
	pages = {108--119},
}

@article{stone_cross-validatory_1974,
	title = {Cross-{Validatory} {Choice} and {Assessment} of {Statistical} {Predictions}},
	volume = {36},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984809},
	abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
	language = {en},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Stone, M.},
	year = {1974},
	pages = {111--147},
}

@article{hothorn_lego_2006,
	title = {A {Lego} {System} for {Conditional} {Inference}},
	volume = {60},
	url = {http://www.jstor.org/stable/27643785},
	language = {en},
	number = {3},
	journal = {The American Statistician},
	author = {Hothorn, Torsten and Hornik, Kurt and Wiel, Mark A. Van De and Zeileis, Achim},
	year = {2006},
	pages = {257--263},
}

@article{linden_using_2017,
	title = {Using classification tree analysis to generate propensity score weights},
	volume = {23},
	issn = {1365-2753},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jep.12744},
	doi = {10.1111/jep.12744},
	abstract = {Rationale, aims and objectives In evaluating non-randomized interventions, propensity scores (PS) estimate the probability of assignment to the treatment group given observed characteristics. Machine learning algorithms have been proposed as an alternative to conventional logistic regression for modelling PS in order to avoid limitations of linear methods. We introduce classification tree analysis (CTA) to generate PS which is a “decision-tree”-like classification model that provides accurate, parsimonious decision rules that are easy to display and interpret, reports P values derived via permutation tests, and evaluates cross-generalizability. Method Using empirical data, we identify all statistically valid CTA PS models and then use them to compute strata-specific, observation-level PS weights that are subsequently applied in outcomes analyses. We compare findings obtained using this framework to logistic regression and boosted regression, by evaluating covariate balance using standardized differences, model predictive accuracy, and treatment effect estimates obtained using median regression and a weighted CTA outcomes model. Results While all models had some imbalanced covariates, main-effects logistic regression yielded the lowest average standardized difference, whereas CTA yielded the greatest predictive accuracy. Nevertheless, treatment effect estimates were generally consistent across all models. Conclusions Assessing standardized differences in means as a test of covariate balance is inappropriate for machine learning algorithms that segment the sample into two or more strata. Because the CTA algorithm identifies all statistically valid PS models for a sample, it is most likely to identify a correctly specified PS model, and should be considered as an alternative approach to modeling the PS.},
	language = {en},
	number = {4},
	urldate = {2021-10-19},
	journal = {Journal of Evaluation in Clinical Practice},
	author = {Linden, Ariel and Yarnold, Paul R.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jep.12744},
	pages = {703--712},
}

@article{setoguchi_evaluating_2008,
	title = {Evaluating uses of data mining techniques in propensity score estimation: a simulation study},
	volume = {17},
	issn = {1099-1557},
	shorttitle = {Evaluating uses of data mining techniques in propensity score estimation},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pds.1555},
	doi = {10.1002/pds.1555},
	abstract = {Background In propensity score modeling, it is a standard practice to optimize the prediction of exposure status based on the covariate information. In a simulation study, we examined in what situations analyses based on various types of exposure propensity score (EPS) models using data mining techniques such as recursive partitioning (RP) and neural networks (NN) produce unbiased and/or efficient results. Method We simulated data for a hypothetical cohort study (n = 2000) with a binary exposure/outcome and 10 binary/continuous covariates with seven scenarios differing by non-linear and/or non-additive associations between exposure and covariates. EPS models used logistic regression (LR) (all possible main effects), RP1 (without pruning), RP2 (with pruning), and NN. We calculated c-statistics (C), standard errors (SE), and bias of exposure-effect estimates from outcome models for the PS-matched dataset. Results Data mining techniques yielded higher C than LR (mean: NN, 0.86; RPI, 0.79; RP2, 0.72; and LR, 0.76). SE tended to be greater in models with higher C. Overall bias was small for each strategy, although NN estimates tended to be the least biased. C was not correlated with the magnitude of bias (correlation coefficient [COR] = −0.3, p = 0.1) but increased SE (COR = 0.7, p {\textless} 0.001). Conclusions Effect estimates from EPS models by simple LR were generally robust. NN models generally provided the least numerically biased estimates. C was not associated with the magnitude of bias but was with the increased SE. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2021-10-19},
	journal = {Pharmacoepidemiology and Drug Safety},
	author = {Setoguchi, Soko and Schneeweiss, Sebastian and Brookhart, M. Alan and Glynn, Robert J. and Cook, E. Francis},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pds.1555},
	pages = {546--555},
}

@misc{cefalu_twang_2021,
	title = {twang: {Toolkit} for {Weighting} and {Analysis} of {Nonequivalent} {Groups}},
	copyright = {GPL-3 {\textbar} file LICENSE},
	shorttitle = {twang},
	url = {https://CRAN.R-project.org/package=twang},
	abstract = {Provides functions for propensity score estimating and weighting, nonresponse weighting, and diagnosis of the weights.},
	urldate = {2021-10-19},
	author = {Cefalu, Matthew and Ridgeway, Greg and McCaffrey, Dan and Morral, Andrew and Griffin, Beth Ann and Burgette, {and} Lane},
	month = jul,
	year = {2021},
}

@article{van_der_laan_super_2007,
	title = {Super {Learner}},
	volume = {6},
	issn = {1544-6115, 2194-6302},
	url = {https://www.degruyter.com/document/doi/10.2202/1544-6115.1309/html},
	doi = {10.2202/1544-6115.1309},
	abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
	number = {1},
	urldate = {2021-10-19},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {van der Laan, Mark J. and Polley, Eric C and Hubbard, Alan E.},
	month = jan,
	year = {2007},
}

@article{bacak_principled_2019,
	title = {Principled {Machine} {Learning} {Using} the {Super} {Learner}: {An} {Application} to {Predicting} {Prison} {Violence}},
	volume = {48},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Principled {Machine} {Learning} {Using} the {Super} {Learner}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124117747301},
	doi = {10.1177/0049124117747301},
	abstract = {A rapidly growing number of algorithms are available to researchers who apply statistical or machine learning methods to answer social science research questions. The unique advantages and limitations of each algorithm are relatively well known, but it is not possible to know in advance which algorithm is best suited for the particular research question and the data set at hand. Typically, researchers end up choosing, in a largely arbitrary fashion, one or a handful of algorithms. In this article, we present the Super Learner—a powerful new approach to statistical learning that leverages a variety of data-adaptive methods, such as random forests and spline regression, and systematically chooses the one, or a weighted combination of many, that produces the best forecasts. We illustrate the use of the Super Learner by predicting violence among inmates from the 2005 Census of State and Federal Adult Correctional Facilities. Over the past 40 years, mass incarceration has drastically weakened prisons’ capacities to ensure inmate safety, yet we know little about the characteristics of prisons related to inmate victimization. We discuss the value of the Super Learner in social science research and the implications of our findings for understanding prison violence.},
	language = {en},
	number = {3},
	urldate = {2021-10-19},
	journal = {Sociological Methods \& Research},
	author = {Baćak, Valerio and Kennedy, Edward H.},
	month = aug,
	year = {2019},
	pages = {698--721},
}

@incollection{furnkranz_decision_2017,
	address = {Boston, MA},
	title = {Decision {Tree}},
	isbn = {978-1-4899-7687-1},
	url = {https://doi.org/10.1007/978-1-4899-7687-1_66},
	abstract = {The induction of decision trees is one of the oldest and most popular techniques for learning discriminatory models, which has been developed independently in the statistical (Breiman et al. 1984; Kass 1980) and machine learning (Hunt et al. 1966; Quinlan 1983, 1986) communities. A decision tree is a tree-structured classification model, which is easy to understand, even by non-expert users, and can be efficiently induced from data. An extensive survey of decision-tree learning can be found in Murthy (1998).},
	language = {en},
	urldate = {2021-10-19},
	booktitle = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	publisher = {Springer US},
	author = {Fürnkranz, Johannes},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2017},
	doi = {10.1007/978-1-4899-7687-1_66},
	pages = {330--335},
}

@incollection{torgo_regression_2017,
	address = {Boston, MA},
	title = {Regression {Trees}},
	isbn = {978-1-4899-7687-1},
	url = {https://doi.org/10.1007/978-1-4899-7687-1_717},
	language = {en},
	urldate = {2021-10-19},
	booktitle = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	publisher = {Springer US},
	author = {Torgo, Luı́s},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2017},
	doi = {10.1007/978-1-4899-7687-1_717},
	pages = {1080--1083},
}

@article{ghosh_nearest_2007,
	title = {On {Nearest} {Neighbor} {Classification} {Using} {Adaptive} {Choice} of k},
	volume = {16},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/106186007X208380},
	doi = {10.1198/106186007X208380},
	abstract = {Nearest neighbor classification is one of the simplest and popular methods for statistical pattern recognition. It classifies an observation x to the class, which is the most frequent in the neighborhood of x. The size of this neighborhood is usually determined by a predefined parameter k. Normally, one uses cross-validation techniques to estimate the optimum value of this parameter, and that estimated value is used for classifying all observations. However, in classification problems, in addition to depending on the training sample, a good choice of k depends on the specific observation to be classified. Therefore, instead of using a fixed value of k over the entire measurement space, a spatially adaptive choice of k may be more useful in practice. This article presents one such adaptive nearest neighbor classification technique, where the value of k is selected depending on the distribution of competing classes in the vicinity of the observation to be classified. The utility of the proposed method has been illustrated using some simulated examples and well-known benchmark datasets. Asymptotic optimality of its misclassification rate has been derived under appropriate regularity conditions.},
	number = {2},
	urldate = {2021-10-19},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Ghosh, Anil K},
	month = jun,
	year = {2007},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/106186007X208380},
	pages = {482--502},
}

@book{zhang_recursive_2010,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Recursive {Partitioning} and {Applications}},
	volume = {0},
	isbn = {978-1-4419-6823-4 978-1-4419-6824-1},
	url = {http://link.springer.com/10.1007/978-1-4419-6824-1},
	language = {en},
	urldate = {2021-10-19},
	publisher = {Springer New York},
	author = {Zhang, Heping and Singer, Burton H.},
	year = {2010},
	doi = {10.1007/978-1-4419-6824-1},
}

@article{zeileis_generalized_2007,
	title = {Generalized {M}-fluctuation tests for parameter instability},
	volume = {61},
	issn = {0039-0402, 1467-9574},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9574.2007.00371.x},
	doi = {10.1111/j.1467-9574.2007.00371.x},
	abstract = {A general class of ﬂuctuation tests for parameter instability in an M-estimation framework is suggested. Tests from this framework can be constructed by ﬁrst choosing an appropriate estimation technique, deriving a partial sum process of the estimation scores which captures instabilities over time, and aggregating this process to a test statistic by using a suitable scalar functional. Inference for these tests is based on functional central limit theorems which are derived under the null hypothesis of parameter stability and local alternatives. For (generalized) linear regression models, concrete tests are derived which cover several known tests for (approximately) normal data but also allow for testing for parameter instability in regressions with binary or count data. The usefulness of the test procedures—complemented by powerful visualizations derived from these—is illustrated using Dow Jones industrial average stock returns, youth homicides in Boston, USA, and illegitimate births in Großarl, Austria.},
	language = {en},
	number = {4},
	urldate = {2021-10-19},
	journal = {Statistica Neerlandica},
	author = {Zeileis, Achim and Hornik, Kurt},
	month = nov,
	year = {2007},
	pages = {488--508},
}

@article{zeileis_model-based_2008,
	title = {Model-{Based} {Recursive} {Partitioning}},
	volume = {17},
	url = {http://www.jstor.org/stable/27594318},
	language = {en},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
	year = {2008},
	pages = {492--514},
}

@article{tutz_improved_2016,
	title = {Improved nearest neighbor classifiers by weighting and selection of predictors},
	volume = {26},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-015-9588-z},
	doi = {10.1007/s11222-015-9588-z},
	abstract = {Nearest neighborhood classiﬁcation is a ﬂexible classiﬁcation method that works under weak assumptions. The basic concept is to use the weighted or un-weighted sums over class indicators of observations in the neighborhood of the target value. Two modiﬁcations that improve the performance are considered here. Firstly, instead of using weights that are solely determined by the distances we estimate the weights by use of a logit model. By using a selection procedure like lasso or boosting the relevant nearest neighbors are automatically selected. Based on the concept of estimation and selection, in the second step, we extend the predictor space. We include nearest neighborhood counts, but also the original predictors themselves and nearest neighborhood counts that use distances in sub dimensions of the predictor space. The resulting classiﬁers combine the strength of nearest neighbor methods with parametric approaches and by use of sub dimensions are able to select the relevant features. Simulations and real data sets demonstrate that the method yields better misclassiﬁcation rates than currently available nearest neighborhood methods and is a strong and ﬂexible competitor in classiﬁcation problems.},
	language = {en},
	number = {5},
	urldate = {2021-10-19},
	journal = {Statistics and Computing},
	author = {Tutz, Gerhard and Koch, Dominik},
	month = sep,
	year = {2016},
	pages = {1039--1057},
}

@article{strobl_introduction_2009,
	title = {An introduction to recursive partitioning: {Rationale}, application, and characteristics of classification and regression trees, bagging, and random forests.},
	volume = {14},
	issn = {1939-1463, 1082-989X},
	shorttitle = {An introduction to recursive partitioning},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0016973},
	doi = {10.1037/a0016973},
	abstract = {Recursive partitioning methods have become popular and widely used tools for nonparametric regression and classiﬁcation in many scientiﬁc ﬁelds. Especially random forests, which can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine, and bioinformatics within the past few years. High-dimensional problems are common not only in genetics, but also in some areas of psychological research, where only a few subjects can be measured because of time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications and to provide descriptive variable importance measures reﬂecting the impact of each variable in both main effects and interactions. The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high-dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application. Application of the methods is illustrated with freely available implementations in the R system for statistical computing.},
	language = {en},
	number = {4},
	urldate = {2021-10-19},
	journal = {Psychological Methods},
	author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
	month = dec,
	year = {2009},
	pages = {323--348},
}

@article{seibold_model-based_2016,
	title = {Model-{Based} {Recursive} {Partitioning} for {Subgroup} {Analyses}},
	volume = {12},
	issn = {2194-573X, 1557-4679},
	url = {https://www.degruyter.com/document/doi/10.1515/ijb-2015-0032/html},
	doi = {10.1515/ijb-2015-0032},
	abstract = {The identification of patient subgroups with differential treatment effects is the first step towards individualised treatments. A current draft guideline by the EMA discusses potentials and problems in subgroup analyses and formulated challenges to the development of appropriate statistical procedures for the data-driven identification of patient subgroups. We introduce model-based recursive partitioning as a procedure for the automated detection of patient subgroups that are identifiable by predictive factors. The method starts with a model for the overall treatment effect as defined for the primary analysis in the study protocol and uses measures for detecting parameter instabilities in this treatment effect. The procedure produces a segmented model with differential treatment parameters corresponding to each patient subgroup. The subgroups are linked to predictive factors by means of a decision tree. The method is applied to the search for subgroups of patients suffering from amyotrophic lateral sclerosis that differ with respect to their Riluzole treatment effect, the only currently approved drug for this disease.},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {The International Journal of Biostatistics},
	author = {Seibold, Heidi and Zeileis, Achim and Hothorn, Torsten},
	month = may,
	year = {2016},
	pages = {45--63},
}

@book{breiman_classification_1998,
	address = {Boca Raton, Fla.},
	edition = {1. CRC Press repr},
	title = {Classification and regression trees},
	isbn = {978-0-412-04841-8},
	language = {eng},
	publisher = {Chapman \& Hall/CRC},
	editor = {Breiman, Leo},
	year = {1998},
}

@article{fielding_binary_1977,
	title = {Binary {Segmentation} in {Survey} {Analysis} with {Particular} {Reference} to {AID}},
	volume = {26},
	issn = {00390526},
	url = {https://www.jstor.org/stable/2988216?origin=crossref},
	doi = {10.2307/2988216},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {The Statistician},
	author = {Fielding, A. and O'Muircheartaigh, C. A.},
	month = mar,
	year = {1977},
	pages = {17},
}

@article{fokkema_detecting_2018,
	title = {Detecting treatment-subgroup interactions in clustered data with generalized linear mixed-effects model trees},
	volume = {50},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-017-0971-x},
	doi = {10.3758/s13428-017-0971-x},
	language = {en},
	number = {5},
	urldate = {2021-10-19},
	journal = {Behavior Research Methods},
	author = {Fokkema, M. and Smits, N. and Zeileis, A. and Hothorn, T. and Kelderman, H.},
	month = oct,
	year = {2018},
	pages = {2016--2034},
}

@article{morgan_problems_1963,
	title = {Problems in the {Analysis} of {Survey} {Data}, and a {Proposal}},
	volume = {58},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500855},
	doi = {10.1080/01621459.1963.10500855},
	language = {en},
	number = {302},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {Morgan, James N. and Sonquist, John A.},
	month = jun,
	year = {1963},
	pages = {415--434},
}

@article{loh_fifty_2014,
	title = {Fifty {Years} of {Classification} and {Regression} {Trees}: {Fifty} {Years} of {Classification} and {Regression} {Trees}},
	volume = {82},
	issn = {03067734},
	shorttitle = {Fifty {Years} of {Classification} and {Regression} {Trees}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12016},
	doi = {10.1111/insr.12016},
	abstract = {Fifty years have passed since the publication of the ﬁrst regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classiﬁcation trees can partition the data with linear splits on subsets of variables and ﬁt nearest neighbor, kernel density, and other models in the partitions. Regression trees can ﬁt almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a signiﬁcant role in helping the techniques gain acceptance and popularity in the broader scientiﬁc community. This article surveys the developments and brieﬂy reviews the key ideas behind some of the major algorithms.},
	language = {en},
	number = {3},
	urldate = {2021-10-19},
	journal = {International Statistical Review},
	author = {Loh, Wei-Yin},
	month = dec,
	year = {2014},
	pages = {329--348},
}

@incollection{mcardle_potential_2013,
	edition = {0},
	title = {The {Potential} of {Model}-based {Recursive} {Partitioning} in the {Social} {Sciences}: {Revisiting} {Ockham}’s {Razor}},
	isbn = {978-0-203-40302-0},
	shorttitle = {The {Potential} of {Model}-based {Recursive} {Partitioning} in the {Social} {Sciences}},
	url = {https://www.taylorfrancis.com/books/9781135044091/chapters/10.4324/9780203403020-12},
	abstract = {A variety of new statistical methods from the ﬁeld of machine learning have the potential to oﬀer new impulses for research in the social, educational and behavioral sciences. In this article we focus on one of these methods: model-based recursive partitioning. This algorithmic approach is reviewed and illustrated by means of instructive examples and an application to the Mincer equation. For readers unfamiliar with algorithmic methods, the explanation starts with the introduction of the predecessor method classiﬁcation and regression trees. With respect to the application and interpretation of model-based recursive partitioning, we address the principle of parsimony and illustrate that the model-based recursive partitioning approach can be employed to test whether a postulated model is in accordance with Ockham’s Razor or whether relevant covariates have been omitted. Finally, an overview of available statistical software is provided to facilitate the applicability in social science research.},
	language = {en},
	urldate = {2021-10-19},
	booktitle = {Contemporary {Issues} in {Exploratory} {Data} {Mining} in the {Behavioral} {Sciences}},
	publisher = {Routledge},
	editor = {McArdle, John J. and Ritschard, Gilbert},
	month = aug,
	year = {2013},
	doi = {10.4324/9780203403020-12},
	pages = {97--117},
}

@article{denison_bayesian_1998,
	title = {A {Bayesian} {CART} algorithm},
	volume = {85},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/85.2.363},
	doi = {10.1093/biomet/85.2.363},
	abstract = {A stochastic search form of classification and regression tree (CART) analysis (Breiman et al., 1984) is proposed, motivated by a Bayesian model. An approximation to a probability distribution over the space of possible trees is explored using reversible jump Markov chain Monte Carlo methods (Green, 1995).},
	language = {en},
	number = {2},
	journal = {Biometrika},
	author = {Denison, David G T and Mallick, K and Smith, Adrian F M},
	month = jun,
	year = {1998},
	pages = {363--377},
}

@article{kapelner_bartmachine_2014,
	title = {{bartMachine}: {Machine} {Learning} with {Bayesian} {Additive} {Regression} {Trees}},
	shorttitle = {{bartMachine}},
	url = {http://arxiv.org/abs/1312.2171},
	abstract = {We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data.},
	urldate = {2021-10-19},
	journal = {arXiv:1312.2171 [cs, stat]},
	author = {Kapelner, Adam and Bleich, Justin},
	month = nov,
	year = {2014},
	note = {arXiv: 1312.2171},
}

@article{wu_bayesian_2007,
	title = {Bayesian {CART}: {Prior} {Specification} and {Posterior} {Simulation}},
	volume = {16},
	url = {http://www.jstor.org/stable/27594229},
	language = {en},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Wu, Yuhong and Tjelmeland, Håkon and West, Mike},
	year = {2007},
	pages = {44--66},
}

@article{pratola_parallel_2014,
	title = {Parallel {Bayesian} {Additive} {Regression} {Trees}},
	volume = {23},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/full/10.1080/10618600.2013.841584},
	doi = {10.1080/10618600.2013.841584},
	language = {en},
	number = {3},
	urldate = {2021-10-19},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Pratola, Matthew T. and Chipman, Hugh A. and Gattiker, James R. and Higdon, David M. and McCulloch, Robert and Rust, William N.},
	month = jul,
	year = {2014},
	pages = {830--852},
}

@article{chipman_bayesian_1998,
	title = {Bayesian {CART} {Model} {Search}},
	volume = {93},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/full/10.1080/01621459.1998.10473750},
	doi = {10.1080/01621459.1998.10473750},
	language = {en},
	number = {443},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = sep,
	year = {1998},
	pages = {935--948},
}

@article{chipman_bart_2010,
	title = {{BART}: {Bayesian} additive regression trees},
	volume = {4},
	issn = {1932-6157},
	shorttitle = {{BART}},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-4/issue-1/BART-Bayesian-additive-regression-trees/10.1214/09-AOAS285.full},
	doi = {10.1214/09-AOAS285},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {The Annals of Applied Statistics},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = mar,
	year = {2010},
}

@article{kallus_stochastic_2020,
	title = {Stochastic {Optimization} {Forests}},
	url = {http://arxiv.org/abs/2008.07473},
	abstract = {We study contextual stochastic optimization problems, where we leverage rich auxiliary observations (e.g., product characteristics) to improve decision making with uncertain variables (e.g., demand). We show how to train forest decision policies for this problem by growing trees that choose splits to directly optimize the downstream decision quality, rather than splitting to improve prediction accuracy as in the standard random forest algorithm. We realize this seemingly computationally intractable problem by developing approximate splitting criteria that utilize optimization perturbation analysis to eschew burdensome re-optimization for every candidate split, so that our method scales to large-scale problems. We prove that our splitting criteria consistently approximate the true risk and that our method achieves asymptotic optimality. We extensively validate our method empirically, demonstrating the value of optimization-aware construction of forests and the success of our efficient approximations. We show that our approximate splitting criteria can reduce running time hundredfold, while achieving performance close to forest algorithms that exactly re-optimize for every candidate split.},
	urldate = {2021-10-19},
	journal = {arXiv:2008.07473 [cs, math, stat]},
	author = {Kallus, Nathan and Mao, Xiaojie},
	month = oct,
	year = {2020},
	note = {arXiv: 2008.07473},
}

@article{hu_fast_2021,
	title = {Fast {Rates} for {Contextual} {Linear} {Optimization}},
	url = {http://arxiv.org/abs/2011.03030},
	abstract = {Incorporating side observations in decision making can reduce uncertainty and boost performance, but it also requires we tackle a potentially complex predictive relationship. While one may use off-the-shelf machine learning methods to separately learn a predictive model and plug it in, a variety of recent methods instead integrate estimation and optimization by fitting the model to directly optimize downstream decision performance. Surprisingly, in the case of contextual linear optimization, we show that the naive plug-in approach actually achieves regret convergence rates that are significantly faster than methods that directly optimize downstream decision performance. We show this by leveraging the fact that specific problem instances do not have arbitrarily bad near-dual-degeneracy. While there are other pros and cons to consider as we discuss and illustrate numerically, our results highlight a nuanced landscape for the enterprise to integrate estimation and optimization. Our results are overall positive for practice: predictive models are easy and fast to train using existing tools, simple to interpret, and, as we show, lead to decisions that perform very well.},
	urldate = {2021-10-19},
	journal = {arXiv:2011.03030 [cs, math, stat]},
	author = {Hu, Yichun and Kallus, Nathan and Mao, Xiaojie},
	month = aug,
	year = {2021},
	note = {arXiv: 2011.03030},
}

@article{loos_verfahren_2013,
	title = {Das {Verfahren} der {Berufskodierung} im {Zensus} 2011},
	journal = {Wirtschaft und Statistik},
	author = {Loos, Christiane and Eisenmenger, Matthias and Bretschi, David},
	month = mar,
	year = {2013},
	pages = {173--184},
}

@article{sebastiani_machine_2002,
	title = {Machine {Learning} in {Automated} {Text} {Categorization}},
	volume = {34},
	issn = {0360-0300},
	url = {\url{http://doi.acm.org/10.1145/505282.505283}},
	doi = {10.1145/505282.505283},
	number = {1},
	journal = {ACM Comput. Surv.},
	author = {Sebastiani, Fabrizio},
	month = mar,
	year = {2002},
	note = {Place: New York, NY, USA
Publisher: ACM},
	pages = {1--47},
}

@inproceedings{chen_xgboost_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	url = {http://doi.acm.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	note = {event-place: San Francisco, California, USA},
	pages = {785--794},
}

@techreport{the_committee_for_the_prize_in_economic_sciences_in_memory_of_alfred_nobel_understanding_2019,
	title = {Understanding {Development} and {Poverty} {Alleviation}},
	url = {https://www.nobelprize.org/uploads/2019/10/advanced-economicsciencesprize2019.pdf},
	urldate = {2021-10-12},
	institution = {Royal Swedish Academy of Sciences},
	author = {{The Committee for the Prize in Economic Sciences in Memory of Alfred Nobel}},
	month = oct,
	year = {2019},
	keywords = {inspirational, review},
}

@inproceedings{bekkerman_high-precision_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {High-{Precision} {Phrase}-{Based} {Document} {Classification} on a {Modern} {Scale}},
	isbn = {978-1-4503-0813-7},
	url = {http://doi.acm.org/10.1145/2020408.2020449},
	doi = {10.1145/2020408.2020449},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Bekkerman, Ron and Gavish, Matan},
	year = {2011},
	note = {event-place: San Diego, California, USA},
	pages = {231--239},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep learning in neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	language = {en},
	urldate = {2021-10-19},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	pages = {85--117},
}

@incollection{brocker_probability_2012,
	address = {Chichester},
	title = {Probability {Forecasts}},
	isbn = {978-1-119-96000-3},
	url = {http://dx.doi.org/10.1002/9781119960003.ch7},
	booktitle = {Forecast {Verification}: {A} {Practitioner}'s {Guide} to {Atmospheric} {Science}, 2nd {Edtion}},
	publisher = {Wiley},
	author = {Bröcker, Jochen},
	editor = {Jollife, Ian T. and Stephenson, David B.},
	year = {2012},
	doi = {10.1002/9781119960003.ch7},
	pages = {119--139},
}

@article{gneiting_probabilistic_2007,
	title = {Probabilistic forecasts, calibration and sharpness},
	volume = {69},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
	doi = {10.1111/j.1467-9868.2007.00587.x},
	abstract = {Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest.},
	language = {en},
	number = {2},
	urldate = {2021-10-16},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2007.00587.x},
	pages = {243--268},
}

@article{ossiander_computer_2006,
	title = {A computer system for coding occupation},
	volume = {49},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajim.20355},
	doi = {10.1002/ajim.20355},
	abstract = {Abstract Background Occupation information is widely used in epidemiologic studies and is collected on most death certificates and many birth certificates in the United States. Coding the massive amount of occupation information collected has been a challenge. Methods A simple word-matching computer program to code occupation entries from vital records was developed. The accuracy of the program was evaluated by comparing its output to codes assigned by human coders. Results In routine use in the Washington State Department of Health (DOH), the computer system codes 96–97\% of the occupation entries on birth and death records. It assigned the correct code on 89\% (95\% confidence interval (87\%, 91\%)) of the records it coded. Conclusions The occupation-coding program is both efficient and accurate and can simplify the process of coding occupation entries from vital records. The system is adaptable and can be modified to use occupation classifications other than the one used by DOH. Am. J. Ind. Med. 2006. © 2006 Wiley-Liss, Inc.},
	number = {10},
	journal = {American Journal of Industrial Medicine},
	author = {Ossiander, Eric M. and Milham, Samuel},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajim.20355},
	pages = {854--857},
}

@incollection{campanelli_quality_1997,
	address = {New York},
	title = {The quality of occupational coding in the {United} {Kingdom}},
	isbn = {978-1-118-49001-3},
	booktitle = {Survey {Measurement} and {Process} {Quality}},
	publisher = {Wiley},
	author = {Campanelli, Pamela and Thomson, Katarina and Moon, Nick and Staples, Tessa},
	editor = {Lyberg, Lars and Biemer, Paul and Collins, Martin and DeLeeuw, Edith and Dippo, Cathryn and Schwarz, Norbert and Trewin, Dennis},
	year = {1997},
	doi = {10.1002/9781118490013.ch19},
	pages = {pp. 437--453},
}

@article{tijdens_self-identification_2015,
	title = {Self-identification of occupation in web surveys: requirements for search trees and look-up tables},
	url = {https://surveyinsights.org/?p=6967},
	journal = {Survey Insights: Methods from the Field},
	author = {Tijdens, Kea},
	year = {2015},
}

@article{henninges_auf_1976,
	title = {Auf dem {Wege} zu homogenen, tätigkeitsorientierten {Berufseinheiten}. {Ein} {Ansatz} zur {Quantifizierung} und Überwindung der {Unschärfebereiche} der amtlichen {Berufssystematik}},
	volume = {9},
	url = {https://ideas.repec.org/a/iab/iabmit/v9i3p285-301.html},
	abstract = {\&quot;Die Arbeit befaßt sich mit einem speziellen, aber nicht neuen Thema der Berufsforschung, nämlich mit dem Informationsgehalt von Berufsbezeichnungen über Tätigkeitsinhalte. Aufbauend auf den bisherigen Bemühungen des IAB, die Aussagefähigkeit der amtlichen Berufssystematik für Forschungs- und Beratungsbelange zu verbessern, wird in der Untersuchung der Versuch unternommen, einen methodischen Ansatz zur Quantifizierung der Unschärfebereiche von Berufsbenennungen zu entwickeln und ihn exemplarisch auf ausgewählte Berufsgruppen anzuwenden. Sodann werden klassifikatorische Möglichkeiten diskutiert, mittels derer die empirisch ermittelten Unschärfebereiche verringert werden könAnsatz, der sich von den in der amtlichen Berufssystematik vogegebenen Ordnungen abkehrt und versucht, Erwerbstätige anhand der Ähnlichkeit ihrer Tätigkeitsprofile empirisch zu klassifizieren. Die Untersuchung versteht sich als Problemstudie. Ihr empirisches Fundament bildet eine im Herbst 1975 erhobene Stichprobe von 458 Erwerbstätigen. Die empirischen Befunde sind an vielen Stellen materialgebunden und haben deshalb den Charakter von Arbeitshypothesen. Sie bedürfen durchgängig einer Validierung durch Vergleichsuntersuchungen.\&quot; Die Untersuchung wurde im IAB durchgeführt.},
	number = {3},
	journal = {Mitteilungen aus der Arbeitsmarkt- und Berufsforschung},
	author = {Henninges, Hasso von},
	year = {1976},
	pages = {285--301},
}

@article{henninges_berufsforschung_1976,
	title = {Berufsforschung im {IAB}. {Versuch} einer {Standortbestimmung}},
	volume = {9},
	url = {https://ideas.repec.org/a/iab/iabmit/v9i1p1-18.html},
	abstract = {\&quot;In den letzten Jahren hat die Berufsforschung des IAB zahlreiche empirische Arbeiten über die Strukturen der Erwerbstätigkeit und deren Wandel vorglegt. Mit der Herausgabe des Handbuchs zu den ausbildungs-, berufs- und wirtschaftszweigspezifischen Beschäftigungschancen (ABC-Handbuch) fand diese Phase der anwendungsorientierten Aufbereitung berufsbezogener Daten einen vorläufigen Abschluß. Der erst Abschnitt empirisch bestimmter Berufsforschung erbrachte eine Fülle neuer Einsichten und Erkenntnisse, sowohl was die Grenzen und Möglichkeiten der berufsstatistischen Kategorien und Schlüsselsysteme angeht als auch bezogen auf die Anwendung der Ergebnisse in der Beratung. In der gegenwärtigen Plateauphase erscheint es angebracht, das bisherige Vorgehen kritisch zu reflektieren und mit neueren Befunden zum \&quot;Berufsphänomen\&quot; - die u.a. zunehmend außerhalb des IAB anfallen - zu konfrontieren. Bei der daraus resultierenden Standortbestimmung der Berufsforschung des IAB wird von den Elementen ausgegangen, die nach gängier Auffassung einen Beruf konstituieren. Werden die heutigen berufsstatistischen und berufskundlichen Instrumente und Informationen daran gemessen, wird deutlich, daß sie lediglich einen Ausschnitt abdecken. Bei diesem Sektor - auf den sich Berufsforschung im IAB z.T. gezwungenermaßen (aufgrund der begrenzten Datenbasis) konzentriert - geht es vorrangig um die Analyse der Arbeitsplatzund Arbeitskraftstrukturen sowie deren Veränderung. Dies gilt insbesondere im Hinblick auf die Probleme des Übergangs der nachwachsenden (nachrückenden) Generation, die verstärkt an zwei Schwellen - auf die sich die Arbeit der Berufsforschung zunehmend orientieren wird - auftreten: Beim Übergang von der Schule in die berufliche Bildung und bei der Übernahme der ersten Arbeitsposition nach der beruflichen Qualifizierung. Offen bleibt, auf welche Weise \&quot;Arbeitsvermögen\&quot; - als Beruf bestimmende Dimension - und \&quot;Qualifikation\&quot; - als für unterschiedlich},
	number = {1},
	journal = {Mitteilungen aus der Arbeitsmarkt- und Berufsforschung},
	author = {Henninges, Hasso von and Stooß, Friedemann and Troll, Lothar},
	year = {1976},
	pages = {1--18},
}

@article{maaz_intercoder-reliabilitat_2009,
	title = {Intercoder-{Reliabilität} bei der {Berufscodierung} nach der {ISCO}-88 und {Validität} des sozioökonomischen {Status}},
	volume = {12},
	issn = {1434-663X},
	doi = {10.1007/s11618-009-0068-0},
	language = {German},
	number = {2},
	journal = {Zeitschrift für Erziehungswissenschaft},
	author = {Maaz, Kai and Trautwein, Ulrich and Gresch, Cornelia and Lüdtke, Oliver and Watermann, Rainer},
	year = {2009},
	note = {Publisher: VS-Verlag},
	pages = {281--301},
}

@article{creecy_trading_1992,
	title = {Trading {MIPS} and {Memory} for {Knowledge} {Engineering}},
	volume = {35},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/135226.135228},
	doi = {10.1145/135226.135228},
	number = {8},
	journal = {Commun. ACM},
	author = {Creecy, Robert H. and Masand, Brij M. and Smith, Stephen J. and Waltz, David L.},
	month = aug,
	year = {1992},
	note = {Place: New York, NY, USA
Publisher: ACM},
	pages = {48--64},
}

@incollection{bailey_web-based_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Web}-{Based} {Automated} {System} for {Industry} and {Occupation} {Coding}},
	volume = {5175},
	isbn = {978-3-540-85480-7},
	url = {\url{http://dx.doi.org/10.1007/978-3-540-85481-4_33}},
	booktitle = {Web {Information} {Systems} {Engineering} - {WISE} 2008},
	publisher = {Springer Berlin Heidelberg},
	author = {Jung, Yuchul and Yoo, Jihee and Myaeng, Sung-Hyon and Han, Dong-Cheol},
	editor = {Bailey, James and Maier, David and Schewe, Klaus-Dieter and Thalheim, Bernhard and Wang, XiaoyangSean},
	year = {2008},
	doi = {$10.1007/978-3-540-85481-4_33$},
	pages = {443--457},
}

@article{troll_unscharfen_1981,
	title = {Unschärfen bei der {Erfassung} des ausgeübten {Berufs} und {Ansätze} zur {Verbesserung} statistischer {Nachweise}},
	volume = {14},
	url = {https://ideas.repec.org/a/iab/iabmit/v14i2p163-179.html},
	abstract = {\&quot;Aussagen zur Berufsstruktur und ihrem Wandel zu gewinnen, waren bisher lediglich über die Erwerbsstatistik zu gewinnen. Sie bot Angaben zu Ausbildung und Beruf im 10-Jahres-Abstand bei den Volks- und Berufszählungen (Zensen) und im Zwei-Jahres-Abstand bei der 1\% Stichprobe des Mikrozensus. Mit dem Aufbau der Beschäftigtenstatistik bei der Bundesanstalt für Arbeit, die u.a. Angaben zur ausgeübten Tätigkeit (Beruf) ausweist, hat sich die Datenlage wesentlich verbessert. ... Ein erster Vergleich der Bestände für das Jahr 1978 zeigt, daß in der Untergliederung nach 10 Berufsbereichen die Abweichungen beim Berufsgewicht in relativ engen Grenzen liegen. Werden die Angaben der beiden statistischen Quellen nach den 328 Berufsordnungen verglichen, treten Unterschiede und Übereinstimmung deutlich zutage. ... Aus dem Vergleich ergeben sich u.a. folgende Überlegungen: ++ Berufe lassen sich lediglich innerhalb gewisser Bandbrei ten zahlenmäßig bestimmen. ++ Je weniger präzise ein Beruf umgangssprachlich benannt und im Bewußtsein der Bevölkerung ausgeprägt ist, um so größer wird der Unschärfebereich. ... ++ Die eindimensionale Erhebung beruflicher Daten über Benennun gen reicht nicht aus, berufliche Strukturen und ihren Wand del im Zeitablauf verläßlich abzubilden.\&quot;},
	number = {2},
	journal = {Mitteilungen aus der Arbeitsmarkt- und Berufsforschung},
	author = {Troll, Lothar},
	year = {1981},
	pages = {163--179},
}

@article{hsu_attributes_1986,
	title = {The attributes diagram: {A} geometrical framework for assessing the quality of probability forecasts},
	volume = {2},
	issn = {0169-2070},
	doi = {https://doi.org/10.1016/0169-2070(86)90048-8},
	abstract = {Abstract In two-event situations, a reliability diagram provides a geometrical framework for evaluating this attribute of probability forecasts. However, reliability is only one of several important attributes of such forecasts. This paper describes an extension of the reliability diagram - the attributes diagram - in which the accuracy, resolution, and skill, as well as the reliability, of probability forecasts can be depicted. Moreover, these geometrical representations are shown to be directly related to quantitative measures of the respective attributes. The interpretation and use of the attributes diagram is illustrated by considering samples of probabilistic quantitative precipitation forecasts. Some possible extensions of this diagram to multiple-event situations are briefly discussed.},
	number = {3},
	journal = {International Journal of Forecasting},
	author = {Hsu, Wu-ron and Murphy, Allan H.},
	year = {1986},
	pages = {285 -- 293},
}

@incollection{bound_measurement_2001,
	title = {Measurement {Error} in {Survey} {Data}},
	volume = {5},
	isbn = {978-0-444-82340-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1573441201050127},
	language = {en},
	urldate = {2021-10-11},
	booktitle = {Handbook of {Econometrics}},
	publisher = {Elsevier},
	author = {Bound, John and Brown, Charles and Mathiowetz, Nancy},
	year = {2001},
	doi = {10.1016/S1573-4412(01)05012-7},
	pages = {3705--3843},
}

@article{liu_learning_2009,
	title = {Learning to {Rank} for {Information} {Retrieval}},
	language = {en},
	author = {Liu, Tie-Yan},
	year = {2009},
	pages = {67},
}

@article{li_short_2011,
	title = {A {Short} {Introduction} to {Learning} to {Rank}},
	volume = {E94.D},
	doi = {10.1587/transinf.E94.D.1854},
	abstract = {Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining. Intensive studies have been conducted on the problem and significant progress has been made[1],[2]. This short paper gives an introduction to learning to rank, and it specifically explains the fundamental problems, existing approaches, and future work of learning to rank. Several learning to rank methods using SVM techniques are described in details.},
	number = {10},
	journal = {IEICE Transactions on Information and Systems},
	author = {Li, Hang},
	year = {2011},
	pages = {1854--1862},
}

@article{yang_evaluation_1999,
	title = {An {Evaluation} of {Statistical} {Approaches} to {Text} {Categorization}},
	volume = {1},
	issn = {1573-7659},
	url = {https://doi.org/10.1023/A:1009982220290},
	doi = {10.1023/A:1009982220290},
	abstract = {This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {Information Retrieval},
	author = {Yang, Yiming},
	month = apr,
	year = {1999},
	pages = {69--90},
}

@article{nelson_computational_2020,
	title = {Computational {Grounded} {Theory}: {A} {Methodological} {Framework}},
	volume = {49},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Computational {Grounded} {Theory}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124117729703},
	doi = {10.1177/0049124117729703},
	abstract = {This article proposes a three-step methodological framework called computational grounded theory, which combines expert human knowledge and hermeneutic skills with the processing power and pattern recognition of computers, producing a more methodologically rigorous but interpretive approach to content analysis. The first, pattern detection step, involves inductive computational exploration of text, using techniques such as unsupervised machine learning and word scores to help researchers to see novel patterns in their data. The second, pattern refinement step, returns to an interpretive engagement with the data through qualitative deep reading or further exploration of the data. The third, pattern confirmation step, assesses the inductively identified patterns using further computational and natural language processing techniques. The result is an efficient, rigorous, and fully reproducible computational grounded theory. This framework can be applied to any qualitative text as data, including transcribed speeches, interviews, open-ended survey data, or ethnographic field notes, and can address many potential research questions.},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {Sociological Methods \& Research},
	author = {Nelson, Laura K.},
	month = feb,
	year = {2020},
	pages = {3--42},
}

@book{lehmann_perspectives_2014,
	title = {Perspectives on ontology learning},
	isbn = {978-1-61499-379-7},
	url = {http://site.ebrary.com/id/10898085},
	language = {English},
	urldate = {2021-10-19},
	author = {Lehmann, J and Völker, Johanna},
	year = {2014},
	note = {OCLC: 889308543},
}

@book{jollife_forecast_2012,
	address = {Chichester},
	title = {Forecast {Verification}: {A} {Practitioner}'s {Guide} to {Atmospheric} {Science}, 2nd {Edtion}},
	isbn = {978-1-119-96000-3},
	url = {http://dx.doi.org/10.1002/9781119960003},
	publisher = {Wiley},
	author = {Jollife, Ian T. and Stephenson, David B.},
	year = {2012},
	doi = {10.1002/9781119960003},
}

@article{domingos_few_2012,
	title = {A few useful things to know about machine learning},
	volume = {55},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2347736.2347755},
	doi = {10.1145/2347736.2347755},
	abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
	language = {en},
	number = {10},
	urldate = {2021-10-19},
	journal = {Communications of the ACM},
	author = {Domingos, Pedro},
	month = oct,
	year = {2012},
	pages = {78--87},
}

@article{loo_stringdist_2014,
	title = {The stringdist {Package} for {Approximate} {String} {Matching}},
	volume = {6},
	url = {https://doi.org/10.32614/RJ-2014-011},
	doi = {10.32614/RJ-2014-011},
	number = {1},
	journal = {The R Journal},
	author = {Loo, Mark P. J. van der},
	year = {2014},
	pages = {111--122},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v33/i01/},
	doi = {10.18637/jss.v033.i01},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include 1 (the lasso), 2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal eﬃciently with sparse features. In comparative timings we ﬁnd that the new algorithms are considerably faster than competing methods.},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	year = {2010},
	pages = {1--22},
}

@article{the_economist_chinas_2020,
	title = {China’s success at {AI} has relied on good data},
	issn = {0013-0613},
	url = {https://www.economist.com/technology-quarterly/2020/01/02/chinas-success-at-ai-has-relied-on-good-data},
	abstract = {But cheap labour has also played an important part},
	urldate = {2021-10-19},
	journal = {The Economist},
	author = {The Economist},
	month = jan,
	year = {2020},
}

@article{efron_prediction_2020,
	title = {Prediction, {Estimation}, and {Attribution}},
	volume = {115},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1762613},
	doi = {10.1080/01621459.2020.1762613},
	language = {en},
	number = {530},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	month = apr,
	year = {2020},
	pages = {636--655},
}

@article{xie_discussion_2020,
	title = {Discussion of {Professor} {Bradley} {Efron}’s {Article} on “{Prediction}, {Estimation}, and {Attribution}”},
	volume = {115},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1762614},
	doi = {10.1080/01621459.2020.1762614},
	language = {en},
	number = {530},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {Xie, Min-ge and Zheng, Zheshi},
	month = apr,
	year = {2020},
	pages = {667--671},
}

@article{varian_big_2014,
	title = {Big {Data}: {New} {Tricks} for {Econometrics}},
	volume = {28},
	issn = {0895-3309},
	shorttitle = {Big {Data}},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.28.2.3},
	doi = {10.1257/jep.28.2.3},
	abstract = {Computers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may require different tools. First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appropriate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more flexible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely known and used by economists.},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {Journal of Economic Perspectives},
	author = {Varian, Hal R.},
	month = may,
	year = {2014},
	pages = {3--28},
}

@article{shmueli_explain_2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full},
	doi = {10.1214/10-STS330},
	abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conﬂation between explanation and prediction is common, yet the distinction must be understood for progressing scientiﬁc knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
	language = {en},
	number = {3},
	urldate = {2021-10-19},
	journal = {Statistical Science},
	author = {Shmueli, Galit},
	month = aug,
	year = {2010},
}

@article{rudin_stop_2019,
	title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	volume = {1},
	issn = {2522-5839},
	url = {http://www.nature.com/articles/s42256-019-0048-x},
	doi = {10.1038/s42256-019-0048-x},
	language = {en},
	number = {5},
	urldate = {2021-10-19},
	journal = {Nature Machine Intelligence},
	author = {Rudin, Cynthia},
	month = may,
	year = {2019},
	pages = {206--215},
}

@article{rona-tas_enlisting_2019,
	title = {Enlisting {Supervised} {Machine} {Learning} in {Mapping} {Scientific} {Uncertainty} {Expressed} in {Food} {Risk} {Analysis}},
	volume = {48},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124117729701},
	doi = {10.1177/0049124117729701},
	abstract = {Recently, both sociology of science and policy research have shown increased interest in scientific uncertainty. To contribute to these debates and create an empirical measure of scientific uncertainty, we inductively devised two systems of classification or ontologies to describe scientific uncertainty in a large corpus of food safety risk assessments with the help of machine learning (ML). We ask three questions: (1) Can we use ML to assist with coding complex documents such as food safety risk assessments on a difficult topic like scientific uncertainty? (2) Can we assess using ML the quality of the ontologies we devised? (3) And, finally, does the quality of our ontologies depend on social factors? We found that ML can do surprisingly well in its simplest form identifying complex meanings, and it does not benefit from adding certain types of complexity to the analysis. Our ML experiments show that in one ontology which is a simple typology, against expectations, semantic opposites attract each other and support the taxonomic structure of the other. And finally, we found some evidence that institutional factors do influence how well our taxonomy of uncertainty performs, but its ability to capture meaning does not vary greatly across the time, institutional context, and cultures we investigated.},
	language = {en},
	number = {3},
	urldate = {2021-10-19},
	journal = {Sociological Methods \& Research},
	author = {Rona-Tas, Akos and Cornuéjols, Antoine and Blanchemanche, Sandrine and Duroy, Antonin and Martin, Christine},
	month = aug,
	year = {2019},
	pages = {608--641},
}

@article{probst_tunability_2018,
	title = {Tunability: {Importance} of {Hyperparameters} of {Machine} {Learning} {Algorithms}},
	shorttitle = {Tunability},
	url = {http://arxiv.org/abs/1802.09596},
	abstract = {Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual conﬁguration by the user or conﬁguring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, deﬁne data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to chose adequate hyperparameter spaces for tuning.},
	language = {en},
	urldate = {2021-10-19},
	journal = {arXiv:1802.09596 [stat]},
	author = {Probst, Philipp and Bischl, Bernd and Boulesteix, Anne-Laure},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.09596},
}

@article{neunhoeffer_how_2019,
	title = {How {Cross}-{Validation} {Can} {Go} {Wrong} and {What} to {Do} {About} {It}},
	volume = {27},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/product/identifier/S1047198718000396/type/journal_article},
	doi = {10.1017/pan.2018.39},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {Political Analysis},
	author = {Neunhoeffer, Marcel and Sternberg, Sebastian},
	month = jan,
	year = {2019},
	pages = {101--106},
}

@article{nagendran_artificial_2020,
	title = {Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies},
	issn = {1756-1833},
	shorttitle = {Artificial intelligence versus clinicians},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.m689},
	doi = {10.1136/bmj.m689},
	abstract = {OBJECTIVE To systematically examine the design, reporting standards, risk of bias, and claims of studies comparing the performance of diagnostic deep learning algorithms for medical imaging with that of expert clinicians. DESIGN Systematic review. DATA SOURCES Medline, Embase, Cochrane Central Register of Controlled Trials, and the World Health Organization trial registry from 2010 to June 2019. ELIGIBILITY CRITERIA FOR SELECTING STUDIES Randomised trial registrations and non-randomised studies comparing the performance of a deep learning algorithm in medical imaging with a contemporary group of one or more expert clinicians. Medical imaging has seen a growing interest in deep learning research. The main distinguishing feature of convolutional neural networks (CNNs) in deep learning is that when CNNs are fed with raw data, they develop their own representations needed for pattern recognition. The algorithm learns for itself the features of an image that are important for classification rather than being told by humans which features to use. The selected studies aimed to use medical imaging for predicting absolute risk of existing disease or classification into diagnostic groups (eg, disease or non-disease). For example, raw chest radiographs tagged with a label such as pneumothorax or no pneumothorax and the CNN learning which pixel patterns suggest pneumothorax.},
	language = {en},
	urldate = {2021-10-19},
	journal = {BMJ},
	author = {Nagendran, Myura and Chen, Yang and Lovejoy, Christopher A and Gordon, Anthony C and Komorowski, Matthieu and Harvey, Hugh and Topol, Eric J and Ioannidis, John P A and Collins, Gary S and Maruthappu, Mahiben},
	month = mar,
	year = {2020},
	pages = {m689},
}

@article{mullainathan_machine_2017,
	title = {Machine {Learning}: {An} {Applied} {Econometric} {Approach}},
	volume = {31},
	issn = {0895-3309},
	shorttitle = {Machine {Learning}},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.31.2.87},
	doi = {10.1257/jep.31.2.87},
	abstract = {Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {Journal of Economic Perspectives},
	author = {Mullainathan, Sendhil and Spiess, Jann},
	month = may,
	year = {2017},
	pages = {87--106},
}

@article{moguerza_support_2006,
	title = {Support {Vector} {Machines} with {Applications}},
	volume = {21},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-21/issue-3/Support-Vector-Machines-with-Applications/10.1214/088342306000000493.full},
	doi = {10.1214/088342306000000493},
	abstract = {Support vector machines (SVMs) appeared in the early nineties as optimal margin classiﬁers in the context of Vapnik’s statistical learning theory. Since then SVMs have been successfully applied to real-world data analysis problems, often providing improved results compared with other techniques. The SVMs operate within the framework of regularization theory by minimizing an empirical risk in a well-posed and consistent way. A clear advantage of the support vector approach is that sparse solutions to classiﬁcation and regression problems are usually obtained: only a few samples are involved in the determination of the classiﬁcation or regression functions. This fact facilitates the application of SVMs to problems that involve a large amount of data, such as text processing and bioinformatics tasks. This paper is intended as an introduction to SVMs and their applications, emphasizing their key features. In addition, some algorithmic extensions and illustrative real-world applications of SVMs are shown.},
	language = {en},
	number = {3},
	urldate = {2021-10-19},
	journal = {Statistical Science},
	author = {Moguerza, Javier M. and Muñoz, Alberto},
	month = aug,
	year = {2006},
}

@article{leblanc_combining_1996,
	title = {Combining {Estiamates} in {Regression} and {Classification}},
	volume = {91},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2291591?origin=crossref},
	doi = {10.2307/2291591},
	language = {en},
	number = {436},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {LeBlanc, Michael and Tibshirani, Robert},
	month = dec,
	year = {1996},
	pages = {1641},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aab3050},
	doi = {10.1126/science.aab3050},
	language = {en},
	number = {6266},
	urldate = {2021-10-19},
	journal = {Science},
	author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
	month = dec,
	year = {2015},
	pages = {1332--1338},
}

@article{knaus_heterogeneous_nodate,
	title = {Heterogeneous {Employment} {Effects} of {Job} {Search} {Programmes}: {A} {Machine} {Learning} {Approach}},
	language = {en},
	author = {Knaus, Michael C and Lechner, Michael},
	pages = {81},
}

@article{kleinberg_prediction_2015,
	title = {Prediction {Policy} {Problems}},
	volume = {105},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.p20151023},
	doi = {10.1257/aer.p20151023},
	abstract = {Most empirical policy work focuses on causal inference. We argue an important class of policy problems does not require causal inference but instead requires predictive inference. Solving these “prediction policy problems” requires more than simple regression techniques, since these are tuned to generating unbiased estimates of coefficients rather than minimizing prediction error. We argue that new developments in the field of “machine learning” are particularly useful for addressing these prediction problems. We use an example from health policy to illustrate the large potential social welfare gains from improved prediction.},
	language = {en},
	number = {5},
	urldate = {2021-10-19},
	journal = {American Economic Review},
	author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad},
	month = may,
	year = {2015},
	pages = {491--495},
}

@article{xia_intentional_2021,
	title = {Intentional {Control} of {Type} {I} {Error} {Over} {Unconscious} {Data} {Distortion}: {A} {Neyman}–{Pearson} {Approach} to {Text} {Classification}},
	volume = {116},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Intentional {Control} of {Type} {I} {Error} {Over} {Unconscious} {Data} {Distortion}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1740711},
	doi = {10.1080/01621459.2020.1740711},
	abstract = {This article addresses the challenges in classifying textual data obtained from open online platforms, which are vulnerable to distortion. Most existing classification methods minimize the overall classification error and may yield an undesirably large Type I error (relevant textual messages are classified as irrelevant), particularly when available data exhibit an asymmetry between relevant and irrelevant information. Data distortion exacerbates this situation and often leads to fallacious prediction. To deal with inestimable data distortion, we propose the use of the Neyman–Pearson (NP) classification paradigm, which minimizes Type II error under a user-specified Type I error constraint. Theoretically, we show that the NP oracle is unaffected by data distortion when the class conditional distributions remain the same. Empirically, we study a case of classifying posts about worker strikes obtained from a leading Chinese microblogging platform, which are frequently prone to extensive, unpredictable and inestimable censorship. We demonstrate that, even though the training and test data are susceptible to different distortion and therefore potentially follow different distributions, our proposed NP methods control the Type I error on test data at the targeted level. The methods and implementation pipeline proposed in our case study are applicable to many other problems involving data distortion. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
	language = {en},
	number = {533},
	urldate = {2021-10-19},
	journal = {Journal of the American Statistical Association},
	author = {Xia, Lucy and Zhao, Richard and Wu, Yanhui and Tong, Xin},
	month = jan,
	year = {2021},
	pages = {68--81},
}

@book{james_introduction_2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {103},
	isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
	url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
	language = {en},
	urldate = {2021-10-19},
	publisher = {Springer New York},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	doi = {10.1007/978-1-4614-7138-7},
}

@article{hofman_prediction_2017,
	title = {Prediction and explanation in social systems},
	volume = {355},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal3856},
	doi = {10.1126/science.aal3856},
	language = {en},
	number = {6324},
	urldate = {2021-10-19},
	journal = {Science},
	author = {Hofman, Jake M. and Sharma, Amit and Watts, Duncan J.},
	month = feb,
	year = {2017},
	pages = {486--488},
}

@incollection{paltoglou_survey_2014,
	address = {Cham},
	title = {A {Survey} of {Automated} {Hierarchical} {Classification} of {Patents}},
	volume = {8830},
	isbn = {978-3-319-12510-7 978-3-319-12511-4},
	url = {http://link.springer.com/10.1007/978-3-319-12511-4_11},
	abstract = {In this era of “big data”, hundreds or even thousands of patent applications arrive every day to patent oﬃces around the world. One of the ﬁrst tasks of the professional analysts in patent oﬃces is to assign classiﬁcation codes to those patents based on their content. Such classiﬁcation codes are usually organized in hierarchical structures of concepts. Traditionally the classiﬁcation task has been done manually by professional experts. However, given the large amount of documents, the patent professionals are becoming overwhelmed. If we add that the hierarchical structures of classiﬁcation are very complex (containing thousands of categories), reliable, fast and scalable methods and algorithms are needed to help the experts in patent classiﬁcation tasks. This chapter describes, analyzes and reviews systems that, based on the textual content of patents, automatically classify such patents into a hierarchy of categories. This chapter focuses specially in the patent classiﬁcation task applied for the International Patent Classiﬁcation (IPC) hierarchy. The IPC is the most used classiﬁcation structure to organize patents, it is world-wide recognized, and several other structures use or are based on it to ensure oﬃce inter-operability.},
	language = {en},
	urldate = {2021-10-19},
	booktitle = {Professional {Search} in the {Modern} {World}},
	publisher = {Springer International Publishing},
	author = {Gomez, Juan Carlos and Moens, Marie-Francine},
	editor = {Paltoglou, Georgios and Loizides, Fernando and Hansen, Preben},
	year = {2014},
	doi = {10.1007/978-3-319-12511-4_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {215--249},
}

@article{fawcett_roc_nodate,
	title = {{ROC} {Graphs}:  {Notes} and {Practical} {Considerations} for {Data} {Mining} {Researchers}},
	abstract = {Receiver Operating Characteristics (ROC) graphs are a useful technique for organizing classiﬁers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been increasingly adopted in the machine learning and data mining research communities. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. This article serves both as a tutorial introduction to ROC graphs and as a practical guide for using them in research.},
	language = {en},
	author = {Fawcett, Tom},
	pages = {28},
}

@article{dressel_accuracy_2018,
	title = {The accuracy, fairness, and limits of predicting recidivism},
	volume = {4},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.aao5580},
	doi = {10.1126/sciadv.aao5580},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {Science Advances},
	author = {Dressel, Julia and Farid, Hany},
	month = jan,
	year = {2018},
	pages = {eaao5580},
}

@incollection{chawla_data_2005,
	address = {Boston, MA},
	title = {Data {Mining} for {Imbalanced} {Datasets}: {An} {Overview}},
	isbn = {978-0-387-25465-4},
	shorttitle = {Data {Mining} for {Imbalanced} {Datasets}},
	url = {https://doi.org/10.1007/0-387-25465-X_40},
	abstract = {A dataset is imbalanced if the classification categories are not approximately equally represented. Recent years brought increased interest in applying machine learning techniques to difficult “real-world” problems, many of which are characterized by imbalanced data. Additionally the distribution of the testing data may differ from that of the training data, and the true misclassification costs may be unknown at learning time. Predictive accuracy, a popular choice for evaluating performance of a classifier, might not be appropriate when the data is imbalanced and/or the costs of different errors vary markedly. In this Chapter, we discuss some of the sampling techniques used for balancing the datasets, and the performance measures more appropriate for mining imbalanced datasets.},
	language = {en},
	urldate = {2021-10-19},
	booktitle = {Data {Mining} and {Knowledge} {Discovery} {Handbook}},
	publisher = {Springer US},
	author = {Chawla, Nitesh V.},
	editor = {Maimon, Oded and Rokach, Lior},
	year = {2005},
	doi = {10.1007/0-387-25465-X_40},
	pages = {853--867},
}

@article{beck_machine_2018,
	title = {Machine {Learning} in {Official} {Statistics}},
	url = {http://arxiv.org/abs/1812.10422},
	abstract = {In the first half of 2018, the Federal Statistical Office of Germany (Destatis) carried out a "Proof of Concept Machine Learning" as part of its Digital Agenda. A major component of this was surveys on the use of machine learning methods in official statistics, which were conducted at selected national and international statistical institutions and among the divisions of Destatis. It was of particular interest to find out in which statistical areas and for which tasks machine learning is used and which methods are applied. This paper is intended to make the results of the surveys publicly accessible.},
	urldate = {2021-10-19},
	journal = {arXiv:1812.10422 [cs, stat]},
	author = {Beck, Martin and Dumpert, Florian and Feuerhake, Joerg},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.10422},
}

@article{amarasinghe_explainable_2021,
	title = {Explainable {Machine} {Learning} for {Public} {Policy}: {Use} {Cases}, {Gaps}, and {Research} {Directions}},
	shorttitle = {Explainable {Machine} {Learning} for {Public} {Policy}},
	url = {http://arxiv.org/abs/2010.14374},
	abstract = {Explainability is a crucial requirement for eﬀectiveness as well as the adoption of Machine Learning (ML) models supporting decisions in high-stakes public policy areas such as health, criminal justice, education, and employment. While the ﬁeld of explainable ML has expanded in recent years, much of this work has not taken real-world needs into account. A majority of proposed methods use benchmark datasets with generic explainability goals without clear use-cases or intended end-users. As a result, the applicability and eﬀectiveness of this large body of theoretical and methodological work on real-world applications is unclear. This paper focuses on ﬁlling this void for the domain of public policy. We develop a taxonomy of explainability use-cases within public policy problems; for each use-case, we deﬁne the endusers of explanations and the speciﬁc goals explainability has to fulﬁll; third, we map existing work to these use-cases, identify gaps, and propose research directions to ﬁll those gaps in order to have a practical societal impact through ML.},
	language = {en},
	urldate = {2021-10-19},
	journal = {arXiv:2010.14374 [cs]},
	author = {Amarasinghe, Kasun and Rodolfa, Kit and Lamba, Hemank and Ghani, Rayid},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.14374},
}

@misc{wolframcom_distance_2021,
	title = {Distance and {Similarity} {Measures}—{Wolfram} {Language} {Documentation}},
	url = {https://reference.wolfram.com/language/guide/DistanceAndSimilarityMeasures.html},
	urldate = {2021-10-19},
	author = {Wolfram.com},
	year = {2021},
}

@article{navarro_guided_2001,
	title = {A guided tour to approximate string matching},
	volume = {33},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/375360.375365},
	doi = {10.1145/375360.375365},
	abstract = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {ACM Computing Surveys},
	author = {Navarro, Gonzalo},
	month = mar,
	year = {2001},
	pages = {31--88},
}

@article{boytsov_indexing_2011,
	title = {Indexing methods for approximate dictionary searching: {Comparative} analysis},
	volume = {16},
	issn = {1084-6654, 1084-6654},
	shorttitle = {Indexing methods for approximate dictionary searching},
	url = {https://dl.acm.org/doi/10.1145/1963190.1963191},
	doi = {10.1145/1963190.1963191},
	abstract = {The primary goal of this article is to survey state-of-the-art indexing methods for approximate dictionary searching. To improve understanding of the field, we introduce a taxonomy that classifies all methods into direct methods and sequence-based filtering methods. We focus on infrequently updated dictionaries, which are used primarily for retrieval. Therefore, we consider indices that are optimized for retrieval rather than for update. The indices are assumed to be associative, that is, capable of storing and retrieving auxiliary information, such as string identifiers. All solutions are lossless and guarantee retrieval of strings within a specified edit distance
              k
              . Benchmark results are presented for the practically important cases of
              k
              =1, 2, and 3. We concentrate on natural language datasets, which include synthetic English and Russian dictionaries, as well as dictionaries of frequent words extracted from the ClueWeb09 collection. In addition, we carry out experiments with dictionaries containing DNA sequences. The article is concluded with a discussion of benchmark results and directions for future research.},
	language = {en},
	urldate = {2021-10-19},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Boytsov, Leonid},
	month = may,
	year = {2011},
}

@article{wiemers_forecasting_2009,
	title = {Forecasting {Behavioural} and {Distributional} {Effects} of the {Bofinger}-{Walwei} {Model} using {Microsimulation}},
	volume = {229},
	issn = {2366-049X, 0021-4027},
	url = {https://www.degruyter.com/document/doi/10.1515/jbnst-2009-0408/html},
	doi = {10.1515/jbnst-2009-0408},
	abstract = {Since Germany’s social assistance reform (“Hartz-IV-Reform”) in 2005 there has been a strong increase in the number of working poor and long-term unemployed. This development is often attributed to the remaining disincentives of the reformed social assistance to take up a low-paid full time job. Therefore, several proposals have been worked out to reduce these disincentives. In this paper we analyse an in-work benefit programme considered by the German government, which follows the proposal of Bofinger et al. (2006). We employ a microsimulation model for estimating labour supply as well as distributional and fiscal effects of this reform proposal. We provide “morning after effects”, i.e. fiscal effects without considering behavioural adjustments, and long run effects, which take into account the labour supply response following the introduction of the reform. We predict the labour supply responses by estimating a discrete choice model for different household types and find a moderate increase in labour supply (103,000 full-time equivalents) as well as overall low negative participation effects. The distributional analysis reveals an overall increase in poverty rates caused by lower earnings disregards as well as substantial deadweight losses, since a large part of the in-work benefit accrues to households who do not belong to the working poor in the status quo.},
	language = {en},
	number = {4},
	urldate = {2021-10-19},
	journal = {Jahrbücher für Nationalökonomie und Statistik},
	author = {Wiemers, Jürgen and Bruckmeier, Kerstin},
	month = aug,
	year = {2009},
	pages = {492--511},
}

@techreport{steiner_dokumentation_2005,
	type = {Research {Report}},
	title = {Dokumentation des {Steuer}-{Transfer}-{Mikrosimulationsmodells} {STSM} 1999 - 2002},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/129218},
	abstract = {EconStor ist ein Publikationsserver für wirtschaftswissenschaftliche Fachliteratur und wird von der ZBW – Leibniz-Informationszentrum Wirtschaft als öffentliche Informationsinfrastruktur betrieben.},
	language = {ger},
	number = {9},
	urldate = {2021-10-19},
	institution = {DIW Data Documentation},
	author = {Steiner, Viktor and Haan, Peter and Wrohlich, Katharina},
	year = {2005},
}

@techreport{bruckmeier_erwerbstatige_2018,
	type = {Research {Report}},
	title = {Erwerbstätige im unteren {Einkommensbereich} stärken: {Ansätze} zur {Reform} von {Arbeitslosengeld} {II}, {Wohngeld} und {Kinderzuschlag}},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	shorttitle = {Erwerbstätige im unteren {Einkommensbereich} stärken},
	url = {https://www.econstor.eu/handle/10419/204765},
	abstract = {Erwerbstätige, die mit einem nur geringen Verdienst für sich oder ihre Familie sorgen müssen, können neben ihrem Erwerbseinkommen Sozialleistungen beziehen. Unter den passiven Sozialleistungen sind die Grundsicherung für Arbeitsuchende, das Wohngeld und der Kinderzuschlag zentral. Eine Reform der drei Leistungen ist angesichts der Komplexität der Transfersysteme und hoher Grenzbelastungen für Geringverdiener notwendig. Gezielte Verbesserungen in den Transfersystemen können zu mehr Beschäftigung unter Geringverdienern beitragen, ihre Inanspruchnahme unter Bedürftigen erhöhen und Einkommensungleichheiten abbauen. Die Bundesregierung greift die bestehende Problematik auf und kündigt eine Entbürokratisierung der Leistungen an. Zudem soll geprüft werden, wie Kinderzuschlag, Wohngeld und Unterhaltsvorschuss besser aufeinander abgestimmt werden können. Der Kinderzuschlag soll ausgeweitet und durch eine Reform die Arbeitsanreize für Bezieher erhöht werden. Generell sollen Familien und Alleinerziehende durch eine Erhöhung des Kindergeldes und einer Rechtsverschiebung der Eckwerte in der Einkommensteuer entlastet werden. In diesem Forschungsbericht untersuchen wir die Auswirkungen der im Koalitionsvertrag verabredeten Maßnahmen auf das Arbeitsangebot und die Einkommensverteilung. Die Effekte werden mit dem Mikrosimulationsmodell des IAB (IAB-MSM), das auf dem Sozio-ökonomischen Panel basiert, simuliert. Den Reformvorhaben der Großen Koalition wird eine umfassende Neuausgestaltung des Transfersystems gegenübergestellt.},
	language = {ger},
	number = {9/2018},
	urldate = {2021-10-19},
	institution = {IAB-Forschungsbericht},
	author = {Bruckmeier, Kerstin and Mühlhan, Jannek and Wiemers, Jürgen},
	year = {2018},
}

@techreport{arntz_arbeitsangebotseffekte_nodate,
	address = {Nürnberg},
	type = {{IAB}-{Forschungsbericht}},
	title = {Arbeitsangebotseffekte und {Verteilungswirkungen} der {Hartz} {IV}-{Reform}},
	language = {de},
	number = {10/2007},
	institution = {Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Arntz, Melanie and Clauss, Markus and Kraus, Margit and Schnabel, Reinhold and Spermann, Alexander and Wiemers, Jürgen},
	pages = {98},
}

@article{desrosieres_statistics_2014,
	title = {Statistics and social critique},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14157},
	doi = {10.1285/i20356609v7i2p348},
	abstract = {This paper focuses on the history of the uses of statistics as a tool for socialcritique. Whereas nowadays they are very often conceived as being in the hands of the powerful, there are many historical cases when they were, on the contrary, used to oppose the authority. The author first illustrates the theory of Ted Porter according to which quantification might be a “tool of weakness”. He then addresses the fact that statistics were used in the context of labour and on living conditions, thus being a resource for the lower class of society  (and presenting the theory of statistics of Pelloutier, an anarchist activist). Finally comes the question of the conditions of success of these counterpropositions, discussed on the examples of the new random experiments in public policies, and of the measure of the 1\% of the richest persons},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Desrosières, Alain},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {348--359},
}

@article{de_rosa_gender_2014,
	title = {Gender {Statactivism} and {NGOs}: {Development} and {Use} of {Gender} {Sensitive}-{Data} for {Mobilizations} and {Women}’s {Rights}},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {Gender {Statactivism} and {NGOs}},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14156},
	doi = {10.1285/i20356609v7i2p314},
	abstract = {Historically, NGOs and civil society organizations have engaged actively in the development of new forms of gender categorizations, gender-sensitive data and gender analysis, mainly within a human rights framework. But, what is the actual space for NGOs to promote mobilization on women’s rights and, more specifically, how do NGOs develop and use gender-sensitive data for social mobilizations? Mainly based on the study of 5 worldwide NGOs and 3 networks of human rights experts, operating at different levels, the paper investigates the different ways these organizations intervene in gender issues, focusing on how they strategically produce and use categorizations and data. An initial typology is presented by analyzing «gender statactivism» of NGOs. Four phases of mobilization are identified: (a) knowledge and framing processes, (b) policy analysis, policy design and policy implementation, (c) action (campaigning and advocacy) and (d) monitoring and evaluation. The paper concludes by discussing potentiality, controversies and issues related to gender statactivism by NGOs, and stresses implications for the debate on the Post-2015 Development agenda and the potentiality for integrating «gender statactivism» into a framework of particularly transnational intersectionality},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {De Rosa, Eugenia},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {314--347},
}

@article{baudot_whos_2014,
	title = {Who's counting? {Institutional} {Autonomy} and the {Production} of {Activity} {Data} for {Disability} {Policy} in {France} (2006-2014)},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {Who's counting?},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14155},
	doi = {10.1285/i20356609v7i2p294},
	abstract = {This article deals with the use of figures for public action piloting in disability policy. This particular public action sector seems to lack piloting figures and comparable data for the monitoring of action in institutions in charge of granting social services for disabled persons, the Maisons départementales des personnes handicapées (MDPH). Based on fieldwork carried out since 2011 in this sector, this article reveals that the production of specific activity figures requires resource mobilisation and precise tasks on the part of organisations. The production of activity data therefore can reflect the autonomy of these administrative organisations. In limiting the capacity of these organisations specifically tasked with benefits attribution decisions, political authorities have also limited the autonomy they could in theory have claimed, and, in so doing, have limited their capacity to make these decisions independently of the political configurations and economic situations within which these benefits are recognised},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Baudot, Pierre-Yves},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {294--313},
}

@article{gilles_recounting_2014,
	title = {Recounting health at work counts: {A} {Tale} of {Numbers} and {Their} {Uses}},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {Recounting health at work counts},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14154},
	doi = {10.1285/i20356609v7i2p278},
	abstract = {In the early 2000s, occupational physicians at Aero, a major aerospace group, teamed up with researchers to build a new type of statistics. Embedded in the EVREST (EVolutions et RElations en Santé au Travail) scheme, these statistics rely on a questionnaire based survey administered during the medical check-up. Each year, the occupational physicians report the survey results to the social partners in several workgroups. Through the use of statistics issued from EVREST, the physicians wanted to display a link between working conditions and health issues, which was often ignored by the institution and its representation of occupational health and safety. The purpose of this article is to analyze the use of quantification, its genesis and its effects. Focussing on the "conflicting uses" around the numbers, this article demonstrates that their production is part of social relations that contribute to the redefinition of the goals initially assigned to instrument by their creators. Finally, the analysis aims to contribute to the reflection on the social conditions for the development and acceptance of alternative indicators},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Gilles, Marion},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {278--293},
}

@article{lury_downs_2014,
	title = {The downs and ups of the consumer price index in {Argentina}: {From} {National} {Statistics} to {Big} {Data}},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {The downs and ups of the consumer price index in {Argentina}},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14153},
	doi = {10.1285/i20356609v7i2p258},
	abstract = {On the 5th of February 2007, the Institute of National Statistics and Census in Argentina (INDEC) released a press statement, giving a percentage figure for that month’s Consumer Price Index (CPI-GBA).  Since the announcement, this number and its subsequent variations have been at the centre of a national and international political, legal and technical controversy. The legitimacy of the numerical value of the percentage has been called into question by a range of actors and has been challenged by the emergence of multiple alternative indicators of inflation. We explore this methodological controversy through the lens of statactivism. We do not describe the controversy in its entirety, but, rather, enter the controversy to develop a comparison of the procedures informing the production of the CPI as a national statistic with those informing its production as a big data number. In both cases, we explore the way in which price is produced as an indicator. In doing so we draw attention to the significance of calculative infrastructures as ubiquitous, multi-layered processes of connectivity, that have the capacity to make surfaces, to draw lines and boundaries, and to enable particular economic and political activities to unfold in multiple and specific ways. We argue that the capacity to connect, to attach and detach, that is immanent to such infrastructures configures price as an indicator in particular ways, and in doing so help make what we call state space, a term which we use to draw attention to how specific configurations of connectivity in the calculative infrastructure enacts a space of possibility for statactivism},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Lury, Celia and Gross, Ana},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {258--277},
}

@article{boris_statistics_2014,
	title = {Statistics and political violence: {Reflections} on the {Social} {Conflict} in 2009 in {Guadeloupe}},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {Statistics and political violence},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14152},
	doi = {10.1285/i20356609v7i2p237},
	abstract = {In early 2009, Guadeloupe experienced a 44-day strike against the high cost of living and against the illegitimate profits some actors realise in the Island’s economy. Some of these dominant actors being heirs of settlers families, high prices are the starting point for a radical political critique. This article analyses the links between violence and the use of numbers in the course of the conflict. The mobilisation was a time of violence, clashes, and intimidation; but the denunciation of abuse also ascribes a central role to quantification, in order to estimate profit. This article shows how the figures comprise an instrument of mediation being used as a substitute for, or in combination with, multifaceted violent actions. It also shows that the figures may be themselves coercive techniques, playing a part in violent relationships. Quantification can therefore be combined, in a plurality of ways, with the transition to violence, not only by avoiding or replacing it, but sometimes overlapping with it or being its instrument},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Boris, Samuel},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {237--257},
}

@article{salle_statactivism_2014,
	title = {Statactivism against the penal machinery in the aftermath of “1968”: {The} {Case} of the {French} {Groupe} d’{Information} {Sur} les {Prisons}},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {Statactivism against the penal machinery in the aftermath of “1968”},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14151},
	doi = {10.1285/i20356609v7i2p221},
	abstract = {The action of the French Groupe d’information sur les prisons (GIP) in the early 1970s has recently been characterized as “optical activism”. By analogy, this article considers the activist efforts of the GIP from the angle of statistical activism or “statactivism”. It assumes that there is something to be gained from re-examining the GIP’s activities from this perspective on the assumption that, because prison was —particularly at that time— a place of deprivation and scarcity, it was a world in which quantities, however low they may have been, did count. Quantification was not the most important of the GIP’s wide range of activities; yet it was crucial under certain circumstances, or for addressing certain issues: if information was "a weapon" (a watchword of the group), then statistical information was no exception to the rule. Emphasizing the issues of prison suicides and class justice, this article reviews different practices of statactivism, from challenging official figures to resorting to an original quantification operation. If the GIP paved the way for a critique that is now commonplace, it has also brought about a decisive and paradoxical shift, by which citing numbers no longer only answered the conventional quantitative question “how many?” (how many prisoners?), but also answered the qualitative and more disturbing question “who?”: who are the prisoners?},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Salle, Grégory},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {221--236},
}

@article{bruno_statactivism_2014,
	title = {Statactivism: {Forms} of action between disclosure and affirmation},
	volume = {7},
	copyright = {Authors who publish with  PACO  agree to the  Creative Commons Attribution-Non commercial-Share alike 3.0 Italian License     Copyrights of each article are hold by the University of Salento.   PACO  allows author(s) to retain publishing rights under permission of the Editorial Staff. But Authors are requested to always indicate that the first version of the article has been published in "Partecipazione e conflitto".},
	issn = {2035-6609},
	shorttitle = {Statactivism},
	url = {http://siba-ese.unisalento.it/index.php/paco/article/view/14150},
	doi = {10.1285/i20356609v7i2p198},
	abstract = {This article introduces the special issue on statactivism, a particular form of action within the repertoire used by contemporary social movements: the mobilization of statistics. Traditionally, statistics has been used by the worker movement within the class conflicts. But in the current configuration of state restructuring, new accumulation regimes, and changes in work organization in capitalists societies, the activist use of statistics is moving. This first article seeks to show the use of statistics and quantification in contentious performances connected with state restructuring, main transformations of the varieties of capitalisms, and changes in work organization regimes. The double role of statistics in representing as well as criticizing reality is considered. After showing how important statistical tools are in producing a shared reading of reality, we will discuss the two main dimensions of statactivism – disclosure and affirmation. In other words, we will see the role of stat-activists in denouncing a certain state of reality, and then the efforts to use statistics in creating equivalency among disparate conditions and in cementing emerging social categories. Finally, we present the main contributions of the various research papers in this special issue regarding the use of statistics as a form of action within a larger repertoire of contentious action. Six empirical papers focus on statactivism against the penal machinery in the early 1970s (Grégory Salle), on the mobilisation on the price index in Guadalupe in 2009 (Boris Samuel), and in Argentina in 2007 (Celia Lury and Ana Gross), on the mobilisations of experts to consolidate a link between working conditions and health issues (Marion Gilles), on the production of activity data for disability policy in France (Pierre-Yves Baudot), and on the use of statistics in social mobilizations for gender equality (Eugenia De Rosa). Alain Desrosières wrote the last paper, coping with mobilizations proposing innovations in the way of measuring inflation, unemployment, poverty, GDP, and climate change. This special issue is dedicated to him, in order to honor his everlasting intellectual legacy},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {PARTECIPAZIONE E CONFLITTO},
	author = {Bruno, Isabelle and Didier, Emmanuel and Vitale, Tommaso},
	month = jul,
	year = {2014},
	note = {Number: 2},
	pages = {198--220},
}

@misc{noauthor_vol_nodate,
	title = {Vol. 7, {No}. 2 (2014). {Special} issue: {Statistics} and {Activism}},
	url = {http://siba-ese.unisalento.it/index.php/paco/issue/view/1248},
	urldate = {2021-10-19},
}

@article{hand_trustworthiness_nodate,
	title = {Trustworthiness of statistical inference},
	volume = {n/a},
	issn = {1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12752},
	doi = {10.1111/rssa.12752},
	abstract = {We examine the role of trustworthiness and trust in statistical inference, arguing that it is the extent of trustworthiness in inferential statistical tools which enables trust in the conclusions. Certain tools, such as the p-value and significance test, have recently come under renewed criticism, with some arguing that they damage trust in statistics. We argue the contrary, beginning from the position that the central role of these methods is to form the basis for trusted conclusions in the face of uncertainty in the data, and noting that it is the misuse and misunderstanding of these tools which damages trustworthiness and hence trust. We go on to argue that recent calls to ban these tools tackle the symptom, not the cause, and themselves risk damaging the capability of science to advance, as well as risking feeding into public suspicion of the discipline of statistics. The consequence could be aggravated mistrust of our discipline and of science more generally. In short, the very proposals could work in quite the contrary direction from that intended. We make some alternative proposals for tackling the misuse and misunderstanding of these methods, and for how trust in our discipline might be promoted.},
	language = {en},
	number = {n/a},
	urldate = {2021-10-19},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Hand, David J.},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12752},
}

@book{levy_sampling_2008,
	address = {Hoboken, NJ, USA},
	title = {Sampling of {Populations}: {Methods} and {Applications}},
	isbn = {978-0-470-37459-7 978-0-470-04007-2},
	shorttitle = {Sampling of {Populations}},
	url = {http://doi.wiley.com/10.1002/9780470374597},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Levy, Paul S. and Lemeshow, Stanley},
	month = jul,
	year = {2008},
	doi = {10.1002/9780470374597},
}

@misc{infas_360_infas360_kompetenzforum-small-area-methoden_programmpdf_2018,
	title = {infas360\_Kompetenzforum-{Small}-{Area}-{Methoden}\_Programm.pdf},
	author = {infas 360},
	year = {2018},
}

@article{burton_design_2006,
	title = {The design of simulation studies in medical statistics},
	volume = {25},
	issn = {02776715, 10970258},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.2673},
	doi = {10.1002/sim.2673},
	abstract = {Simulation studies use computer intensive procedures to assess the performance of a variety of statistical methods in relation to a known truth. Such evaluation cannot be achieved with studies of real data alone. Designing high-quality simulations that reﬂect the complex situations seen in practice, such as in prognostic factors studies, is not a simple process. Unfortunately, very few published simulation studies provide sufﬁcient details to allow readers to understand fully all the processes required to design a simulation study. When planning a simulation study, it is recommended that a detailed protocol be produced, giving full details of how the study will be performed, analysed and reported. This paper details the important considerations necessary when designing any simulation study, including deﬁning speciﬁc objectives of the study, determining the procedures for generating the data sets and the number of simulations to perform. A checklist highlighting the important considerations when designing a simulation study is provided. A small review of the literature identiﬁes the current practices within published simulation studies. Copyright q 2006 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {24},
	urldate = {2021-10-18},
	journal = {Statistics in Medicine},
	author = {Burton, Andrea and Altman, Douglas G. and Royston, Patrick and Holder, Roger L.},
	month = dec,
	year = {2006},
	pages = {4279--4292},
}

@article{kleiber_reproducible_2013,
	title = {Reproducible {Econometric} {Simulations}},
	volume = {2},
	issn = {2156-6674},
	url = {https://www.degruyter.com/document/doi/10.1515/jem-2012-0004/html},
	doi = {10.1515/jem-2012-0004},
	abstract = {Reproducibility of economic research has attracted considerable attention in recent years. So far, the discussion has focused mainly on reproducibility of empirical analyses. This paper addresses a further aspect of reproducibility, the reproducibility of computational experiments. More specifically, we contribute to the emerging literature on reproducibility in economics along three lines: (i) we document how simulations of various types are an integral part of publications in modern econometrics, (ii) we provide some general guidelines about how to set up reproducible simulation experiments, and, finally, (iii) we provide a case study from time series econometrics that illustrates the main issues arising in connection with reproducibility, emphasizing the use of modular tools.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Journal of Econometric Methods},
	author = {Kleiber, Christian and Zeileis, Achim},
	month = jul,
	year = {2013},
	note = {Publisher: De Gruyter},
	pages = {89--99},
}

@misc{hable_prasentation_2012,
	type = {Lecture},
	title = {Präsentation wissenschaftlicher {Arbeiten} in {Statistik} und {Simulationen}},
	author = {Hable, Robert},
	year = {2012},
}

@misc{groemping_cran_2021,
	title = {{CRAN} {Task} {View}: {Design} of {Experiments} ({DoE}) \& {Analysis} of {Experimental} {Data}},
	shorttitle = {{CRAN} {Task} {View}},
	url = {https://CRAN.R-project.org/view=ExperimentalDesign},
	urldate = {2021-10-18},
	author = {Groemping, Ulrike},
	month = jun,
	year = {2021},
}

@book{bischof_computersimulationen_2017,
	address = {Darmstadt},
	title = {Computersimulationen verstehen. {Ein} {Toolkit} für interdisziplinär {Forschende} aus den {Geistes}- und {Sozialwissenschaften}},
	copyright = {CC-BY-SA 4.0 International - Creative Commons, Namensnennung, Weitergabe unter gleichen Bedingungen},
	url = {http://tuprints.ulb.tu-darmstadt.de/6079/},
	abstract = {Das Toolkit, eine Art Hand- oder Überblicksbuch soll Geistes-, Sozial- und Kulturwissenschaftlern die Möglichkeit eröffnen, sich auf breiterer Basis und umfassender mit methodischen Aspekten der Computersimulation zu befassen. Kapitel 1 führt in die insbesondere philosophischen Herausforderungen des Hochleistungsrechnens ein. Kapitel 2 bietet eine Übersicht über die grundlegenden methodischen Zugänge. Kapitel 3 führt
dann in die Details einer Simulationsmethode hinein, es widmet sich den gleichungsbasierten Simulationen.},
	language = {de},
	urldate = {2021-10-18},
	author = {Bischof, Christian and Formanek, Nico and Gehring, Petra and Herrmann, Michael and Hubig, Christoph and Kaminski, Andreas and Wolf, Felix},
	year = {2017},
}

@book{carsey_monte_2014,
	address = {Los Angeles},
	title = {Monte {Carlo} simulation and resampling methods for social science},
	isbn = {978-1-4522-8890-1},
	publisher = {Sage},
	author = {Carsey, Thomas M. and Harden, Jeffrey J.},
	year = {2014},
}

@article{silberzahn_many_2018,
	title = {Many {Analysts}, {One} {Data} {Set}: {Making} {Transparent} {How} {Variations} in {Analytic} {Choices} {Affect} {Results}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Many {Analysts}, {One} {Data} {Set}},
	url = {http://journals.sagepub.com/doi/10.1177/2515245917747646},
	doi = {10.1177/2515245917747646},
	abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
	language = {en},
	number = {3},
	urldate = {2021-10-18},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
	month = sep,
	year = {2018},
	pages = {337--356},
}

@article{roulston_evaluating_2002,
	title = {Evaluating {Probabilistic} {Forecasts} {Using} {Information} {Theory}},
	volume = {130},
	issn = {0027-0644, 1520-0493},
	url = {http://journals.ametsoc.org/doi/10.1175/1520-0493(2002)130<1653:EPFUIT>2.0.CO;2},
	doi = {10.1175/1520-0493(2002)130<1653:EPFUIT>2.0.CO;2},
	abstract = {The problem of assessing the quality of an operational forecasting system that produces probabilistic forecasts is addressed using information theory. A measure of the quality of the forecasting scheme, based on the amount of a data compression it allows, is outlined. This measure, called ignorance, is a logarithmic scoring rule that is a modiﬁed version of relative entropy and can be calculated for real forecasts and realizations. It is equivalent to the expected returns that would be obtained by placing bets proportional to the forecast probabilities. Like the cost–loss score, ignorance is not equivalent to the Brier score, but, unlike cost–loss scores, ignorance easily generalizes beyond binary decision scenarios. The use of the skill score is illustrated by evaluating the ECMWF ensemble forecasts for temperature at London’s Heathrow airport.},
	language = {en},
	number = {6},
	urldate = {2021-10-18},
	journal = {Monthly Weather Review},
	author = {Roulston, Mark S. and Smith, Leonard A.},
	month = jun,
	year = {2002},
	pages = {1653--1660},
}

@article{hoeting_bayesian_1999,
	title = {Bayesian {Model} {Averaging}: {A} {Tutorial}},
	volume = {14},
	issn = {0883-4237},
	shorttitle = {Bayesian model averaging},
	url = {https://projecteuclid.org/journals/statistical-science/volume-14/issue-4/Bayesian-model-averaging--a-tutorial-with-comments-by-M/10.1214/ss/1009212519.full},
	doi = {10.1214/ss/1009212519},
	abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-conﬁdent inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-ofsample predictive performance. We also provide a catalogue of currently available BMA software.},
	language = {en},
	number = {4},
	urldate = {2021-10-18},
	journal = {Statistical Science},
	author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
	month = nov,
	year = {1999},
	pages = {382--417},
}

@article{mcshane_large-scale_2019,
	title = {Large-{Scale} {Replication} {Projects} in {Contemporary} {Psychological} {Research}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1505655},
	doi = {10.1080/00031305.2018.1505655},
	abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next—an issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching large-scale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous—even if the sign of the effect is consistent—the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
	number = {sup1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {McShane, Blakeley B. and Tackett, Jennifer L. and Böckenholt, Ulf and Gelman, Andrew},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1505655},
	pages = {99--105},
}

@article{hubbard_quality_2019,
	title = {Quality {Control} for {Scientific} {Research}: {Addressing} {Reproducibility}, {Responsiveness}, and {Relevance}},
	volume = {73},
	issn = {0003-1305},
	shorttitle = {Quality {Control} for {Scientific} {Research}},
	url = {https://doi.org/10.1080/00031305.2018.1543138},
	doi = {10.1080/00031305.2018.1543138},
	abstract = {Efforts to address a reproducibility crisis have generated several valid proposals for improving the quality of scientific research. We argue there is also need to address the separate but related issues of relevance and responsiveness. To address relevance, researchers must produce what decision makers actually need to inform investments and public policy—that is, the probability that a claim is true or the probability distribution of an effect size given the data. The term responsiveness refers to the irregularity and delay in which issues about the quality of research are brought to light. Instead of relying on the good fortune that some motivated researchers will periodically conduct efforts to reveal potential shortcomings of published research, we could establish a continuous quality-control process for scientific research itself. Quality metrics could be designed through the application of this statistical process control for the research enterprise. We argue that one quality control metric—the probability that a research hypothesis is true—is required to address at least relevance and may also be part of the solution for improving responsiveness and reproducibility. This article proposes a “straw man” solution which could be the basis of implementing these improvements. As part of this solution, we propose one way to “bootstrap” priors. The processes required for improving reproducibility and relevance can also be part of a comprehensive statistical quality control for science itself by making continuously monitored metrics about the scientific performance of a field of research.},
	number = {sup1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {Hubbard, Douglas W. and Carriquiry, Alicia L.},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1543138},
	pages = {46--55},
}

@article{goodman_why_2019,
	title = {Why is {Getting} {Rid} of {P}-{Values} {So} {Hard}? {Musings} on {Science} and {Statistics}},
	volume = {73},
	issn = {0003-1305},
	shorttitle = {Why is {Getting} {Rid} of {P}-{Values} {So} {Hard}?},
	url = {https://doi.org/10.1080/00031305.2018.1558111},
	doi = {10.1080/00031305.2018.1558111},
	abstract = {The current concerns about reproducibility have focused attention on proper use of statistics across the sciences. This gives statisticians an extraordinary opportunity to change what are widely regarded as statistical practices detrimental to the cause of good science. However, how that should be done is enormously complex, made more difficult by the balkanization of research methods and statistical traditions across scientific subdisciplines. Working within those sciences while also allying with science reform movements—operating simultaneously on the micro and macro levels—are the key to making lasting change in applied science.},
	number = {sup1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {Goodman, Steven N.},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1558111},
	pages = {26--30},
}

@techreport{bollen_social_2015,
	type = {Report of the {Subcommittee} on {Replicability} in {Science} {Advisory} {Committee} to the {National} {Science} {Foundation} {Directorate} for {Social}, {Behavioral}, and {Economic} {Sciences}.},
	title = {Social, {Behavioral}, and {Economic} {Sciences} {Perspectives} on {Robust} and {Reliable} {Science}},
	institution = {National Science Foundation},
	author = {Bollen, Kenneth and Cacioppo, John T. and Kaplan, Robert M. and Krosnick, Jon A. and Olds, James L.},
	month = may,
	year = {2015},
	pages = {29},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {There is increasing concern that most current published research ﬁndings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientiﬁc ﬁeld. In this framework, a research ﬁnding is less likely to be true when the studies conducted in a ﬁeld are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater ﬂexibility in designs, deﬁnitions, outcomes, and analytical modes; when there is greater ﬁnancial and other interest and prejudice; and when more teams are involved in a scientiﬁc ﬁeld in chase of statistical signiﬁcance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientiﬁc ﬁelds, claimed research ﬁndings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2021-10-18},
	journal = {PLoS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pages = {e124},
}

@book{randall_irreproducibility_2018,
	title = {The irreproducibility crisis of modern science: causes, consequences, and the road to reform},
	isbn = {978-0-9986635-5-5},
	shorttitle = {The irreproducibility crisis of modern science},
	url = {https://www.nas.org/images/documents/irreproducibility_report/NAS_irreproducibilityReport.pdf},
	abstract = {The inability of science to discern truth properly and its politicization go hand in hand. The "Irreproducibility Crisis" builds on this history of concern over the threats to scientific integrity, but it is also a departure. In this case, we are calling out a particular class of errors in contemporary science. Those errors are sometimes connected to the politicization of the sciences and scientific misconduct, but sometimes not. The reforms we call for would make for better science in the sense of limiting needless errors, but those reforms would also narrow the opportunities for sloppy political advocacy and damaging government edicts. This report deals with an epistemic problem, which is most visible in the large numbers of articles in reputable peer-reviewed journals in the sciences that have turned out to be invalid or highly questionable. Findings from experimental work or observational studies turn out, time and again, to be irreproducible. The high rates of irreproducibility are an ongoing scandal that rightly has upset a large portion of the scientific community},
	language = {en},
	urldate = {2021-10-18},
	author = {Randall, David and Welser, Chrisopher and {National Association of Scholars (U.S.)}},
	year = {2018},
	note = {OCLC: 1046657692},
}

@article{mcshane_abandon_2019,
	title = {Abandon {Statistical} {Significance}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1527253},
	doi = {10.1080/00031305.2018.1527253},
	abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
	number = {sup1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1527253},
	pages = {235--245},
}

@techreport{benjamin_redefine_2017,
	type = {Working {Paper} {No}. 2017-3.0},
	title = {Redefine {Statistical} {Significance}},
	abstract = {We propose to change the default P-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005.},
	language = {en},
	institution = {University of Pennsylvania, Department of Criminology},
	author = {Benjamin, Daniel J and Berger, James O and Johannesson, Magnus},
	year = {2017},
	pages = {19},
}

@misc{schenck_role_2019,
	title = {Role reversal: how to test for significance when we want to demonstrate a zero-difference between groups},
	author = {Schenck, Patrick},
	month = oct,
	year = {2019},
}

@article{wasserstein_moving_2019,
	title = {Moving to a {World} {Beyond} "p {\textless} 0.05” [{Special} {Issue} {Editorial}]},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913},
	doi = {10.1080/00031305.2019.1583913},
	language = {en},
	number = {sup1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
	month = mar,
	year = {2019},
	pages = {1--19},
}

@article{ioannidis_what_2019,
	title = {What {Have} {We} ({Not}) {Learnt} from {Millions} of {Scientific} {Papers} with {P} {Values}?},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1447512},
	doi = {10.1080/00031305.2018.1447512},
	language = {en},
	number = {sup1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {Ioannidis, John P. A.},
	month = mar,
	year = {2019},
	pages = {20--25},
}

@misc{watteler_conceptualizing_2019,
	address = {Zagreb},
	title = {Conceptualizing the link between {Public} {Policy} and {Public} {Attitude} {Surveys}},
	language = {en},
	author = {Watteler, Oliver},
	month = jul,
	year = {2019},
}

@article{tversky_availability_1973,
	title = {Availability: {A} {Heuristic} for {Judging} {Frequency} and {Probability}},
	volume = {5},
	shorttitle = {Availability},
	number = {2},
	journal = {Cognitive Psychology},
	author = {Tversky, Amos and Kahneman, Daniel},
	year = {1973},
	pages = {207--232},
}

@article{ernst_permutation_2004,
	title = {Permutation {Methods}: {A} {Basis} for {Exact} {Inference}},
	volume = {19},
	issn = {0883-4237},
	shorttitle = {Permutation {Methods}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-19/issue-4/Permutation-Methods-A-Basis-for-Exact-Inference/10.1214/088342304000000396.full},
	doi = {10.1214/088342304000000396},
	abstract = {The use of permutation methods for exact inference dates back to Fisher in 1935. Since then, the practicality of such methods has increased steadily with computing power. They can now easily be employed in many situations without concern for computing difﬁculties. We discuss the reasoning behind these methods and describe situations when they are exact and distribution-free. We illustrate their use in several examples.},
	language = {en},
	number = {4},
	urldate = {2021-10-18},
	journal = {Statistical Science},
	author = {Ernst, Michael D.},
	month = nov,
	year = {2004},
}

@article{cobb_introductory_2007,
	title = {The {Introductory} {Statistics} {Course}: {A} {Ptolemaic} {Curriculum}?},
	volume = {1},
	issn = {1933-4214},
	shorttitle = {The {Introductory} {Statistics} {Course}},
	url = {https://escholarship.org/uc/item/6hb3k0nz},
	doi = {10.5070/T511000028},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Technology Innovations in Statistics Education},
	author = {Cobb, George W},
	month = oct,
	year = {2007},
}

@article{macdonald_occupation_2009,
	title = {Occupation as {Socioeconomic} {Status} or {Environmental} {Exposure}? {A} {Survey} of {Practice} {Among} {Population}-based {Cardiovascular} {Studies} in the {United} {States} (with discussion)},
	volume = {169},
	issn = {0002-9262, 1476-6256},
	shorttitle = {Occupation as {Socioeconomic} {Status} or {Environmental} {Exposure}?},
	url = {https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kwp082},
	doi = {10.1093/aje/kwp082},
	language = {en},
	number = {12},
	urldate = {2021-10-18},
	journal = {American Journal of Epidemiology},
	author = {MacDonald, L. A. and Cohen, A. and Baron, S. and Burchfiel, C. M.},
	month = jun,
	year = {2009},
	pages = {1411--1421},
}

@article{johnson_impact_1999,
	title = {The {Impact} of {Specific} {Occupation} on {Mortality} in the {U}.{S}. {National} {Longitudinal} {Mortality} {Study}},
	volume = {36},
	issn = {0070-3370},
	url = {https://www.jstor.org/stable/2648058},
	doi = {10.2307/2648058},
	abstract = {We compare mortality differences for specific and general categories of occupations using a national cohort of approximately 380,000 persons aged 25-64 from the U.S. National Longitudinal Mortality Study. Based on comparisons of relative risk obtained from Cox proportional-hazards model analyses, higher risk is observed in moving across the occupational spectrum from the technical, highly skilled occupations to less-skilled and generally more labor-intensive occupations. Mortality differences obtained for social status groups of specific occupations are almost completely accounted for by adjustments for income and education. Important differences are shown to exist for selected specific occupations beyond those accounted for by social status, income, and education. High-risk specific occupations include taxi drivers, cooks, longshoremen, and transportation operatives. Low-risk specific occupations include lawyers, natural scientists, teachers, farmers, and a variety of engineers.},
	number = {3},
	urldate = {2021-10-18},
	journal = {Demography},
	author = {Johnson, Norman J. and Sorlie, Paul D. and Backlund, Eric},
	year = {1999},
	note = {Publisher: Springer},
	pages = {355--367},
}

@article{mackenbach_changes_2016,
	title = {Changes in mortality inequalities over two decades: register based study of {European} countries},
	issn = {1756-1833},
	shorttitle = {Changes in mortality inequalities over two decades},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.i1732},
	doi = {10.1136/bmj.i1732},
	language = {en},
	urldate = {2021-10-18},
	journal = {BMJ},
	author = {Mackenbach, Johan P and Kulhánová, Ivana and Artnik, Barbara and Bopp, Matthias and Borrell, Carme and Clemens, Tom and Costa, Giuseppe and Dibben, Chris and Kalediene, Ramune and Lundberg, Olle and Martikainen, Pekka and Menvielle, Gwenn and Östergren, Olof and Prochorskas, Remigijus and Rodríguez-Sanz, Maica and Strand, Bjørn Heine and Looman, Caspar W N and de Gelder, Rianne},
	month = apr,
	year = {2016},
	pages = {i1732},
}

@article{mood_logistic_2010,
	title = {Logistic {Regression}: {Why} {We} {Cannot} {Do} {What} {We} {Think} {We} {Can} {Do}, and {What} {We} {Can} {Do} {About} {It}},
	volume = {26},
	issn = {0266-7215, 1468-2672},
	shorttitle = {Logistic {Regression}},
	url = {https://academic.oup.com/esr/article-lookup/doi/10.1093/esr/jcp006},
	doi = {10.1093/esr/jcp006},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {European Sociological Review},
	author = {Mood, C.},
	month = feb,
	year = {2010},
	pages = {67--82},
}

@article{kuha_group_2020,
	title = {On {Group} {Comparisons} {With} {Logistic} {Regression} {Models}},
	volume = {49},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124117747306},
	doi = {10.1177/0049124117747306},
	abstract = {It is widely believed that regression models for binary responses are problematic if we want to compare estimated coefficients from models for different groups or with different explanatory variables. This concern has two forms. The first arises if the binary model is treated as an estimate of a model for an unobserved continuous response and the second when models are compared between groups that have different distributions of other causes of the binary response. We argue that these concerns are usually misplaced. The first of them is only relevant if the unobserved continuous response is really the subject of substantive interest. If it is, the problem should be addressed through better measurement of this response. The second concern refers to a situation which is unavoidable but unproblematic, in that causal effects and descriptive associations are inherently group dependent and can be compared as long as they are correctly estimated.},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Sociological Methods \& Research},
	author = {Kuha, Jouni and Mills, Colin},
	month = may,
	year = {2020},
	pages = {498--525},
}

@article{karlson_comparing_2012,
	title = {Comparing {Regression} {Coefficients} {Between} {Same}-sample {Nested} {Models} {Using} {Logit} and {Probit}: {A} {New} {Method}},
	volume = {42},
	issn = {0081-1750, 1467-9531},
	shorttitle = {Comparing {Regression} {Coefficients} {Between} {Same}-sample {Nested} {Models} {Using} {Logit} and {Probit}},
	url = {http://journals.sagepub.com/doi/10.1177/0081175012444861},
	doi = {10.1177/0081175012444861},
	abstract = {Logit and probit models are widely used in empirical sociological research. However, the common practice of comparing the coefficients of a given variable across differently specified models fitted to the same sample does not warrant the same interpretation in logits and probits as in linear regression. Unlike linear models, the change in the coefficient of the variable of interest cannot be straightforwardly attributed to the inclusion of confounding variables. The reason for this is that the variance of the underlying latent variable is not identified and will differ between models. We refer to this as the problem of rescaling. We propose a solution that allows researchers to assess the influence of confounding relative to the influence of rescaling, and we develop a test to assess the statistical significance of confounding. A further problem in making comparisons is that, in most cases, the error distribution, and not just its variance, will differ across models. Monte Carlo analyses indicate that other methods that have been proposed for dealing with the rescaling problem can lead to mistaken inferences if the error distributions are very different. In contrast, in all scenarios studied, our approach performs as least as well as, and in some cases better than, others when faced with differences in the error distributions. We present an example of our method using data from the National Education Longitudinal Study.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Sociological Methodology},
	author = {Karlson, Kristian Bernt and Holm, Anders and Breen, Richard},
	month = aug,
	year = {2012},
	pages = {286--313},
}

@article{best_modellvergleich_2012,
	title = {Modellvergleich und {Ergebnisinterpretation} in {Logit}- und {Probit}-{Regressionen}},
	volume = {64},
	issn = {0023-2653, 1861-891X},
	url = {http://link.springer.com/10.1007/s11577-012-0167-4},
	doi = {10.1007/s11577-012-0167-4},
	language = {de},
	number = {2},
	urldate = {2021-10-18},
	journal = {KZfSS Kölner Zeitschrift für Soziologie und Sozialpsychologie},
	author = {Best, Henning and Wolf, Christof},
	month = jun,
	year = {2012},
	pages = {377--395},
}

@article{auspurg_gruppenvergleiche_2011,
	title = {Gruppenvergleiche bei {Regressionen} mit binären abhängigen {Variablen} – {Probleme} und {Fehleinschätzungen} am {Beispiel} von {Bildungschancen} im {Kohortenverlauf} / {Group} {Comparisons} for {Regression} {Models} with {Binary} {Dependent} {Variables} – {Problems} and {Pitfalls} {Illustrated} by {Differences} in {Educational} {Opportunities} between {Cohorts}},
	volume = {40},
	issn = {2366-0325, 0340-1804},
	url = {https://www.degruyter.com/document/doi/10.1515/zfsoz-2011-0104/html},
	doi = {10.1515/zfsoz-2011-0104},
	abstract = {This research note refers to a known, but rarely noticed problem which arises when coefficients from regression models with binary dependent variables are compared over groups like cohorts or social classes. In order to attain valid and viable comparisons of coefficients and odds ratios (OR) from logit and probit models between groups, it has to be assumed that the unobserved heterogeneity is equal for all these groups. This is obviously an unrealistic assumption if data stem from different cohorts or if estimations are based on data from different countries and samples. Therefore, we propose for group comparisons the use of average marginal effects instead of OR. Moreover, we suggest a method of testing group differences in such models for statistical significance. Using the example of comparing educational opportunities over different birth cohorts, we illustrate that considering the problem of unobserved heterogeneity leads to significantly different conclusions.},
	language = {de},
	number = {1},
	urldate = {2021-10-18},
	journal = {Zeitschrift für Soziologie},
	author = {Auspurg, Katrin and Hinz, Thomas},
	month = feb,
	year = {2011},
	pages = {62--73},
}

@article{raftery_discussion_2003,
	title = {Discussion: {Performance} of {Bayesian} {Model} {Averaging}},
	volume = {98},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Discussion},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214503000000891},
	doi = {10.1198/016214503000000891},
	language = {en},
	number = {464},
	urldate = {2021-10-18},
	journal = {Journal of the American Statistical Association},
	author = {Raftery, Adrian E and Zheng, Yingye},
	month = dec,
	year = {2003},
	pages = {931--938},
}

@article{onorante_dynamic_2016,
	title = {Dynamic model averaging in large model spaces using dynamic {Occam}׳s window},
	volume = {81},
	issn = {00142921},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0014292115001099},
	doi = {10.1016/j.euroecorev.2015.07.013},
	abstract = {Bayesian model averaging has become a widely used approach to accounting for uncertainty about the structural form of the model generating the data. When data arrive sequentially and the generating model can change over time, Dynamic Model Averaging (DMA) extends model averaging to deal with this situation. Often in macroeconomics, however, many candidate explanatory variables are available and the number of possible models becomes too large for DMA to be applied in its original form. We propose a new method for this situation which allows us to perform DMA without considering the whole model space, but using a subset of models and dynamically optimizing the choice of models at each point in time. This yields a dynamic form of Occam's window. We evaluate the method in the context of the problem of nowcasting GDP in the Euro area. We find that its forecasting performance compares well with that of other methods.},
	language = {en},
	urldate = {2021-10-18},
	journal = {European Economic Review},
	author = {Onorante, Luca and Raftery, Adrian E.},
	month = jan,
	year = {2016},
	pages = {2--14},
}

@article{hjort_frequentist_2003,
	title = {Frequentist {Model} {Average} {Estimators}},
	volume = {98},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214503000000828},
	doi = {10.1198/016214503000000828},
	language = {en},
	number = {464},
	urldate = {2021-10-18},
	journal = {Journal of the American Statistical Association},
	author = {Hjort, Nils Lid and Claeskens, Gerda},
	month = dec,
	year = {2003},
	pages = {879--899},
}

@inproceedings{snow_cheap_2008,
	address = {USA},
	series = {{EMNLP} '08},
	title = {Cheap and fast---but is it good? evaluating non-expert annotations for natural language tasks},
	shorttitle = {Cheap and fast---but is it good?},
	abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng, Andrew Y.},
	month = oct,
	year = {2008},
	pages = {254--263},
}

@book{wirth_qualitatskriterien_2015,
	address = {Köln},
	series = {Methoden und {Forschungslogik} der {Kommunikationswissenschaft}},
	title = {Qualitätskriterien in der {Inhaltsanalyse}},
	isbn = {978-3-86962-151-7 978-3-86962-150-0},
	language = {ger},
	number = {12},
	publisher = {von Halem},
	editor = {Wirth, Werner and Degen, Martin},
	year = {2015},
}

@article{potter_rethinking_1999,
	title = {Rethinking validity and reliability in content analysis},
	volume = {27},
	issn = {0090-9882, 1479-5752},
	url = {https://www.tandfonline.com/doi/full/10.1080/00909889909365539},
	doi = {10.1080/00909889909365539},
	language = {en},
	number = {3},
	urldate = {2021-10-18},
	journal = {Journal of Applied Communication Research},
	author = {Potter, W. James and Levine‐Donnerstein, Deborah},
	month = aug,
	year = {1999},
	pages = {258--284},
}

@article{chandler_conducting_2016,
	title = {Conducting {Clinical} {Research} {Using} {Crowdsourced} {Convenience} {Samples}},
	volume = {12},
	issn = {1548-5943, 1548-5951},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-clinpsy-021815-093623},
	doi = {10.1146/annurev-clinpsy-021815-093623},
	abstract = {Crowdsourcing has had a dramatic impact on the speed and scale at which scientiﬁc research can be conducted. Clinical scientists have particularly beneﬁted from readily available research study participants and streamlined recruiting and payment systems afforded by Amazon Mechanical Turk (MTurk), a popular labor market for crowdsourcing workers. MTurk has been used in this capacity for more than ﬁve years. The popularity and novelty of the platform have spurred numerous methodological investigations, making it the most studied nonprobability sample available to researchers. This article summarizes what is known about MTurk sample composition and data quality with an emphasis on ﬁndings relevant to clinical psychological research. It then addresses methodological issues with using MTurk—many of which are common to other nonprobability samples but unfamiliar to clinical science researchers—and suggests concrete steps to avoid these issues or minimize their impact.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Annual Review of Clinical Psychology},
	author = {Chandler, Jesse and Shapiro, Danielle},
	month = mar,
	year = {2016},
	pages = {53--81},
}

@article{lamprianou_easuring_2020,
	title = {Μeasuring and {Visualizing} {Coders}’ {Reliability}: {New} {Approaches} and {Guidelines} {From} {Experimental} {Data}},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Μeasuring and {Visualizing} {Coders}’ {Reliability}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124120926198},
	doi = {10.1177/0049124120926198},
	abstract = {This study investigates inter- and intracoder reliability, proposing a new approach based on social network analysis (SNA) and exponential random graph models (ERGM). During a recent exit poll, the responses of voters to two open-ended questions were recorded. A coding experiment was conducted where a group of coders coded a sample of text segments. Analyzing the data, we show that the proposed SNA/ERGM method extends significantly our analytical leverage, beyond what popular tools such as Krippendorff’s a and Fleiss’s k have to offer. The reliability of coding for individual coders differed significantly for the two questions although they were very similar and the same codebook was used. We conclude that the main advantages of the proposed SNA/ERGM method are the intuitive visualizations and the nuanced measurements. Detailed guidelines are provided for practitioners who would like to use the proposed method in operational settings.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Sociological Methods \& Research},
	author = {Lamprianou, Iasonas},
	month = jun,
	year = {2020},
	pages = {004912412092619},
}

@article{kalton_study_1979,
	title = {A {Study} of {Coder} {Variability}},
	volume = {28},
	issn = {00359254},
	url = {https://www.jstor.org/stable/10.2307/2347199?origin=crossref},
	doi = {10.2307/2347199},
	abstract = {Surveyquestionnaireosftencontainopen-endedquestionsf, orwhichinterviewers arerequiredtorecordrespondentrse' pliesverbatimE. rrorscanariseincodingthese repliesin preparationforstatisticaalnalysis.The paperreportstheresultsof an experimenetxaminingthelevelsofreliabilitayttainedbysix professionaclodersin makingjudgementaclodingsof a sampleof responsesto six surveyquestions. A sizeabledegreeofunreliabilitwyas found,especiallywiththeuse ofgeneraland "catch-all"codes. Intra-codercorrelationcoefficientms,easuringthe correlated componenotfcodervariancew, eregenerallysmall;itis noted,howevert,hateven smallvaluesofthesecoefficienctasnleadto a substantiallossinprecisionforsurvey results.},
	language = {en},
	number = {3},
	urldate = {2021-10-18},
	journal = {Applied Statistics},
	author = {Kalton, Graham and Stowell, Richard},
	year = {1979},
	pages = {276},
}

@article{ipeirotis_repeated_2014,
	title = {Repeated labeling using multiple noisy labelers},
	volume = {28},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-013-0306-1},
	doi = {10.1007/s10618-013-0306-1},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Data Mining and Knowledge Discovery},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Sheng, Victor S. and Wang, Jing},
	month = mar,
	year = {2014},
	pages = {402--441},
}

@article{artstein_inter-coder_2008,
	title = {Inter-{Coder} {Agreement} for {Computational} {Linguistics}},
	volume = {34},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/34/4/555-596/1999},
	doi = {10.1162/coli.07-034-R2},
	abstract = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.},
	language = {en},
	number = {4},
	urldate = {2021-10-18},
	journal = {Computational Linguistics},
	author = {Artstein, Ron and Poesio, Massimo},
	month = dec,
	year = {2008},
	pages = {555--596},
}

@book{international_labour_office_international_1969,
	address = {Geneva},
	edition = {Rev. ed. 1968},
	title = {International standard classification of occupations: {ISCO}-68},
	shorttitle = {International standard classification of occupations},
	publisher = {International Labour Office},
	editor = {International Labour Office},
	year = {1969},
}

@book{international_labour_office_international_1990,
	address = {Geneva},
	edition = {Rev. ed. 1988},
	title = {International standard classification of occupations: {ISCO}-88},
	isbn = {978-92-2-106438-1},
	shorttitle = {International standard classification of occupations},
	publisher = {International Labour Office},
	editor = {International Labour Office},
	year = {1990},
}

@book{bundesanstalt_fur_arbeit_klassifizierung_1988,
	address = {Nürnberg},
	title = {Klassifizierung der {Berufe}},
	shorttitle = {{KdB} 1988},
	url = {https://statistik.arbeitsagentur.de/DE/Navigation/Grundlagen/Klassifikationen/Klassifikation-der-Berufe/KldB1975-1992/KldB1975-1992-Nav.html},
	abstract = {Statistisches Bundesamt},
	publisher = {Bundesanstalt für Arbeit},
	author = {Bundesanstalt für Arbeit},
	month = sep,
	year = {1988},
}

@book{office_for_national_statistics_standard_2010,
	address = {Basingstoke, Hampshire},
	title = {Standard occupational classification 2010},
	isbn = {978-0-230-24819-9 978-0-230-27223-1 978-0-230-27224-8},
	language = {en},
	publisher = {Palgrave Macmillan},
	author = {{Office for National Statistics} and Government Statistical Service},
	year = {2010},
	note = {OCLC: 691102853},
}

@techreport{hoffmann_coding_2001,
	address = {Geneva},
	type = {{STAT} {Working} paper {No}. 2001-2},
	title = {Coding occupation and industry in a population census},
	copyright = {Copyright and permissions © 1996-2021 International Labour Organization - /global/copyright/lang--en/index.htm},
	url = {http://www.ilo.org/global/statistics-and-databases/WCMS_087877/lang--en/index.htm},
	abstract = {This working paper covers issues which must be addressed when
questions related to ‘occupation’ and ‘industry’ are to be included in a
population census.},
	language = {en},
	urldate = {2021-10-18},
	institution = {International Labour Organization},
	author = {Hoffmann, Eivind},
	month = jun,
	year = {2001},
}

@incollection{ganzeboom_three_2003,
	address = {Boston, MA},
	title = {Three {Internationally} {Standardised} {Measures} for {Comparative} {Research} on {Occupational} {Status}},
	isbn = {978-1-4419-9186-7},
	url = {https://doi.org/10.1007/978-1-4419-9186-7_9},
	abstract = {The classification and scaling of occupations constitutes the foundation of much, if not most, research on social stratification. Whether one studies access to desirable positions in societies (such as education or income), consumer styles, health outcomes, social interaction patterns, or social values and attitudes, measures of social background will more likely than not include a measure of social position derived from occupational position. In addition, the study of access to occupations is an important research topic in its own right. Ever since it was recognised that the division of labour is the kernel of social inequality, stratification researchers have developed ways to derive social status measures from information on occupations. Typically, this involves two steps. First, information about occupations is secured in a detailed classification of several hundred categories, often census or other official classifications. In comparative research with existing data, the task is often to reconcile the various classifications that have been used to code detailed occupational information in the component studies. In a second step, these detailed occupational classifications are recoded into status measures of more manageable size and sociological relevance, depending on the preferences of researchers and the nature of their research questions. There are many derived scales and broad classifications in circulation (Grusky and van Rompaey 1992).},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {Advances in {Cross}-{National} {Comparison}: {A} {European} {Working} {Book} for {Demographic} and {Socio}-{Economic} {Variables}},
	publisher = {Springer US},
	author = {Ganzeboom, Harry B. G. and Treiman, Donald J.},
	editor = {Hoffmeyer-Zlotnik, Jürgen H. P. and Wolf, Christof},
	year = {2003},
	doi = {10.1007/978-1-4419-9186-7_9},
	pages = {159--193},
}

@incollection{hoffmann_revised_1994,
	address = {Budapest},
	title = {The revised {International} {Standard} {Classification} of {Occupations} ({ISCO}-88)},
	isbn = {978-1-85866-008-0},
	shorttitle = {The revised {ISCO}-88},
	language = {en},
	booktitle = {Labour {Statistics} for a {Market} {Economy}. {Challenges} and {Solutions} in the {Transition} {Countries} of {Central} and {Eastern} {Europe} and the {Former} {Soviet} {Union}},
	publisher = {Central European University Press},
	author = {Hoffmann, Eivind},
	editor = {Scott, M.},
	year = {1994},
	pages = {189--202},
}

@incollection{hoffmann_international_2003,
	address = {Boston, MA},
	title = {International {Statistical} {Comparisons} of {Occupational} and {Social} {Structures}},
	isbn = {978-1-4419-9186-7},
	url = {https://doi.org/10.1007/978-1-4419-9186-7_8},
	abstract = {The objective of this paper is to present ILO’s work with the International Standard Classification of Occupations (ISCO-88) as well as general issues of importance for the creation of data sets which can be used for comparative statistical studies of social and economic structures and their changes.},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {Advances in {Cross}-{National} {Comparison}: {A} {European} {Working} {Book} for {Demographic} and {Socio}-{Economic} {Variables}},
	publisher = {Springer US},
	author = {Hoffmann, Eivind},
	editor = {Hoffmeyer-Zlotnik, Jürgen H. P. and Wolf, Christof},
	year = {2003},
	doi = {10.1007/978-1-4419-9186-7_8},
	pages = {137--158},
}

@book{hoffmeyer-zlotnik_advances_2003,
	address = {Boston, MA},
	title = {Advances in {Cross}-{National} {Comparison}},
	isbn = {978-1-4613-4828-3 978-1-4419-9186-7},
	url = {http://link.springer.com/10.1007/978-1-4419-9186-7},
	language = {en},
	urldate = {2021-10-18},
	publisher = {Springer US},
	editor = {Hoffmeyer-Zlotnik, Jürgen H. P. and Wolf, Christof},
	year = {2003},
	doi = {10.1007/978-1-4419-9186-7},
}

@article{breitung_how_2021,
	title = {How far can we forecast? {Statistical} tests of the predictive content},
	volume = {36},
	issn = {1099-1255},
	shorttitle = {How far can we forecast?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2817},
	doi = {10.1002/jae.2817},
	abstract = {We develop tests for the null hypothesis that forecasts become uninformative beyond some maximum forecast horizon h∗. The forecast may result from a survey of forecasters or from an estimated parametric model. The first class of tests compares the mean-squared prediction error of the forecast to the variance of the evaluation sample, whereas the second class of tests compares it with the mean-squared prediction error of the recursive mean. We show that the forecast comparison may easily be performed by adopting the encompassing principle, which results in simple regression tests with standard asymptotic inference. Our tests are applied to forecasts of macroeconomic key variables from the survey of Consensus Economics. The results suggest that these forecasts are barely informative beyond two to four quarters ahead.},
	language = {en},
	number = {4},
	urldate = {2021-10-18},
	journal = {Journal of Applied Econometrics},
	author = {Breitung, Jörg and Knüppel, Malte},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2817},
	pages = {369--392},
}

@article{sperling_neue_1968,
	title = {Neue {Aufgaben} und {Probleme} der {Berufsstatistik}},
	volume = {52},
	journal = {Allgemeines Statistisches Archiv. Bd},
	author = {Sperling, Hans},
	year = {1968},
	pages = {259--284},
}

@article{zopfy_statistische_1955,
	title = {Die statistische {Erfassung} des {Berufes} und der sozialen {Stellung} für ökonomische und soziologische {Untersuchungen}},
	volume = {39},
	journal = {Allgemeines Statistisches Archiv. Bd},
	author = {Zopfy, Franz},
	year = {1955},
	pages = {305--311},
}

@book{united_nations_measuring_2010,
	address = {New York},
	title = {Measuring the economically active in population censuses: a handbook},
	isbn = {978-92-1-161526-5},
	shorttitle = {Measuring the economically active in population censuses},
	publisher = {United Nations. Department of Economic and Social Affairs},
	editor = {United Nations and International Labour Office},
	year = {2010},
}

@incollection{krengel_anforderungen_1980,
	address = {Berlin},
	title = {Anforderungen an eine nichttautologische {Klassifikation} intransparenter {Mengen} von sozialstatistischen {Daten}},
	isbn = {978-3-428-04694-2},
	language = {de},
	booktitle = {Empirische {Wirtschaftsforschung}: {Konzeptionen}, {Verfahren} und {Ergebnisse}: {Festschrift} für {Rolf} {Krengel} aus {Anlass} seines 60. {Geburtstages}},
	publisher = {Duncker \& Humblot},
	author = {Mertens, Dieter},
	editor = {Krengel, Rolf and Frohn, Joachim and Stäglin, Reiner},
	year = {1980},
	pages = {357--363},
}

@article{spilker_probleme_1967,
	title = {Probleme der {Zuordnung} nach dem {Schwerpunkt} der wirtschaftlichen {Tätigkeit}},
	volume = {51},
	journal = {Allgemeines Statistisches Archiv. Bd},
	author = {Spilker, Hans},
	year = {1967},
	pages = {31},
}

@book{frieling_verfahren_1980,
	address = {Stuttgart},
	series = {Betriebswirtschaftliche {Abhandlungen}},
	title = {Verfahren und {Nutzen} der {Klassifikation} von {Berufen}: {Darstellung} und {Kritik} verschiedener {Ansätze} in {Theorie} und {Praxis}},
	isbn = {978-3-7910-0282-8 978-3-7910-0281-1},
	shorttitle = {Verfahren und {Nutzen} der {Klassifikation} von {Berufen}},
	number = {n.F., Bd. 49},
	publisher = {Poeschel},
	author = {Frieling, Ekkehart},
	year = {1980},
}

@article{burgdorfer_volks-_1934,
	title = {Die {Volks}-, {Berufs}- und {Betriebszählung} 1933},
	volume = {23},
	url = {https://dstatg.de/publikationen/entwicklung-des-allgemeinen-statistischen-archivs-1890-2008/online-register/band-23},
	journal = {Allgemeines Statistisches Archiv},
	author = {Burgdörfer, Friedrich},
	year = {1934},
	pages = {145 -- 171},
}

@techreport{budlender_whither_2003,
	type = {{ILO} {Working} {Paper}},
	title = {Whither the {International} {Standard} {Classification} of {Occupations} ({ISCO}-88)?},
	url = {https://econpapers.repec.org/paper/iloilowps/993667063402676.htm},
	abstract = {Makes recommendations for updating the International Standard Classification of Occupations (ISCO-88) to reflect changes that have taken place in the world of work during the fifteen years since its development. Recommends that the basic principles and structure of the current version should not be revised.},
	urldate = {2021-10-18},
	institution = {International Labour Organization},
	author = {Budlender, Debbie},
	year = {2003},
}

@article{blaschke_beruf_nodate,
	title = {Beruf und berufliche {Verweisbarkeit}},
	language = {de},
	author = {Blaschke, Dieter and Plath, Hans-Eberhard},
	pages = {25},
}

@incollection{dostal_berufsbegriff_2002,
	series = {Beiträge zur {Arbeitsmarktund} {Berufsforschung}},
	title = {Der {Berufsbegriff} in der {Berufsforschung} des {IAB}},
	volume = {250},
	booktitle = {{IAB}-{Kompendium} {Arbeitsmarktund} {Berufsforschung}},
	publisher = {Institut für Arbeitsmarkt und Berufsforschung},
	author = {Dostal, Werner},
	editor = {Kleinhenz, Gerhard},
	year = {2002},
	pages = {463--474},
}

@article{dostal_moglichkeiten_1999,
	title = {Möglichkeiten und {Grenzen} der quantitativen {Berufsforschung} am {IAB}. {Eine} {Bestandsaufnahme}},
	volume = {1},
	number = {99},
	journal = {Mitteilungen aus der Arbeitsmarkt und Berufsforschung},
	author = {Dostal, Werner and Schade, Hans-Joachim and Parmentier, Klaus},
	year = {1999},
	pages = {41--60},
}

@incollection{hoffmann_mapping_1994,
	address = {Budapest},
	title = {Mapping a national classification of occupations into {ISCO}-88. outline of a strategy},
	isbn = {978-1-85866-008-0},
	shorttitle = {Mapping a national classification of occupations into {ISCO}-88},
	language = {en},
	booktitle = {Labour {Statistics} for a {Market} {Economy}. {Challenges} and {Solutions} in the {Transition} {Countries} of {Central} and {Eastern} {Europe} and the {Former} {Soviet} {Union}},
	publisher = {Central European University Press},
	author = {Hoffmann, Eivind},
	editor = {Chernyshev, I.},
	year = {1994},
	pages = {203--209},
}

@article{oldham_not_2010,
	title = {Not what it was and not what it will be: {The} future of job design research: {FUTURE} {OF} {JOB} {DESIGN} {RESEARCH}},
	volume = {31},
	issn = {08943796},
	shorttitle = {Not what it was and not what it will be},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/job.678},
	doi = {10.1002/job.678},
	language = {en},
	number = {2-3},
	urldate = {2021-10-18},
	journal = {Journal of Organizational Behavior},
	author = {Oldham, Greg R. and Hackman, J. Richard},
	month = feb,
	year = {2010},
	pages = {463--479},
}

@article{hackman_development_1975,
	title = {Development of the {Job} {Diagnostic} {Survey}.},
	volume = {60},
	issn = {0021-9010},
	url = {http://content.apa.org/journals/apl/60/2/159},
	doi = {10.1037/h0076546},
	abstract = {Describes the Job Diagnostic Survey (JDS) which is intended to (a) diagnose existing jobs to determine whether (and how) they might be redesigned to improve employee motivation and productivity and (b) evaluate the effects of job changes on employees. The instrument is based on a specific theory of how job design affects work motivation, and provides measures of (a) objective job dimensions, (b) individual psychological states resulting from these dimensions, (c) affective reactions of employees to the job and work setting, and (d) individual growth need strength (interpreted as the readiness of individuals to respond to "enriched" jobs). Reliability and validity data are summarized for 658 employees on 62 different jobs in 7 organizations who responded to a revised version of the instrument.},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Journal of Applied Psychology},
	author = {Hackman, J. Richard and Oldham, Greg R.},
	year = {1975},
	pages = {159--170},
}

@article{abbott_sociology_1993,
	title = {The {Sociology} of {Work} and {Occupations}},
	volume = {19},
	issn = {0360-0572, 1545-2115},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.so.19.080193.001155},
	doi = {10.1146/annurev.so.19.080193.001155},
	abstract = {I review recent studies of work and occupations. Most of this work proceeds at the individual level, studying individual characteristics of workers, qualities of the work experience, and, to a lesser extent, stages of the work experience. Structural analysis is less common and often treats structural phenomena as aggregates rather than emergents, except in the area of labor relations. A substantial literature—probably a third of the total—examines particular occupations. In general the literature is divided into two “sides”—one focused on gender, inequality, and career/life cycle issues, the other on unions, and industrial and labor relations. Between these are smaller foci on theoretical issues and on general structures of work. I conclude that with the possible exceptions of Marxism and the study of professions, subfields of work and occupations lack the synthetic theory that would enable synthesis of empirical results. I also consider the twofold role of politicization in the area: the positive role of driving empirical investigation of new areas, the negative one of taking its own politics as unproblematic},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Annual Review of Sociology},
	author = {Abbott, Andrew},
	month = aug,
	year = {1993},
	pages = {187--209},
}

@article{miner_idiosyncratic_1987,
	title = {Idiosyncratic {Jobs} in {Formalized} {Organizations}},
	volume = {32},
	issn = {00018392},
	url = {https://www.jstor.org/stable/2392908?origin=crossref},
	doi = {10.2307/2392908},
	language = {en},
	number = {3},
	urldate = {2021-10-18},
	journal = {Administrative Science Quarterly},
	author = {Miner, Anne S.},
	month = sep,
	year = {1987},
	pages = {327},
}

@incollection{cohen_jobs_2016,
	series = {Research in the {Sociology} of {Organizations}},
	title = {Jobs as {Gordian} {Knots}: {A} {New} {Perspective} {Linking} {Individuals}, {Tasks}, {Organizations}, and {Institutions}},
	isbn = {978-1-78635-436-5 978-1-78635-435-8},
	shorttitle = {Jobs as {Gordian} {Knots}},
	url = {https://www.emerald.com/insight/content/doi/10.1108/S0733-558X20160000047013/full/html},
	abstract = {Jobs fundamentally influence and are influenced by individuals, organizations, and societies. However, jobs themselves are largely conceptualized in an atomized and disembodied way. They are understood as being designed, altered, and dissolved and bringing their consequences one at a time. I advance an alternative view of jobs as a system of ties that span jobs, organizations, and the environment beyond organizational boundaries. These ties create Gordian Knots that hold jobs in place and explain how they change. I illustrate the model with case study evidence and propose an agenda for research on jobs as organizational systems.},
	language = {en},
	number = {47},
	urldate = {2021-10-18},
	booktitle = {The {Structuring} of {Work} in {Organizations}},
	publisher = {Emerald Group Publishing Limited},
	author = {Cohen, Lisa E.},
	editor = {Cohen, Lisa E. and Burton, M. Diane and Lounsbury, Michael},
	month = aug,
	year = {2016},
	doi = {10.1108/S0733-558X20160000047013},
	pages = {25--59},
}

@article{quenouille_notes_1956,
	title = {Notes on {Bias} in {Estimation}},
	volume = {43},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2332914?origin=crossref},
	doi = {10.2307/2332914},
	language = {en},
	number = {3/4},
	urldate = {2021-10-18},
	journal = {Biometrika},
	author = {Quenouille, M. H.},
	month = dec,
	year = {1956},
	pages = {353},
}

@book{udupa_sahana_artificial_2021,
	title = {Artificial {Intelligence}, {Extreme} {Speech} and the {Challenges} of {Online} {Content} {Moderation}},
	url = {https://epub.ub.uni-muenchen.de/id/eprint/76087},
	urldate = {2021-10-18},
	publisher = {Universitätsbibliothek der Ludwig-Maximilians-Universität München},
	author = {Udupa, Sahana and Hickok, Elonnai and Maronikolakis, Antonis and Schuetze, Hinrich and Csuka, Laura and Wisiorek, Axel and Nann, Leah},
	year = {2021},
	doi = {10.5282/UBM/EPUB.76087},
}

@article{gramacy_shiny_2020,
	title = {A {Shiny} {Update} to an {Old} {Experiment} {Game}},
	volume = {74},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1505659},
	doi = {10.1080/00031305.2018.1505659},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {The American Statistician},
	author = {Gramacy, Robert B.},
	month = jan,
	year = {2020},
	pages = {87--92},
}

@misc{pohle_new_2018,
	address = {Linz},
	title = {A {New} {Perspective} on {Forecast} {Evaluation} in {Econometrics} -   {Using} {Decompositions} of {Proper} {Scoring} {Rules} to {Analyze} {Deeper} {Facets} of {Forecast} {Quality} - {Nachwuchsworkshop} - {Linz} 2018},
	language = {en},
	author = {Pohle, Marc-Oliver},
	month = sep,
	year = {2018},
}

@article{winkler_scoring_1996,
	title = {Scoring rules and the evaluation of probabilities},
	volume = {5},
	issn = {1133-0686, 1863-8260},
	url = {http://link.springer.com/10.1007/BF02562681},
	doi = {10.1007/BF02562681},
	abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for "good" probabilities. This paper reviews scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of "goodness" of probabilities, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Test},
	author = {Winkler, Robert L.},
	month = jun,
	year = {1996},
	pages = {1--60},
}

@article{winkler_evaluating_1994,
	title = {Evaluating {Probabilities}: {Asymmetric} {Scoring} {Rules}},
	volume = {40},
	url = {http://www.jstor.org/stable/2632926},
	language = {en},
	number = {11},
	journal = {Management Science},
	author = {Winkler, Robert L.},
	year = {1994},
	pages = {1395--1405},
}

@article{winkler_scoring_1969,
	title = {Scoring {Rules} and the {Evaluation} of {Probability} {Assessors}},
	volume = {64},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501037},
	doi = {10.1080/01621459.1969.10501037},
	language = {en},
	number = {327},
	urldate = {2021-10-18},
	journal = {Journal of the American Statistical Association},
	author = {Winkler, Robert L.},
	month = sep,
	year = {1969},
	pages = {1073--1078},
}

@article{shuford_admissible_1966,
	title = {Admissible probability measurement procedures},
	volume = {31},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289503},
	doi = {10.1007/BF02289503},
	abstract = {Admissible probability measurement procedures utilize scoring systems with a very special property that guarantees that any student, at whatever level of knowledge or skill, can maximize his expected score if and only if he honestly reflects his degree-of-belief probabilities. Section 1 introduces the notion of a scoring system with the reproducing property and derives the necessary and sufficient condition for the case of a test item with just two possible answers. A method is given for generating a virtually inexhaustible number of scoring systems, both symmetric and asymmetric, with the reproducing property. A negative result concerning the existence of a certain subclass of reproducing scoring systems for the case of more than two possible answers is obtained. Whereas Section 1 is concerned with those instances in which the possible answers to a query are stated in the test itself, Section 2 is concerned with those instances in which the student himself must provide the possible answer(s). In this case, it is shown that a certain minor modification of a scoring system with the reproducing property yields the desired admissible probability measurement procedure.},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Psychometrika},
	author = {Shuford, Emir H. and Albert, Arthur and Edward Massengill, H.},
	month = jun,
	year = {1966},
	pages = {125--145},
}

@article{savage_elicitation_1971,
	title = {Elicitation of {Personal} {Probabilities} and {Expectations}},
	volume = {66},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482346},
	doi = {10.1080/01621459.1971.10482346},
	language = {en},
	number = {336},
	urldate = {2021-10-18},
	journal = {Journal of the American Statistical Association},
	author = {Savage, Leonard J.},
	month = dec,
	year = {1971},
	pages = {783--801},
}

@article{murphy_diagnostic_1992,
	title = {Diagnostic verification of probability forecasts},
	volume = {7},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/0169207092900288},
	doi = {10.1016/0169-2070(92)90028-8},
	abstract = {Verification of probability forecasts traditionally consists largely of the computation of a few overall performance measures. This paper outlines a diagnostic approach to the evaluation of probability forecasts. The basic elements of this approach are the joint distribution of forecasts and observations and the conditional and marginal distributions associated with factorizations of the joint distribution. These distributions and their summary measures, together with selected performance measures and their decompositions, provide potentially insightful and useful information concerning the fundamental characteristics of the forecasts of interest, the corresponding observations, and their relationship. This approach and the associated methodology are illustrated by presenting some results of an analysis of U.S. National Weather Service probability of precipitation (PoP) forecasts. The diagnostic analysis of PoP forecasts consists of graphical displays and quantitative measures describing various aspects (or attributes) of forecast quality, including calibration (or reliability), refinement, resolution, discrimination, accuracy, bias, and skill. In general, the samples of PoP forecasts examined here are relatively well-calibrated, unbiased, and skillful, but lacking to some degree in accuracy, refinement, resolution, and discrimination. Some differences in these characteristics as a function of forecast type (modelbased/subjective), season (cool/warm), and lead time are noted. Diagnostic verification of probability forecasts has obvious benefits to modelers and forecasters in terms of providing detailed feedback and suggesting ways in which forecasts might be improved.},
	language = {en},
	number = {4},
	urldate = {2021-10-18},
	journal = {International Journal of Forecasting},
	author = {Murphy, Allan H. and Winkler, Robert L.},
	month = mar,
	year = {1992},
	pages = {435--455},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2021-10-18},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	pages = {359--378},
}

@article{gneiting_making_2011,
	title = {Making and {Evaluating} {Point} {Forecasts}},
	volume = {106},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.r10138},
	doi = {10.1198/jasa.2011.r10138},
	language = {en},
	number = {494},
	urldate = {2021-10-18},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann},
	month = jun,
	year = {2011},
	pages = {746--762},
}

@article{gneiting_editorial_2008,
	title = {Editorial: {Probabilistic} forecasting},
	volume = {171},
	issn = {0964-1998, 1467-985X},
	shorttitle = {Editorial},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2007.00522.x},
	doi = {10.1111/j.1467-985X.2007.00522.x},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gneiting, Tilmann},
	month = apr,
	year = {2008},
	pages = {319--321},
}

@techreport{buja_loss_2005,
	address = {Philadelphia},
	title = {Loss {Functions} for {Binary} {Class} {Probability} {Estimation} and {Classiﬁcation}: {Structure} and {Applications}},
	url = {http://www-stat.wharton.upenn.edu/~buja/},
	institution = {University of Pennsylvania, Department of Statistics},
	author = {Buja, Andreas and Stuetzle, Werner and Shen, Yi},
	month = nov,
	year = {2005},
}

@article{bickel_comparisons_2007,
	title = {Some {Comparisons} among {Quadratic}, {Spherical}, and {Logarithmic} {Scoring} {Rules}},
	volume = {4},
	issn = {1545-8490, 1545-8504},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/deca.1070.0089},
	doi = {10.1287/deca.1070.0089},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Decision Analysis},
	author = {Bickel, J. Eric},
	month = jun,
	year = {2007},
	pages = {49--65},
}

@article{abramson_probability_1995,
	series = {Probability {Forecasting}},
	title = {Probability forecasting},
	volume = {11},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/016920709402000F},
	doi = {10.1016/0169-2070(94)02000-F},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {International Journal of Forecasting},
	author = {Abramson, Bruce and Clemen, Robert},
	month = mar,
	year = {1995},
	pages = {1--4},
}

@techreport{hanke_living_2019,
	title = {Living {Income} {Reference} {Price} for {Vanilla} from {Uganda} and {Madagascar}},
	url = {https://www.fairtrade.net/library/living-income-reference-prices-for-vanilla},
	language = {en},
	urldate = {2021-10-17},
	institution = {Fairtrade International},
	author = {Hänke, Hendrik},
	month = nov,
	year = {2019},
	pages = {82},
}

@book{anker_living_2017,
	title = {Living {Wages} {Around} the {World}},
	isbn = {978-1-78643-146-2},
	url = {http://www.elgaronline.com/view/9781786431455.xml},
	language = {en},
	urldate = {2021-10-18},
	publisher = {Edward Elgar Publishing},
	author = {Anker, Richard and Anker, Martha},
	year = {2017},
	doi = {10.4337/9781786431462},
}

@inproceedings{bolukbasi_man_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	isbn = {978-1-5108-3881-9},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	year = {2016},
	note = {event-place: Barcelona, Spain},
	pages = {4356--4364},
}

@book{lyberg_survey_1997,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Survey {Measurement} and {Process} {Quality}},
	isbn = {978-0-471-16559-0 978-1-118-49001-3},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118490013},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Wiley},
	editor = {Lyberg, Lars and Biemer, Paul and Collins, Martin and De Leeuw, Edith and Dippo, Cathryn and Schwarz, Norbert and Trewin, Dennis},
	month = feb,
	year = {1997},
	doi = {10.1002/9781118490013},
}

@incollection{granquist_editing_1997,
	address = {New York},
	title = {Editing of survey data: {How} much is enough?},
	isbn = {978-1-118-49001-3},
	shorttitle = {Editing of {Survey} {Data}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118490013.ch18},
	language = {en},
	booktitle = {Survey {Measurement} and {Process} {Quality}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Granquist, Leopold and Kovar, John},
	editor = {Lyberg, Lars and Biemer, Paul and Collins, Martin and DeLeeuw, Edith and Dippo, Cathryn and Schwarz, Norbert and Trewin, Dennis},
	year = {1997},
	doi = {10.1002/9781118490013.ch19},
	pages = {415--435},
}

@book{mckay_illari_causality_2011,
	address = {Oxford},
	title = {Causality in the {Sciences}},
	isbn = {978-0-19-957413-1},
	url = {https://oxford.universitypressscholarship.com/10.1093/acprof:oso/9780199574131.001.0001/acprof-9780199574131},
	abstract = {There is a need for integrated thinking about causality, probability, and mechanism in scientific methodology. A panoply of disciplines, ranging from epidemiology and biology through to econometrics and physics, routinely make use of these concepts to infer causal relationships. But each of these disciplines has developed its own methods, where causality and probability often seem to have different understandings, and where the mechanisms involved often look very different. This variegated situation raises the question of whether progress in understanding the tools of causal inference in some sciences can lead to progress in other sciences, or whether the sciences are really using different concepts. Causality and probability are long-established central concepts in the sciences, with a corresponding philosophical literature examining their problems. The philosophical literature examining the concept of mechanism, on the other hand, is more recent and there has been no clear account of how mechanisms relate to causality and probability. If we are to understand causal inference in the sciences, we need to develop some account of the relationship between causality, probability, and mechanism. This book represents a joint project by philosophers and scientists to tackle this question, and related issues, as they arise in a wide variety of disciplines across the sciences.},
	language = {eng},
	urldate = {2021-10-17},
	publisher = {Oxford University Press},
	editor = {McKay Illari, Phyllis and Russo, Federica and Williamson, Jon},
	year = {2011},
	doi = {10.1093/acprof:oso/9780199574131.001.0001},
}

@article{zhao_men_2017,
	title = {Men {Also} {Like} {Shopping}: {Reducing} {Gender} {Bias} {Amplification} using {Corpus}-level {Constraints}},
	shorttitle = {Men {Also} {Like} {Shopping}},
	url = {http://arxiv.org/abs/1707.09457},
	abstract = {Language is increasingly being used to deﬁne rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classiﬁcation and visual semantic role labeling. We ﬁnd that (a) datasets for these tasks contain signiﬁcant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further ampliﬁes the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias ampliﬁcation by 47.5\% and 40.5\% for multilabel classiﬁcation and visual semantic role labeling, respectively.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.09457 [cs, stat]},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.09457},
}

@book{rubin_multiple_1987,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Multiple {Imputation} for {Nonresponse} in {Surveys}},
	isbn = {978-0-470-31669-6 978-0-471-08705-2},
	url = {http://doi.wiley.com/10.1002/9780470316696},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Rubin, Donald B.},
	month = jun,
	year = {1987},
	doi = {10.1002/9780470316696},
}

@article{kusner_counterfactual_2018,
	title = {Counterfactual {Fairness}},
	url = {http://arxiv.org/abs/1703.06856},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our deﬁnition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1703.06856 [cs, stat]},
	author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
	month = mar,
	year = {2018},
	note = {arXiv: 1703.06856},
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478354},
	doi = {10.1080/01621459.1986.10478354},
	language = {en},
	number = {396},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	month = dec,
	year = {1986},
	pages = {945--960},
}

@article{gneiting_probabilistic_2014,
	title = {Probabilistic {Forecasting}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-062713-085831},
	doi = {10.1146/annurev-statistics-062713-085831},
	abstract = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples.},
	language = {en},
	number = {1},
	urldate = {2021-10-16},
	journal = {Annual Review of Statistics and Its Application},
	author = {Gneiting, Tilmann and Katzfuss, Matthias},
	month = jan,
	year = {2014},
	pages = {125--151},
}

@article{berk_fairness_2017,
	title = {Fairness in {Criminal} {Justice} {Risk} {Assessments}: {The} {State} of the {Art}},
	shorttitle = {Fairness in {Criminal} {Justice} {Risk} {Assessments}},
	url = {http://arxiv.org/abs/1703.09207},
	abstract = {Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoﬀs between diﬀerent kinds of fairness and between fairness and accuracy.
Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.
Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.
Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is diﬀerent base rates across diﬀerent legally protected groups. There is a need to consider challenging tradeoﬀs.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1703.09207 [stat]},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
	month = may,
	year = {2017},
	note = {arXiv: 1703.09207},
}

@article{king_why_2019,
	title = {Why {Propensity} {Scores} {Should} {Not} {Be} {Used} for {Matching}},
	volume = {27},
	url = {https://www.youtube.com/watch?v=rBv39pK1iEs},
	doi = {10.1017/pan.2019.11},
	number = {4},
	journal = {Political Analysis},
	author = {King, Gary and Nielsen, Richard},
	year = {2019},
	note = {Publisher: Cambridge University Press},
	pages = {435--454},
}

@article{pearl_theoretical_2018,
	title = {Theoretical {Impediments} to {Machine} {Learning} {With} {Seven} {Sparks} from the {Causal} {Revolution}},
	url = {http://arxiv.org/abs/1801.04016},
	abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.},
	urldate = {2021-10-17},
	journal = {arXiv:1801.04016 [cs, stat]},
	author = {Pearl, Judea},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.04016},
}

@misc{noauthor_summer_nodate,
	title = {Summer {Institute} 2015 {Methods} {Lectures}: {Machine} {Learning} for {Economists}},
	shorttitle = {Summer {Institute} 2015 {Methods} {Lectures}},
	url = {https://www.nber.org/lecture/summer-institute-2015-methods-lectures-machine-learning-economists},
	language = {en},
	urldate = {2021-10-17},
	journal = {NBER},
}

@book{delamater_handbook_2013,
	address = {Dordrecht},
	series = {Handbooks of {Sociology} and {Social} {Research}},
	title = {Handbook of {Social} {Psychology}},
	isbn = {978-94-007-6771-3 978-94-007-6772-0},
	url = {http://link.springer.com/10.1007/978-94-007-6772-0},
	urldate = {2021-10-17},
	publisher = {Springer Netherlands},
	editor = {DeLamater, John and Ward, Amanda},
	year = {2013},
	doi = {10.1007/978-94-007-6772-0},
}

@book{turner_handbook_2001,
	series = {Handbooks of {Sociology} and {Social} {Research}},
	title = {Handbook of {Sociological} {Theory}},
	isbn = {978-0-387-32458-6},
	url = {http://link.springer.com/10.1007/0-387-36274-6},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer US},
	editor = {Turner, Jonathan H.},
	year = {2001},
	doi = {10.1007/0-387-36274-6},
}

@misc{schenck_erroneous_2020,
	title = {Erroneous usage of control variables},
	author = {Schenck, Patrick},
	month = jan,
	year = {2020},
}

@misc{schussler_causal_2019,
	address = {MZES Social Science Data Lab},
	title = {Causal {Graphs}},
	url = {https://www.youtube.com/watch?v=sGRzQbRw2AU},
	abstract = {This workshop discusses causal graphs as a fundamental modelling framework and highly useful tool for empirical researchers in the social sciences. Questions addressed in interaction with participants include drawing and interpreting a graph, understanding d-separation, the nature of post-treatment bias and other common mistakes in observational studies, the connection of causal graphs to structural models and potential outcomes, and using them to better understand instrumental variable and mediation analysis. 

Presenter
Julian Schuessler is a PhD Student at the Graduate School of Decision Sciences at the University of Konstanz, Germany, where he is also affiliated with the Center for Data and Methods. His research focuses on public support for the European Union, political economy, and quantitative methods. His methodological interests include non-parametric causal inference, especially using graphs, and Bayesian statistics.},
	urldate = {2021-10-17},
	author = {Schüssler, Julian},
	month = nov,
	year = {2019},
}

@incollection{elwert_graphical_2013,
	address = {Dordrecht},
	title = {Graphical {Causal} {Models}},
	isbn = {978-94-007-6094-3},
	url = {https://doi.org/10.1007/978-94-007-6094-3_13},
	abstract = {This chapter discusses the use of directed acyclic graphs (DAGs) for causal inference in the observational social sciences. It focuses on DAGs’ main uses, discusses central principles, and gives applied examples. DAGs are visual representations of qualitative causal assumptions: They encode researchers’ beliefs about how the world works. Straightforward rules map these causal assumptions onto the associations and independencies in observable data. The two primary uses of DAGs are (1) determining the identifiability of causal effects from observed data and (2) deriving the testable implications of a causal model. Concepts covered in this chapter include identification, d-separation, confounding, endogenous selection, and overcontrol. Illustrative applications then demonstrate that conditioning on variables at any stage in a causal process can induce as well as remove bias, that confounding is a fundamentally causal rather than an associational concept, that conventional approaches to causal mediation analysis are often biased, and that causal inference in social networks inherently faces endogenous selection bias. The chapter discusses several graphical criteria for the identification of causal effects of single, time-point treatments (including the famous backdoor criterion), as well identification criteria for multiple, time-varying treatments.},
	booktitle = {Handbook of {Causal} {Analysis} for {Social} {Research}},
	publisher = {Springer Netherlands},
	author = {Elwert, Felix},
	editor = {Morgan, Stephen L.},
	year = {2013},
	doi = {10.1007/978-94-007-6094-3_13},
	pages = {245--273},
}

@book{morgan_handbook_2013,
	address = {Dordrecht},
	series = {Handbooks of {Sociology} and {Social} {Research}},
	title = {Handbook of {Causal} {Analysis} for {Social} {Research}},
	isbn = {978-94-007-6093-6 978-94-007-6094-3},
	url = {http://link.springer.com/10.1007/978-94-007-6094-3},
	urldate = {2021-10-17},
	publisher = {Springer Netherlands},
	editor = {Morgan, Stephen L.},
	year = {2013},
	doi = {10.1007/978-94-007-6094-3},
}

@article{elwert_endogenous_2014,
	title = {Endogenous {Selection} {Bias}: {The} {Problem} of {Conditioning} on a {Collider} {Variable}},
	volume = {40},
	issn = {0360-0572, 1545-2115},
	shorttitle = {Endogenous {Selection} {Bias}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-soc-071913-043455},
	doi = {10.1146/annurev-soc-071913-043455},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Annual Review of Sociology},
	author = {Elwert, Felix and Winship, Christopher},
	month = jul,
	year = {2014},
	pages = {31--53},
}

@article{lederer_control_2019,
	title = {Control of {Confounding} and {Reporting} of {Results} in {Causal} {Inference} {Studies}. {Guidance} for {Authors} from {Editors} of {Respiratory}, {Sleep}, and {Critical} {Care} {Journals}},
	volume = {16},
	issn = {2329-6933, 2325-6621},
	url = {https://www.atsjournals.org/doi/10.1513/AnnalsATS.201808-564PS},
	doi = {10.1513/AnnalsATS.201808-564PS},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Annals of the American Thoracic Society},
	author = {Lederer, David J. and Bell, Scott C. and Branson, Richard D. and Chalmers, James D. and Marshall, Rachel and Maslove, David M. and Ost, David E. and Punjabi, Naresh M. and Schatz, Michael and Smyth, Alan R. and Stewart, Paul W. and Suissa, Samy and Adjei, Alex A. and Akdis, Cezmi A. and Azoulay, Élie and Bakker, Jan and Ballas, Zuhair K. and Bardin, Philip G. and Barreiro, Esther and Bellomo, Rinaldo and Bernstein, Jonathan A. and Brusasco, Vito and Buchman, Timothy G. and Chokroverty, Sudhansu and Collop, Nancy A. and Crapo, James D. and Fitzgerald, Dominic A. and Hale, Lauren and Hart, Nicholas and Herth, Felix J. and Iwashyna, Theodore J. and Jenkins, Gisli and Kolb, Martin and Marks, Guy B. and Mazzone, Peter and Moorman, J. Randall and Murphy, Thomas M. and Noah, Terry L. and Reynolds, Paul and Riemann, Dieter and Russell, Richard E. and Sheikh, Aziz and Sotgiu, Giovanni and Swenson, Erik R. and Szczesniak, Rhonda and Szymusiak, Ronald and Teboul, Jean-Louis and Vincent, Jean-Louis},
	month = jan,
	year = {2019},
	pages = {22--28},
}

@misc{taylor_when_2021,
	title = {When do we actually need causal inference?},
	url = {https://www.youtube.com/watch?v=2dv7NrYExzo},
	abstract = {Talk delivered July 13, 2021. Visit https://www.nyhackr.org to learn more and follow https://twitter.com/nyhackr},
	urldate = {2021-10-17},
	author = {Taylor},
	month = jul,
	year = {2021},
}

@book{wooldridge_econometric_2010,
	address = {Cambridge, Mass},
	edition = {2nd ed},
	title = {Econometric analysis of cross section and panel data},
	isbn = {978-0-262-23258-6},
	publisher = {MIT Press},
	author = {Wooldridge, Jeffrey M.},
	year = {2010},
	note = {OCLC: ocn627701062},
}

@article{crossley_can_2017,
	title = {Can {Survey} {Participation} {Alter} {Household} {Saving} {Behaviour}?},
	volume = {127},
	issn = {0013-0133, 1468-0297},
	url = {https://academic.oup.com/ej/article/127/606/2332/5068847},
	doi = {10.1111/ecoj.12398},
	language = {en},
	number = {606},
	urldate = {2021-10-17},
	journal = {The Economic Journal},
	author = {Crossley, Thomas F. and de Bresser, Jochem and Delaney, Liam and Winter, Joachim},
	month = nov,
	year = {2017},
	pages = {2332--2357},
}

@article{hahn_role_1998,
	title = {On the {Role} of the {Propensity} {Score} in {Efficient} {Semiparametric} {Estimation} of {Average} {Treatment} {Effects}},
	volume = {66},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2998560},
	doi = {10.2307/2998560},
	abstract = {In this paper, the role of the propensity score in the efficient estimation of average treatment effects is examined. Under the assumption that the treatment is ignorable given some observed characteristics, it is shown that the propensity score is ancillary for estimation of the average treatment effects. The propensity score is not ancillary for estimation of average treatment effects on the treated. It is suggested that the marginal value of the propensity score lies entirely in the "dimension reduction." Efficient semiparametric estimators of average treatment effects and average treatment effects on the treated are shown to take the form of relevant sample averages of the data completed by the nonparametric imputation method. It is shown that the projection on the propensity score is not necessary for efficient semiparametric estimation of average treatment effects on the treated even if the propensity score is known. An application to the experimental data reveals that conditioning on the propensity score may even result in a loss of efficiency.},
	number = {2},
	urldate = {2021-10-17},
	journal = {Econometrica},
	author = {Hahn, Jinyong},
	year = {1998},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {315--331},
}

@article{robins_semiparametric_1995,
	title = {Semiparametric {Efficiency} in {Multivariate} {Regression} {Models} with {Missing} {Data}},
	volume = {90},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476494},
	doi = {10.1080/01621459.1995.10476494},
	abstract = {We consider the efficiency bound for the estimation of the parameters of semiparametric models defined solely by restrictions on the means of a vector of correlated outcomes, Y, when the data on Y are missing at random. We show that the semiparametric variance bound is the asymptotic variance of the optimal estimator in a class of inverse probability of censoring weighted estimators and that this bound is unchanged if the data are missing completely at random. For this case we study the asymptotic performance of the generalized estimating equations (GEE) estimators of mean parameters and show that the optimal GEE estimator is inefficient except for special cases. The optimal weighted estimator depends on unknown population quantities. But for monotone missing data, we propose an adaptive estimator whose asymptotic variance can achieve the bound.},
	number = {429},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea},
	month = mar,
	year = {1995},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476494},
	pages = {122--129},
}

@article{rubin_matching_1996,
	title = {Matching {Using} {Estimated} {Propensity} {Scores}: {Relating} {Theory} to {Practice}},
	volume = {52},
	issn = {0006-341X},
	shorttitle = {Matching {Using} {Estimated} {Propensity} {Scores}},
	url = {https://www.jstor.org/stable/2533160},
	doi = {10.2307/2533160},
	abstract = {Matched sampling is a standard technique in the evaluation of treatments in observational studies. Matching on estimated propensity scores comprises an important class of procedures when there are numerous matching variables. Recent theoretical work (Rubin, D. B. and Thomas, N., 1992, The Annals of Statistics 20, 1079-1093) on affinely invariant matching methods with ellipsoidal distributions provides a general framework for evaluationg the operating characteristics of such methods. Moreover, Rubin and Thomas (1992, Biometrika 79, 797-809) uses this framework to derive several analytic approximations under normality for the distribution of the first two moments of the matching variables in samples obtained by matching on estimated linear propensity scores. Here we provide a bridge between these theoretical approximations and actual practice. First, we complete and refine the nomal-based analytic approximations, thereby making it possible to apply these results to practice. Second, we perform Monte Carlo evaluations of the analytic results under normal and nonnormal ellipsoidal distributions, which confirm the accuracy of the analytic approximations, and demonstrate the predictable ways in which the approximations deviate from simulation results when normal assumptions are violated within the ellipsoidal family. Third, we apply the analytic approximations to real data with clearly nonellipsoidal distributions, and show that the thoretical expressions, although derived under artificial distributional conditions, produce useful guidance for practice. Our results delineate the wide range of settings in which matching on estimated linear propensity scores performs well, thereby providing useful information for the design of matching studies. When matching with a particular data set, our theoretical approximations provide benchmarks for expected performance under favorable conditions, thereby identifying matching variables requiring special treatment. After matching is complete and data analysis is at hand, our results provide the variances required to compute valid standard errors for common estimators.},
	number = {1},
	urldate = {2021-10-17},
	journal = {Biometrics},
	author = {Rubin, Donald B. and Thomas, Neal},
	year = {1996},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {249--264},
}

@book{fisher_design_1935,
	title = {The design of experiments},
	abstract = {Different types of experimentation are considered with reference to their logical structure, to show that valid conclusions may be drawn from them without using the disputed theory of inductive inferences, i.e., of arguing from observation to explanatory theory. This is possible if a null hypothesis is explicitly formulated when the experiment is designed; this hypothesis can never be proved, but may be disproved with whatever probability one will accept as demonstrating a positive result. Chapters II, III, and IV illustrate simple applications of the principles involved in sensitiveness, significance, tests of wider hypotheses, validity, and estimation and elimination of error. More elaborate structures are treated in later chapters. Chapter titles are: (V) the Latin square; (VI) factorial design in experimentation; (VII) confounding; (VIII) special cases of partial confounding; (IX) increase of precision by concomitant measurements: statistical control; (X) generalization of null hypotheses: fiducial probability; (XI) measurement of amount of information in general.},
	publisher = {Oliver \& Boyd},
	author = {Fisher, R. A.},
	year = {1935},
}

@book{pearl_causality_2000,
	address = {Cambridge, U.K. ; New York},
	title = {Causality: models, reasoning, and inference},
	isbn = {978-0-521-89560-6 978-0-521-77362-1},
	shorttitle = {Causality},
	url = {http://bayes.cs.ucla.edu/BOOK-2K/},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2000},
}

@book{rubin_matched_2006,
	address = {Cambridge},
	title = {Matched {Sampling} for {Causal} {Effects}},
	isbn = {978-0-511-81072-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511810725},
	urldate = {2021-10-17},
	publisher = {Cambridge University Press},
	author = {Rubin, Donald B.},
	year = {2006},
	doi = {10.1017/CBO9780511810725},
}

@article{morgan_rerandomization_2015,
	title = {Rerandomization to {Balance} {Tiers} of {Covariates}},
	volume = {110},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2015.1079528},
	doi = {10.1080/01621459.2015.1079528},
	language = {en},
	number = {512},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Morgan, Kari Lock and Rubin, Donald B.},
	month = oct,
	year = {2015},
	pages = {1412--1421},
}

@article{morgan_rerandomization_2012,
	title = {Rerandomization to improve covariate balance in experiments},
	volume = {40},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-2/Rerandomization-to-improve-covariate-balance-in-experiments/10.1214/12-AOS1008.full},
	doi = {10.1214/12-AOS1008},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {The Annals of Statistics},
	author = {Morgan, Kari Lock and Rubin, Donald B.},
	month = apr,
	year = {2012},
}

@article{petersen_causal_2014,
	title = {Causal {Models} and {Learning} from {Data}: {Integrating} {Causal} {Modeling} and {Statistical} {Estimation}},
	volume = {25},
	issn = {1044-3983},
	shorttitle = {Causal {Models} and {Learning} from {Data}},
	url = {http://journals.lww.com/00001648-201405000-00013},
	doi = {10.1097/EDE.0000000000000078},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {Epidemiology},
	author = {Petersen, Maya L. and van der Laan, Mark J.},
	month = may,
	year = {2014},
	pages = {418--426},
}

@article{j_schuemie_how_2020,
	title = {How {Confident} {Are} {We} {About} {Observational} {Findings} in {Health} {Care}: {A} {Benchmark} {Study}},
	shorttitle = {How {Confident} {Are} {We} {About} {Observational} {Findings} in {Health} {Care}},
	url = {https://hdsr.mitpress.mit.edu/pub/fxz7kr65},
	doi = {10.1162/99608f92.147cc28e},
	abstract = {Health care professionals increasingly rely on observational health care data, such as administrative claims and electronic health records, to estimate the causal effects of interventions. However, limited prior studies raise concerns about the real-world performance of the statistical and epidemiological methods that are used. We present the “Observational Health Data Sciences and Informatics (OHDSI) Methods Benchmark” that aims to evaluate the performance of effect estimation methods on real data. The benchmark comprises a gold standard, a set of metrics, and a set of open source software tools. The gold standard is a collection of real negative controls (drug-outcome pairs where no causal effect appears to exist) and synthetic positive controls (drug-outcome pairs that augment negative controls with simulated causal effects). We apply the benchmark using four large health care databases to evaluate methods commonly used in practice: the new-user cohort, self-controlled cohort, casecontrol, case-crossover, and self-controlled case series designs. The results confirm the concerns about these methods, showing that for most methods the operating characteristics deviate considerably from nominal levels. For example, in most contexts, only half of the 95\% confidence intervals we calculated contain the corresponding true effect size. We previously developed an ‘empirical calibration’ procedure to restore these characteristics and we also evaluate this procedure. While no one method dominates, self-controlled methods such as the empirically calibrated self-controlled case series perform well across a wide range of scenarios.},
	language = {en},
	urldate = {2021-10-17},
	journal = {Harvard Data Science Review},
	author = {J. Schuemie, Martijn and Soledad Cepede, M. and A. Suchard, Marc and Yang, Jianxiao and Tian Alejandro Schuler, Yuxi and B. Ryan, Patrick and Madigan, David and Hripcsak, George},
	month = jan,
	year = {2020},
}

@article{saint-mont_randomization_2015,
	title = {Randomization {Does} {Not} {Help} {Much}, {Comparability} {Does}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0132102},
	doi = {10.1371/journal.pone.0132102},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {PLOS ONE},
	author = {Saint-Mont, Uwe},
	editor = {Tu, Yu-Kang},
	month = jul,
	year = {2015},
	pages = {e0132102},
}

@article{rosenbaum_central_1983,
	title = {The central role of the propensity score in observational studies for causal effects},
	volume = {70},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/70.1.41},
	doi = {10.1093/biomet/70.1.41},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subelassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	year = {1983},
	pages = {41--55},
}

@article{rohlfing_check_2021,
	title = {Check {Your} {Truth} {Conditions}! {Clarifying} the {Relationship} between {Theories} of {Causation} and {Social} {Science} {Methods} for {Causal} {Inference}},
	volume = {50},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124119826156},
	doi = {10.1177/0049124119826156},
	abstract = {Theories of causation in philosophy ask what makes causal claims true and establish the so-called truth conditions allowing one to separate causal from noncausal relationships. We argue that social scientists should be aware of truth conditions of causal claims because they imply which method of causal inference can establish whether a specific claim holds true. A survey of social scientists shows that this is worth emphasizing because many respondents have unclear concepts of causation and link methods to philosophical criteria in an incoherent way. We link five major theories of causation to major small and large-n methods of causal inference to provide clear guidelines to researchers and improve dialogue across methods. While most theories can be linked to more than one method, we argue that structural counterfactual theories are most useful for the social sciences since they require neither social and natural laws nor physical processes to assess causal claims.},
	language = {en},
	number = {4},
	urldate = {2021-10-17},
	journal = {Sociological Methods \& Research},
	author = {Rohlfing, Ingo and Zuber, Christina Isabel},
	month = nov,
	year = {2021},
	pages = {1623--1659},
}

@techreport{what_works_clearinghouse_what_2020,
	address = {Washington, DC},
	title = {What {Works} {Clearinghouse} {Standards} {Handbook}, {Version} 4.1},
	shorttitle = {Standards {Handbook}, {V} 4.1},
	url = {https://ies.ed.gov/ncee/wwc/handbooks},
	urldate = {2021-10-17},
	institution = {U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance},
	author = {What Works Clearinghouse},
	month = jan,
	year = {2020},
	pages = {116},
}

@book{pearl_book_2018,
	address = {New York},
	title = {The book of why: the new science of cause and effect},
	isbn = {978-0-465-09761-6},
	shorttitle = {The book of why},
	abstract = {"Everyone has heard the claim, "Correlation does not imply causation." What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like "Was it the aspirin that stopped my headache?" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world"--},
	publisher = {Basic Books},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2018},
}

@article{mercer_theory_2017,
	title = {Theory and {Practice} in {Nonprobability} {Surveys}},
	volume = {81},
	issn = {0033-362X, 1537-5331},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfw060},
	doi = {10.1093/poq/nfw060},
	language = {en},
	number = {S1},
	urldate = {2021-10-17},
	journal = {Public Opinion Quarterly},
	author = {Mercer, Andrew W. and Kreuter, Frauke and Keeter, Scott and Stuart, Elizabeth A.},
	year = {2017},
	pages = {250--271},
}

@article{kohler_nonprobability_2019,
	title = {Nonprobability {Sampling} and {Causal} {Analysis}},
	volume = {6},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-030718-104951},
	doi = {10.1146/annurev-statistics-030718-104951},
	abstract = {The long-standing approach of using probability samples in social science research has come under pressure through eroding survey response rates, advanced methodology, and easier access to large amounts of data. These factors, along with an increased awareness of the pitfalls of the nonequivalent comparison group design for the estimation of causal effects, have moved the attention of applied researchers away from issues of sampling and toward issues of identification. This article discusses the usability of samples with unknown selection probabilities for various research questions. In doing so, we review assumptions necessary for descriptive and causal inference and discuss research strategies developed to overcome sampling limitations.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Annual Review of Statistics and Its Application},
	author = {Kohler, Ulrich and Kreuter, Frauke and Stuart, Elizabeth A.},
	month = mar,
	year = {2019},
	pages = {149--172},
}

@article{keiding_standardization_2014,
	title = {Standardization and {Control} for {Confounding} in {Observational} {Studies}: {A} {Historical} {Perspective}},
	volume = {29},
	issn = {0883-4237},
	shorttitle = {Standardization and {Control} for {Confounding} in {Observational} {Studies}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/Standardization-and-Control-for-Confounding-in-Observational-Studies--A/10.1214/13-STS453.full},
	doi = {10.1214/13-STS453},
	abstract = {Control for confounders in observational studies was generally handled through stratiﬁcation and standardization until the 1960s. Standardization typically reweights the stratum-speciﬁc rates so that exposure categories become comparable. With the development ﬁrst of loglinear models, soon also of nonlinear regression techniques (logistic regression, failure time regression) that the emerging computers could handle, regression modelling became the preferred approach, just as was already the case with multiple regression analysis for continuous outcomes. Since the mid 1990s it has become increasingly obvious that weighting methods are still often useful, sometimes even necessary. On this background we aim at describing the emergence of the modelling approach and the reﬁnement of the weighting approach for confounder control.},
	language = {en},
	number = {4},
	urldate = {2021-10-17},
	journal = {Statistical Science},
	author = {Keiding, Niels and Clayton, David},
	month = nov,
	year = {2014},
}

@article{kalisch_causal_2014,
	title = {Causal {Structure} {Learning} and {Inference}: {A} {Selective} {Review}},
	volume = {11},
	issn = {1684-3703},
	shorttitle = {Causal {Structure} {Learning} and {Inference}},
	url = {http://www.tandfonline.com/doi/full/10.1080/16843703.2014.11673322},
	doi = {10.1080/16843703.2014.11673322},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Quality Technology \& Quantitative Management},
	author = {Kalisch, Markus and Bühlmann, Peter},
	month = jan,
	year = {2014},
	pages = {3--21},
}

@article{imbens_recent_2009,
	title = {Recent {Developments} in the {Econometrics} of {Program} {Evaluation}},
	volume = {47},
	issn = {0022-0515},
	url = {https://pubs.aeaweb.org/doi/10.1257/jel.47.1.5},
	doi = {10.1257/jel.47.1.5},
	abstract = {Many empirical questions in economics and other social sciences depend on causal effects of programs or policies. In the last two decades, much research has been done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of, earlier work in both the statistics and econometrics literatures. It has by now reached a level of maturity that makes it an important tool in many areas of empirical research in economics, including labor economics, public finance, development economics, industrial organization, and other areas of empirical microeconomics. In this review, we discuss some of the recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more technical research.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Journal of Economic Literature},
	author = {Imbens, Guido W and Wooldridge, Jeffrey M},
	month = mar,
	year = {2009},
	pages = {5--86},
}

@article{imbens_bayesian_1997,
	title = {Bayesian inference for causal effects in randomized experiments with noncompliance},
	volume = {25},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-25/issue-1/Bayesian-inference-for-causal-effects-in-randomized-experiments-with-noncompliance/10.1214/aos/1034276631.full},
	doi = {10.1214/aos/1034276631},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {The Annals of Statistics},
	author = {Imbens, Guido W. and Rubin, Donald B.},
	month = feb,
	year = {1997},
}

@article{imai_misunderstandings_2008,
	title = {Misunderstandings between experimentalists and observationalists about causal inference},
	volume = {171},
	issn = {0964-1998, 1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2007.00527.x},
	doi = {10.1111/j.1467-985X.2007.00527.x},
	abstract = {We attempt to clarify, and suggest how to avoid, several serious misunderstandings about and fallacies of causal inference. These issues concern some of the most fundamental advantages and disadvantages of each basic research design. Problems include improper use of hypothesis tests for covariate balance between the treated and control groups, and the consequences of using randomization, blocking before randomization and matching after assignment of treatment to achieve covariate balance. Applied researchers in a wide range of scientiﬁc disciplines seem to fall prey to one or more of these fallacies and as a result make suboptimal design or analysis choices. To clarify these points, we derive a new four-part decomposition of the key estimation errors in making causal inferences. We then show how this decomposition can help scholars from different experimental and observational research traditions to understand better each other’s inferential problems and attempted solutions.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Imai, Kosuke and King, Gary and Stuart, Elizabeth A.},
	month = apr,
	year = {2008},
	pages = {481--502},
}

@article{imai_estimating_2013,
	title = {Estimating treatment effect heterogeneity in randomized program evaluation},
	volume = {7},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Estimating-treatment-effect-heterogeneity-in-randomized-program-evaluation/10.1214/12-AOAS593.full},
	doi = {10.1214/12-AOAS593},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {The Annals of Applied Statistics},
	author = {Imai, Kosuke and Ratkovic, Marc},
	month = mar,
	year = {2013},
}

@article{huber_treatment_2014,
	title = {Treatment {Evaluation} in the {Presence} of {Sample} {Selection}},
	volume = {33},
	issn = {0747-4938, 1532-4168},
	url = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2013.806197},
	doi = {10.1080/07474938.2013.806197},
	language = {en},
	number = {8},
	urldate = {2021-10-17},
	journal = {Econometric Reviews},
	author = {Huber, Martin},
	month = nov,
	year = {2014},
	pages = {869--905},
}

@article{green_modeling_2012,
	title = {Modeling {Heterogeneous} {Treatment} {Effects} in {Survey} {Experiments} with {Bayesian} {Additive} {Regression} {Trees}},
	volume = {76},
	issn = {0033-362X, 1537-5331},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfs036},
	doi = {10.1093/poq/nfs036},
	abstract = {Survey experimenters routinely test for systematically varying treatment effects by using interaction terms between the treatment indicator and covariates. Parametric models, such as linear or logistic regression, are currently used to search for systematic treatment effect heterogeneity but suffer from several shortcomings; in particular, the potential for bias due to model misspecification and the large amount of discretion they introduce into the analysis of experimental data. Here, we explicate what we believe to be a better approach. Drawing on the statistical learning literature, we discuss Bayesian Additive Regression Trees (BART), a method for analyzing treatment effect heterogeneity. BART automates the detection of nonlinear relationships and interactions, thereby reducing researchers’ discretion when analyzing experimental data. These features make BART an appealing “off-the-shelf” tool for survey experimenters who want to model systematic treatment effect heterogeneity in a flexible and robust manner. In order to illustrate how BART can be used to detect and model heterogeneous treatment effects, we reanalyze a well-known survey experiment on welfare attitudes from the General Social Survey.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {Public Opinion Quarterly},
	author = {Green, D. P. and Kern, H. L.},
	month = sep,
	year = {2012},
	pages = {491--511},
}

@article{frumento_evaluating_2012,
	title = {Evaluating the {Effect} of {Training} on {Wages} in the {Presence} of {Noncompliance}, {Nonemployment}, and {Missing} {Outcome} {Data}},
	volume = {107},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2011.643719},
	doi = {10.1080/01621459.2011.643719},
	language = {en},
	number = {498},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Frumento, Paolo and Mealli, Fabrizia and Pacini, Barbara and Rubin, Donald B.},
	month = jun,
	year = {2012},
	pages = {450--466},
}

@article{freedman_randomization_2008,
	title = {Randomization {Does} {Not} {Justify} {Logistic} {Regression}},
	volume = {23},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-23/issue-2/Randomization-Does-Not-Justify-Logistic-Regression/10.1214/08-STS262.full},
	doi = {10.1214/08-STS262},
	abstract = {The logit model is often used to analyze experimental data. However, randomization does not justify the model, so the usual estimators can be inconsistent. A consistent estimator is proposed. Neyman’s non-parametric setup is used as a benchmark. In this setup, each subject has two potential responses, one if treated and the other if untreated; only one of the two responses can be observed. Beside the mathematics, there are simulation results, a brief review of the literature, and some recommendations for practice. Key words and phrases: Models, randomization, logistic regression, logit, average predicted probability.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Statistical Science},
	author = {Freedman, David A.},
	month = may,
	year = {2008},
}

@article{frangakis_principal_2002,
	title = {Principal {Stratification} in {Causal} {Inference}},
	volume = {58},
	issn = {0006341X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2002.00021.x},
	doi = {10.1111/j.0006-341X.2002.00021.x},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Biometrics},
	author = {Frangakis, Constantine E. and Rubin, Donald B.},
	month = mar,
	year = {2002},
	pages = {21--29},
}

@article{blakely_reflection_2021,
	title = {Reflection on modern methods: when worlds collide—prediction, machine learning and causal inference},
	volume = {49},
	issn = {0300-5771, 1464-3685},
	shorttitle = {Reflection on modern methods},
	url = {https://academic.oup.com/ije/article/49/6/2058/5531243},
	doi = {10.1093/ije/dyz132},
	abstract = {Causal inference requires theory and prior knowledge to structure analyses, and is not usually thought of as an arena for the application of prediction modelling. However, contemporary causal inference methods, premised on counterfactual or potential outcomes approaches, often include processing steps before the ﬁnal estimation step. The purposes of this paper are: (i) to overview the recent emergence of prediction underpinning steps in contemporary causal inference methods as a useful perspective on contemporary causal inference methods, and (ii) explore the role of machine learning (as one approach to ‘best prediction’) in causal inference. Causal inference methods covered include propensity scores, inverse probability of treatment weights (IPTWs), G computation and targeted maximum likelihood estimation (TMLE). Machine learning has been used more for propensity scores and TMLE, and there is potential for increased use in G computation and estimation of IPTWs.},
	language = {en},
	number = {6},
	urldate = {2021-10-17},
	journal = {International Journal of Epidemiology},
	author = {Blakely, Tony and Lynch, John and Simons, Koen and Bentley, Rebecca and Rose, Sherri},
	month = jan,
	year = {2021},
	pages = {2058--2064},
}

@article{baldi_bayesian_2020,
	title = {Bayesian {Causality}},
	volume = {74},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1647876},
	doi = {10.1080/00031305.2019.1647876},
	abstract = {Although no universally accepted definition of causality exists, in practice one is often faced with the question of statistically assessing causal relationships in different settings. We present a uniform general approach to causality problems derived from the axiomatic foundations of the Bayesian statistical framework. In this approach, causality statements are viewed as hypotheses, or models, about the world and the fundamental object to be computed is the posterior distribution of the causal hypotheses, given the data and the background knowledge. Computation of the posterior, illustrated here in simple examples, may involve complex probabilistic modeling but this is no different than in any other Bayesian modeling situation. The main advantage of the approach is its connection to the axiomatic foundations of the Bayesian framework, and the general uniformity with which it can be applied to a variety of causality settings, ranging from specific to general cases, or from causes of effects to effects of causes.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {The American Statistician},
	author = {Baldi, Pierre and Shahbaba, Babak},
	month = jul,
	year = {2020},
	pages = {249--257},
}

@article{athey_state_2017,
	title = {The {State} of {Applied} {Econometrics}: {Causality} and {Policy} {Evaluation}},
	volume = {31},
	issn = {0895-3309},
	shorttitle = {The {State} of {Applied} {Econometrics}},
	url = {https://pubs.aeaweb.org/doi/10.1257/jep.31.2.3},
	doi = {10.1257/jep.31.2.3},
	abstract = {In this paper, we discuss recent developments in econometrics that we view as important for empirical researchers working on policy evaluation questions. We focus on three main areas, in each case, highlighting recommendations for applied work. First, we discuss new research on identification strategies in program evaluation, with particular focus on synthetic control methods, regression discontinuity, external validity, and the causal interpretation of regression methods. Second, we discuss various forms of supplementary analyses, including placebo analyses as well as sensitivity and robustness analyses, intended to make the identification strategies more credible. Third, we discuss some implications of recent advances in machine learning methods for causal effects, including methods to adjust for differences between treated and control units in high-dimensional settings, and methods for identifying and estimating heterogenous treatment effects.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Journal of Economic Perspectives},
	author = {Athey, Susan and Imbens, Guido W.},
	month = may,
	year = {2017},
	pages = {3--32},
}

@article{athey_matrix_2021,
	title = {Matrix {Completion} {Methods} for {Causal} {Panel} {Data} {Models}},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1891924},
	doi = {10.1080/01621459.2021.1891924},
	language = {en},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Athey, Susan and Bayati, Mohsen and Doudchenko, Nikolay and Imbens, Guido and Khosravi, Khashayar},
	month = may,
	year = {2021},
	pages = {1--15},
}

@article{armitage_fisher_2003,
	title = {Fisher, {Bradford} {Hill}, and randomization},
	volume = {32},
	issn = {1464-3685, 0300-5771},
	url = {https://academic.oup.com/ije/article-lookup/doi/10.1093/ije/dyg286},
	doi = {10.1093/ije/dyg286},
	language = {en},
	number = {6},
	urldate = {2021-10-17},
	journal = {International Journal of Epidemiology},
	author = {Armitage, Peter},
	month = dec,
	year = {2003},
	pages = {925--928},
}

@article{an_landscape_2018,
	title = {The {Landscape} of {Causal} {Inference}: {Perspective} {From} {Citation} {Network} {Analysis}},
	volume = {72},
	issn = {0003-1305, 1537-2731},
	shorttitle = {The {Landscape} of {Causal} {Inference}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1360794},
	doi = {10.1080/00031305.2017.1360794},
	abstract = {Causal inference is a fast-growing multidisciplinary field that has drawn extensive interests from statistical sciences and health and social sciences. In this article, we gather comprehensive information on publications and citations in causal inference and provide a review of the field from the perspective of citation network analysis. We provide descriptive analyses by showing the most cited publications, the most prolific and the most cited authors, and structural properties of the citation network. Then, we examine the citation network through exponential random graph models (ERGMs). We show that both technical aspects of the publications (e.g., publication length, time and quality) and social processes such as homophily (the tendency to cite publications in the same field or with shared authors), cumulative advantage, and transitivity (the tendency to cite references’ references), matter for citations. We also provide specific analysis of citations among the top authors in the field and present a ranking and clustering of the authors. Overall, our article reveals new insights into the landscape of the field of causal inference and may serve as a case study for analyzing citation networks in a multidisciplinary field and for fitting ERGMs on big networks. Supplementary materials for this article are available online.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {The American Statistician},
	author = {An, Weihua and Ding, Ying},
	month = jul,
	year = {2018},
	pages = {265--277},
}

@article{salimi_causal_2020,
	title = {Causal {Relational} {Learning}},
	url = {http://arxiv.org/abs/2004.03644},
	abstract = {Causal inference is at the heart of empirical research in natural and social sciences and is critical for scientiﬁc discovery and informed decision making. The gold standard in causal inference is performing randomized controlled trials; unfortunately these are not always feasible due to ethical, legal, or cost constraints. As an alternative, methodologies for causal inference from observational data have been developed in statistical studies and social sciences. However, existing methods critically rely on restrictive assumptions such as the study population consisting of homogeneous elements that can be represented in a single ﬂat table, where each row is referred to as a unit. In contrast, in many real-world settings, the study domain naturally consists of heterogeneous elements with complex relational structure, where the data is naturally represented in multiple related tables. In this paper, we present a formal framework for causal inference from such relational data. We propose a declarative language called CaRL for capturing causal background knowledge and assumptions and specifying causal queries using simple Datalog-like rules. CaRL provides a foundation for inferring causality and reasoning about the effect of complex interventions in relational domains. We present an extensive experimental evaluation on real relational data to illustrate the applicability of CaRL in social sciences and healthcare.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:2004.03644 [cs]},
	author = {Salimi, Babak and Parikh, Harsh and Kayali, Moe and Roy, Sudeepa and Getoor, Lise and Suciu, Dan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.03644},
}

@incollection{rueschemeyer_can_2003,
	series = {Cambridge {Studies} in {Comparative} {Politics}},
	title = {Can {One} or a {Few} {Cases} {Yield} {Theoretical} {Gains}?},
	booktitle = {Comparative {Historical} {Analysis} in the {Social} {Sciences}},
	publisher = {Cambridge University Press},
	author = {Rueschemeyer, Dietrich},
	editor = {Mahoney, James and Rueschemeyer, DietrichEditors},
	year = {2003},
	doi = {10.1017/CBO9780511803963.010},
	pages = {305--336},
}

@book{roberts_logic_1996,
	address = {University Park, Pa},
	title = {The logic of historical explanation},
	isbn = {978-0-271-01442-5 978-0-271-01443-2},
	publisher = {Pennsylvania State University Press},
	author = {Roberts, Clayton},
	year = {1996},
}

@incollection{pierson_big_2003,
	series = {Cambridge {Studies} in {Comparative} {Politics}},
	title = {Big, {Slow}-{Moving}, and…{Invisible}: {Macrosocial} {Processes} in the {Study} of {Comparative} {Politics}},
	booktitle = {Comparative {Historical} {Analysis} in the {Social} {Sciences}},
	publisher = {Cambridge University Press},
	author = {Pierson, Paul},
	editor = {Mahoney, James and Rueschemeyer, DietrichEditors},
	year = {2003},
	doi = {10.1017/CBO9780511803963.006},
	pages = {177--207},
}

@book{mahoney_comparative_2003,
	address = {Cambridge},
	series = {Cambridge {Studies} in {Comparative} {Politics}},
	title = {Comparative {Historical} {Analysis} in the {Social} {Sciences}},
	isbn = {978-0-521-81610-6},
	url = {https://www.cambridge.org/core/books/comparative-historical-analysis-in-the-social-sciences/0654B5F390778FCD909E4A06318BFAFF},
	abstract = {This book systematically investigates the past accomplishments and future agendas of contemporary comparative-historical analysis. Its core essays explore three major issues: the accumulation of knowledge in the field over the past three decades, the analytic tools used to study temporal process and historical patterns, and the methodologies available for making inferences and for building theories. The introductory and concluding essays situate the field as a whole by comparing it to alternative approaches within the social sciences. Comparative Historical Analysis in the Social Sciences will serve as an invaluable resource for scholars in the field, and it will represent a challenge to many other social scientists - especially those who have raised skeptical concerns about comparative-historical analysis in the past.},
	urldate = {2021-10-17},
	publisher = {Cambridge University Press},
	editor = {Mahoney, James and Rueschemeyer, Dietrich},
	year = {2003},
	doi = {10.1017/CBO9780511803963},
}

@article{mayntz_mechanisms_2004,
	title = {Mechanisms in the {Analysis} of {Social} {Macro}-{Phenomena}},
	volume = {34},
	issn = {0048-3931, 1552-7441},
	url = {http://journals.sagepub.com/doi/10.1177/0048393103262552},
	doi = {10.1177/0048393103262552},
	abstract = {The term “(social) mechanism” is frequently encountered in the social science literature, but there is considerable confusion about the exact meaning of the term. The article begins by addressing the main conceptual issues. Use of this term is the hallmark of an approach that is critical of the explanatory deficits of correlational analysis and of the covering-law model, advocating instead the causal reconstruction of the processes that account for given macro-phenomena. The term “social mechanisms” should be used to refer to recurrent processes generating a specific kind of outcome. Explanation of social macro-phenomena by mechanisms typically involves causal regression to lower-level elements, as stipulated by methodological individualism. While there exist a good many mechanism models to explain emergent effects of collective behavior, we lack a similarly systematic treatment of generative mechanisms in which institutions and specific kinds of structural configurations play the decisive role.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Philosophy of the Social Sciences},
	author = {Mayntz, Renate},
	month = jun,
	year = {2004},
	pages = {237--259},
}

@article{grzymala-busse_time_2011,
	title = {Time {Will} {Tell}? {Temporality} and the {Analysis} of {Causal} {Mechanisms} and {Processes}},
	volume = {44},
	issn = {0010-4140, 1552-3829},
	shorttitle = {Time {Will} {Tell}?},
	url = {http://journals.sagepub.com/doi/10.1177/0010414010390653},
	doi = {10.1177/0010414010390653},
	abstract = {Causal inference and the logic of historical explanation are grounded in temporality. Yet the relationship between causal analysis and aspects of temporality, such as duration, tempo, acceleration, and timing, is often less clear. Using examples from analyses of institutional change and postcommunist regime transitions, the author argues that aspects of temporality allow us to predict which causal mechanisms can unfold and to differentiate causal sequences. Explicitly specifying the role of temporality can thus improve scholars’ understanding of political mechanisms, sequences, and the processes they constitute.},
	language = {en},
	number = {9},
	urldate = {2021-10-17},
	journal = {Comparative Political Studies},
	author = {Grzymala-Busse, Anna},
	month = sep,
	year = {2011},
	pages = {1267--1297},
}

@incollection{mckay_illari_getting_2011,
	title = {Getting past {Hume} in the philosophy of social science},
	isbn = {978-0-19-957413-1},
	url = {https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199574131.001.0001/acprof-9780199574131-chapter-14},
	urldate = {2021-10-17},
	booktitle = {Causality in the {Sciences}},
	publisher = {Oxford University Press},
	author = {Groff, Ruth},
	editor = {McKay Illari, Phyllis and Russo, Federica and Williamson, Jon},
	month = mar,
	year = {2011},
	doi = {10.1093/acprof:oso/9780199574131.003.0014},
	doi = {10.1093/acprof:oso/9780199574131.003.0014},
	pages = {296--316},
}

@article{glennan_rethinking_2002,
	title = {Rethinking {Mechanistic} {Explanation}},
	volume = {69},
	issn = {0031-8248, 1539-767X},
	url = {https://www.journals.uchicago.edu/doi/10.1086/341857},
	doi = {10.1086/341857},
	language = {en},
	number = {S3},
	urldate = {2021-10-17},
	journal = {Philosophy of Science},
	author = {Glennan, Stuart},
	month = sep,
	year = {2002},
	pages = {S342--S353},
}

@article{gross_pragmatist_2009,
	title = {A {Pragmatist} {Theory} of {Social} {Mechanisms}},
	volume = {74},
	issn = {0003-1224, 1939-8271},
	url = {http://journals.sagepub.com/doi/10.1177/000312240907400302},
	doi = {10.1177/000312240907400302},
	abstract = {Some sociologists have recently argued that a major aim of sociological inquiry is to identify the mechanisms by which cause and effect relationships in the social world come about. This article argues that existing accounts of social mechanisms are problematic because they rest on either inadequately developed or questionable understandings of social action. Building on an insight increasingly common among sociological theorists—that action should be conceptualized in terms of social practices—I mobilize ideas from the tradition of classical American pragmatism to develop a more adequate theory of mechanisms. I identify three kinds of analytical problems the theory is especially well poised to address and then lay out an agenda for future research.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {American Sociological Review},
	author = {Gross, Neil},
	month = jun,
	year = {2009},
	pages = {358--379},
}

@book{brady_rethinking_2010,
	address = {Lanham, Md},
	edition = {2nd ed},
	title = {Rethinking social inquiry: diverse tools, shared standards},
	isbn = {978-1-4422-0343-3 978-1-4422-0344-0},
	shorttitle = {Rethinking social inquiry},
	url = {https://rowman.com/ISBN/9781442203440/Rethinking-Social-Inquiry-Diverse-Tools-Shared-Standards-Second-Edition},
	publisher = {Rowman \& Littlefield Publishers},
	editor = {Brady, Henry E. and Collier, David},
	year = {2010},
	note = {OCLC: ocn633147797},
}

@book{hedstrom_social_1998,
	edition = {1},
	title = {Social {Mechanisms}: {An} {Analytical} {Approach} to {Social} {Theory}},
	isbn = {978-0-521-59687-9 978-0-521-59319-9 978-0-511-66390-1},
	shorttitle = {Social {Mechanisms}},
	url = {https://www.cambridge.org/core/product/identifier/9780511663901/type/book},
	urldate = {2021-10-17},
	publisher = {Cambridge University Press},
	editor = {Hedström, Peter and Swedberg, Richard},
	month = jan,
	year = {1998},
	doi = {10.1017/CBO9780511663901},
}

@book{illari_causality_2014,
	address = {Oxford, United Kingdom},
	edition = {First edition},
	title = {Causality: philosophical theory meets scientific practice},
	isbn = {978-0-19-966267-8},
	shorttitle = {Causality},
	publisher = {Oxford University Press},
	author = {Illari, Phyllis McKay and Russo, Federica},
	year = {2014},
	note = {OCLC: ocn891671186},
}

@book{gerring_case_2017,
	address = {Cambridge, United Kingdom ; New York, NY},
	edition = {Second edition},
	series = {Strategies for social inquiry},
	title = {Case study research: principles and practices},
	isbn = {978-1-107-18126-7},
	shorttitle = {Case study research},
	publisher = {Cambridge University Press},
	author = {Gerring, John},
	year = {2017},
}

@article{gerring_single-outcome_2006,
	title = {Single-{Outcome} {Studies}: {A} {Methodological} {Primer}},
	volume = {21},
	issn = {0268-5809, 1461-7242},
	shorttitle = {Single-{Outcome} {Studies}},
	url = {http://journals.sagepub.com/doi/10.1177/0268580906067837},
	doi = {10.1177/0268580906067837},
	abstract = {Most methodological work on case studies understands this topic as a study of a case where the objective is to discover something about a broader population of cases. Yet, many case studies (so-called) do not assume this nomothetic goal; their aim is to investigate a bounded unit in an attempt to elucidate a single outcome occurring within that unit. This is referred to as a single-outcome study to distinguish it from the usual genre of case study. In this article, the author discusses the utility of single-outcome studies and the different types of argumentation and causal logic that they embrace. The author proceeds to discuss the methodological components of the single-outcome study, which is understood according to three analytic angles: nested analysis (large- N cross-case analysis), most-similar analysis (small- N cross-case analysis) and within-case analysis (evidence drawn from the case of special interest). The article concludes with a discussion of a common difficulty encountered by single-outcome analysis, that is, reconciling cross-case and within-case evidence, both of which purport to explain the single outcome of interest.},
	language = {en},
	number = {5},
	urldate = {2021-10-17},
	journal = {International Sociology},
	author = {Gerring, John},
	month = sep,
	year = {2006},
	pages = {707--734},
}

@article{fairfield_explicit_2017,
	title = {Explicit {Bayesian} {Analysis} for {Process} {Tracing}: {Guidelines}, {Opportunities}, and {Caveats}},
	volume = {25},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Explicit {Bayesian} {Analysis} for {Process} {Tracing}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/explicit-bayesian-analysis-for-process-tracing-guidelines-opportunities-and-caveats/F93D3EA784730ED731AC910EEF306231},
	doi = {10.1017/pan.2017.14},
	abstract = {Bayesian probability holds the potential to serve as an important bridge between qualitative and quantitative methodology. Yet whereas Bayesian statistical techniques have been successfully elaborated for quantitative research, applying Bayesian probability to qualitative research remains an open frontier. This paper advances the burgeoning literature on Bayesian process tracing by drawing on expositions of Bayesian “probability as extended logic” from the physical sciences, where probabilities represent rational degrees of belief in propositions given the inevitably limited information we possess. We provide step-by-step guidelines for explicit Bayesian process tracing, calling attention to technical points that have been overlooked or inadequately addressed, and we illustrate how to apply this approach with the first systematic application to a case study that draws on multiple pieces of detailed evidence. While we caution that efforts to explicitly apply Bayesian learning in qualitative social science will inevitably run up against the difficulty that probabilities cannot be unambiguously specified, we nevertheless envision important roles for explicit Bayesian analysis in pinpointing the locus of contention when scholars disagree on inferences, and in training intuition to follow Bayesian probability more systematically.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {Political Analysis},
	author = {Fairfield, Tasha and Charman, Andrew E.},
	month = jul,
	year = {2017},
	note = {Publisher: Cambridge University Press},
	pages = {363--380},
}

@book{cartwright_hunting_2007,
	address = {Cambridge ; New York},
	title = {Hunting causes and using them: approaches in philosophy and economics},
	isbn = {978-0-521-86081-9 978-0-521-67798-1},
	shorttitle = {Hunting causes and using them},
	publisher = {Cambridge University Press},
	author = {Cartwright, Nancy},
	year = {2007},
	note = {OCLC: ocm71347550},
}

@article{bunge_how_2004,
	title = {How {Does} {It} {Work}?: {The} {Search} for {Explanatory} {Mechanisms}},
	volume = {34},
	issn = {0048-3931},
	shorttitle = {How {Does} {It} {Work}?},
	url = {https://doi.org/10.1177/0048393103262550},
	doi = {10.1177/0048393103262550},
	abstract = {This article addresses the following problems: What is a mechanism, how can it be discovered, and what is the role of the knowledge of mechanisms in scientific explanation and technological control? The proposed answers are these. A mechanism is one of the processes in a concrete system that makes it what it is — for example, metabolism in cells, interneuronal connections in brains, work in factories and offices, research in laboratories, and litigation in courts of law. Because mechanisms are largely or totally imperceptible, they must be conjectured. Once hypothesized they help explain, because a deep scientific explanation is an answer to a question of the form, “How does it work, that is, what makes it tick—what are its mechanisms?” Thus, by contrast with the subsumption of particulars under a generalization, an explanation proper consists in unveiling some lawful mechanism, as when political stability is explained by either coercion, public opinion manipulation, or democratic participation. Finding mechanisms satisfies not only the yearning for understanding, but also the need for control.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Philosophy of the Social Sciences},
	author = {Bunge, Mario},
	month = jun,
	year = {2004},
	note = {Publisher: SAGE Publications Inc},
	pages = {182--210},
}

@article{beach_multi-method_2020,
	title = {Multi-{Method} {Research} in the {Social} {Sciences}: {A} {Review} of {Recent} {Frameworks} and a {Way} {Forward}},
	volume = {55},
	issn = {0017-257X, 1477-7053},
	shorttitle = {Multi-{Method} {Research} in the {Social} {Sciences}},
	url = {https://www.cambridge.org/core/journals/government-and-opposition/article/multimethod-research-in-the-social-sciences-a-review-of-recent-frameworks-and-a-way-forward/3BCF2566AAAE679BA5C6F7634F8281D7},
	doi = {10.1017/gov.2018.53},
	abstract = {This article reviews recent attempts to develop multi-method social scientific frameworks. The article starts by discussing the ontological and epistemological foundations underlying case studies and variance-based approaches, differentiating approaches into bottom-up, case-based and top-down, variance-based approaches. Case-based approaches aim to learn how a causal process works within a case, whereas variance-based approaches assess mean causal effects across a set of cases. However, because of the different fundamental assumptions, it is very difficult for in-depth studies of individual cases to communicate meaningfully with claims about mean causal effects across a large set of cases. The conclusions discuss the broader challenges this distinction has for the study of comparative politics more broadly.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Government and Opposition},
	author = {Beach, Derek},
	month = jan,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	pages = {163--182},
}

@article{beach_integrating_2018,
	title = {Integrating {Cross}-case {Analyses} and {Process} {Tracing} in {Set}-{Theoretic} {Research}: {Strategies} and {Parameters} of {Debate}},
	volume = {47},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Integrating {Cross}-case {Analyses} and {Process} {Tracing} in {Set}-{Theoretic} {Research}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124115613780},
	doi = {10.1177/0049124115613780},
	abstract = {In recent years, there has been increasing interest in the combination of two methods on the basis of set theory. In our introduction and this special issue, we focus on two variants of cross-case set-theoretic methods— qualitative comparative analysis (QCA) and typological theory (TT)—and their combination with process tracing (PT). Our goal is to broaden and deepen set-theoretic empirical research and equip scholars with guidance on how to implement it in multimethod research (MMR). At first glance, set-theoretic cross-case methods and PT seem to be highly compatible when causal relationships are conceptualized in terms of set theory. However, multiple issues have not so far been thoroughly addressed. Our article builds on the emerging MMR literature and seeks to enhance it in four ways. First, we offer a comprehensive and coherent elaboration of the two sequences in which case studies can be combined with a cross-case method. Second, we expand the perspective and discuss QCA and TT as two alternative methods for the cross-case analysis. Third, based on the idea of analytical priority, we introduce the distinction between a condition-centered and a mechanism-centered variant of set-theoretic MMR. Fourth, we point attention to the challenges of theorizing and analyzing arrangements of conditions and mechanisms associated with sufficient conjunctions.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Sociological Methods \& Research},
	author = {Beach, Derek and Rohlfing, Ingo},
	month = jan,
	year = {2018},
	pages = {3--36},
}

@article{illari_defence_2013,
	title = {In {Defence} of {Activities}},
	volume = {44},
	issn = {1572-8587},
	url = {https://doi.org/10.1007/s10838-013-9217-5},
	doi = {10.1007/s10838-013-9217-5},
	abstract = {In this paper, we examine what is to be said in defence of Machamer, Darden and Craver’s (MDC) controversial dualism about activities and entities (Machamer, Darden and Craver’s in Philos Sci 67:1–25, 2000). We explain why we believe the notion of an activity to be a novel, valuable one, and set about clearing away some initial objections that can lead to its being brushed aside unexamined. We argue that substantive debate about ontology can only be effective when desiderata for an ontology are explicitly articulated. We distinguish three such desiderata. The first is a more permissive descriptive ontology of science, the second a more reductive ontology prioritising understanding, and the third a more reductive ontology prioritising minimalism. We compare MDC’s entities-activities ontology to its closest rival, the entities-capacities ontology, and argue that the entities-activities ontology does better on all three desiderata.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Journal for General Philosophy of Science},
	author = {Illari, Phyllis and Williamson, Jon},
	month = jul,
	year = {2013},
	pages = {69--83},
}

@article{machamer_activities_2004,
	title = {Activities and {Causation}: {The} {Metaphysics} and {Epistemology} of {Mechanisms}},
	volume = {18},
	issn = {0269-8595, 1469-9281},
	shorttitle = {Activities and {Causation}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02698590412331289242},
	doi = {10.1080/02698590412331289242},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {International Studies in the Philosophy of Science},
	author = {Machamer, Peter},
	month = mar,
	year = {2004},
	pages = {27--39},
}

@article{hedstrom_causal_2010,
	title = {Causal {Mechanisms} in the {Social} {Sciences}},
	volume = {36},
	issn = {0360-0572, 1545-2115},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.soc.012809.102632},
	doi = {10.1146/annurev.soc.012809.102632},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Annual Review of Sociology},
	author = {Hedström, Peter and Ylikoski, Petri},
	month = jun,
	year = {2010},
	pages = {49--67},
}

@article{russo_generic_2011,
	title = {Generic versus single-case causality: the case of autopsy},
	volume = {1},
	issn = {1879-4912, 1879-4920},
	shorttitle = {Generic versus single-case causality},
	url = {http://link.springer.com/10.1007/s13194-010-0012-4},
	doi = {10.1007/s13194-010-0012-4},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {European Journal for Philosophy of Science},
	author = {Russo, Federica and Williamson, Jon},
	month = jan,
	year = {2011},
	pages = {47--69},
}

@article{mahoney_toward_2008,
	title = {Toward a {Unified} {Theory} of {Causality}},
	volume = {41},
	issn = {0010-4140, 1552-3829},
	url = {http://journals.sagepub.com/doi/10.1177/0010414007313115},
	doi = {10.1177/0010414007313115},
	abstract = {In comparative research, analysts conceptualize causation in contrasting ways when they pursue explanation in particular cases (case-oriented research) versus large populations (population-oriented research). With case-oriented research, they understand causation in terms of necessary, sufficient, INUS, and SUIN causes. With population-oriented research, by contrast, they understand causation as mean causal effects. This article explores whether it is possible to translate the kind of causal language that is used in case-oriented research into the kind of causal language that is used in population-oriented research (and vice versa). The article suggests that such translation is possible, because certain types of INUS causes manifest themselves as variables that exhibit partial effects when studied in population-oriented research. The article concludes that the conception of causation adopted in case-oriented research is appropriate for the population level, whereas the conception of causation used in population-oriented research is valuable for making predictions in the face of uncertainty.},
	language = {en},
	number = {4-5},
	urldate = {2021-10-17},
	journal = {Comparative Political Studies},
	author = {Mahoney, James},
	month = apr,
	year = {2008},
	pages = {412--436},
}

@book{beach_process-tracing_2019,
	address = {Ann Arbor, MI},
	title = {Process-{Tracing} {Methods}: {Foundations} and {Guidelines}},
	isbn = {978-0-472-13123-5 978-0-472-12478-7},
	shorttitle = {Process-{Tracing} {Methods}},
	url = {http://www.press.umich.edu/10072208},
	language = {en},
	urldate = {2021-10-17},
	publisher = {University of Michigan Press},
	author = {Beach, Derek and Pedersen, Rasmus},
	year = {2019},
	doi = {10.3998/mpub.10072208},
}

@misc{beach_advanced_2019,
	title = {Advanced {Process}-tracing {Workshop}},
	abstract = {The promise of process tracing as a methodological tool is that it enables the researcher to study more-or-less directly the causal mechanism(s) linking a cause (or set of causes) and an
outcome, allowing us to open up the ‘black box’ of causality itself. By unpacking causal mechanisms into their constituent parts, composed of entities engaging in activities, and then
tracing the empirical manifestations these activities leave in actual cases, we are able to collect what has been termed mechanistic evidence upon which we can make causal inferences
about how causal mechanisms actually work (Craver and Darden, 2013; Machamer, Darden and Craver, 2000; Machamer, 2004). Strong causal inferences about the effect a cause has on an
outcome are naturally only possible when we use evidence of difference-making that is produced through experimental manipulation across cases (Woodward, 2003). However, when we
use mechanistic evidence to make causal inferences, we are using observational, within-case evidence to make causal inferences about the actual operation of mechanisms in real world
cases (Russo and Williamson, 2007; Illari, 2011; Waskan, 2011). In other words, instead of studying causal effects we are studying how things work.},
	author = {Beach, Derek},
	month = feb,
	year = {2019},
}

@article{vansteelandt_discussion_2021,
	title = {Discussion of {Kallus} and {Mo}, {Qi}, and {Liu}: {New} {Objectives} for {Policy} {Learning}},
	volume = {116},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Discussion of {Kallus} and {Mo}, {Qi}, and {Liu}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1844718},
	doi = {10.1080/01621459.2020.1844718},
	language = {en},
	number = {534},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Vansteelandt, Stijn and Dukes, Oliver},
	month = apr,
	year = {2021},
	pages = {675--679},
}

@article{kallus_rejoinder_2021,
	title = {Rejoinder: {New} {Objectives} for {Policy} {Learning}},
	volume = {116},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Rejoinder},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1866580},
	doi = {10.1080/01621459.2020.1866580},
	language = {en},
	number = {534},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Kallus, Nathan},
	month = apr,
	year = {2021},
	pages = {694--698},
}

@article{kallus_more_2021,
	title = {More {Efficient} {Policy} {Learning} via {Optimal} {Retargeting}},
	volume = {116},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1788948},
	doi = {10.1080/01621459.2020.1788948},
	abstract = {Policy learning can be used to extract individualized treatment regimes from observational data in healthcare, civics, e-commerce, and beyond. One big hurdle to policy learning is a commonplace lack of overlap in the data for different actions, which can lead to unwieldy policy evaluation and poorly performing learned policies. We study a solution to this problem based on retargeting, that is, changing the population on which policies are optimized. We first argue that at the population level, retargeting may induce little to no bias. We then characterize the optimal reference policy and retargeting weights in both binaryaction and multi-action settings. We do this in terms of the asymptotic efficient estimation variance of the new learning objective. We further consider weights that additionally control for potential bias due to retargeting. Extensive empirical results in a simulation study and a case study of personalized job counseling demonstrate that retargeting is a fairly easy way to significantly improve any policy learning procedure applied to observational data. Supplementary materials for this article are available online.},
	language = {en},
	number = {534},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Kallus, Nathan},
	month = apr,
	year = {2021},
	pages = {646--658},
}

@article{mo_learning_2021,
	title = {Learning {Optimal} {Distributionally} {Robust} {Individualized} {Treatment} {Rules}},
	volume = {116},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1796359},
	doi = {10.1080/01621459.2020.1796359},
	abstract = {Recent development in the data-driven decision science has seen great advances in individualized decision making. Given data with individual covariates, treatment assignments and outcomes, policy makers best individualized treatment rule (ITR) that maximizes the expected outcome, known as the value function. Many existing methods assume that the training and testing distributions are the same. However, the estimated optimal ITR may have poor generalizability when the training and testing distributions are not identical. In this article, we consider the problem of finding an optimal ITR from a restricted ITR class where there are some unknown covariate changes between the training and testing distributions. We propose a novel distributionally robust ITR (DR-ITR) framework that maximizes the worst-case value function across the values under a set of underlying distributions that are “close” to the training distribution. The resulting DR-ITR can guarantee the performance among all such distributions reasonably well. We further propose a calibrating procedure that tunes the DR-ITR adaptively to a small amount of calibration data from a target population. In this way, the calibrated DR-ITR can be shown to enjoy better generalizability than the standard ITR based on our numerical studies. Supplementary materials for this article are available online.},
	language = {en},
	number = {534},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Mo, Weibin and Qi, Zhengling and Liu, Yufeng},
	month = apr,
	year = {2021},
	pages = {659--674},
}

@article{kremer_experimentation_2020,
	title = {Experimentation, {Innovation}, and {Economics}},
	volume = {110},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.110.7.1974},
	doi = {10.1257/aer.110.7.1974},
	abstract = {The experimental method not only helps identify causal relationships, but also provides economists with a rich sense of context, focuses research on specific practical questions, stimulates collaboration with practitioners and specialists from other fields, and allows for rapid iteration. In this lecture, I present a series of examples illustrating how together these features make the experimental approach a powerful tool for advancing scientific understanding, informing policy, and promoting innovation. I then discuss how institutions can be designed to accelerate innovation and direct it toward the world’s most pressing needs. (JEL B31, C90, I10, O15, O30, O43)},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {American Economic Review},
	author = {Kremer, Michael},
	month = jul,
	year = {2020},
	pages = {1974--1994},
}

@article{duflo_field_2020,
	title = {Field {Experiments} and the {Practice} of {Policy}},
	volume = {110},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.110.7.1952},
	doi = {10.1257/aer.110.7.1952},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {American Economic Review},
	author = {Duflo, Esther},
	month = jul,
	year = {2020},
	pages = {1952--1973},
}

@article{banerjee_field_2020,
	title = {Field {Experiments} and the {Practice} of {Economics}},
	volume = {110},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.110.7.1937},
	doi = {10.1257/aer.110.7.1937},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {American Economic Review},
	author = {Banerjee, Abhijit Vinayak},
	month = jul,
	year = {2020},
	pages = {1937--1951},
}

@article{shadish_can_2008,
	title = {Can {Nonrandomized} {Experiments} {Yield} {Accurate} {Answers}? {A} {Randomized} {Experiment} {Comparing} {Random} and {Nonrandom} {Assignments} - with discussion},
	volume = {103},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Can {Nonrandomized} {Experiments} {Yield} {Accurate} {Answers}?},
	url = {https://www.tandfonline.com/doi/full/10.1198/016214508000000733},
	doi = {10.1198/016214508000000733},
	language = {en},
	number = {484},
	urldate = {2021-10-17},
	journal = {Journal of the American Statistical Association},
	author = {Shadish, William R. and Clark, M. H. and Steiner, Peter M.},
	month = dec,
	year = {2008},
	keywords = {starred},
	pages = {1334--1344},
}

@book{lepkowski_advances_2007,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Advances in {Telephone} {Survey} {Methodology}},
	isbn = {978-0-470-17340-4 978-0-471-74531-0},
	url = {http://doi.wiley.com/10.1002/9780470173404},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Lepkowski, James M. and Tucker, Clyde and Brick, J. Michael and Leeuw, Edith D. de and Japec, Lilli and Lavrakas, Paul J. and Link, Michael W. and Sangster, Roberta L.},
	month = nov,
	year = {2007},
	doi = {10.1002/9780470173404},
}

@book{conrad_envisioning_2007,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Envisioning the {Survey} {Interview} of the {Future}},
	isbn = {978-0-470-18337-3 978-0-471-78627-6},
	url = {http://doi.wiley.com/10.1002/9780470183373},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Conrad, Frederick G. and Schober, Michael F.},
	month = nov,
	year = {2007},
	doi = {10.1002/9780470183373},
}

@book{chambers_analysis_2003,
	edition = {1},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Analysis of {Survey} {Data}},
	isbn = {978-0-471-89987-7 978-0-470-86720-4},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0470867205},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Wiley},
	editor = {Chambers, R. L. and Skinner, C. J.},
	month = mar,
	year = {2003},
	doi = {10.1002/0470867205},
}

@book{stoop_improving_2010,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Improving {Survey} {Response}: {Lessons} learned from the {European} {Social} {Survey}},
	isbn = {978-0-470-68833-5 978-0-470-51669-0},
	shorttitle = {Improving {Survey} {Response}},
	url = {http://doi.wiley.com/10.1002/9780470688335},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Stoop, Ineke and Billiet, Jaak and Koch, Achim and Fitzgerald, Rory},
	month = mar,
	year = {2010},
	doi = {10.1002/9780470688335},
}

@book{biemer_latent_2010,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Latent {Class} {Analysis} of {Survey} {Error}},
	isbn = {978-0-470-89115-5 978-0-470-28907-5},
	shorttitle = {Latent {Class} {Analysis} of {Survey} {Error}},
	url = {http://doi.wiley.com/10.1002/9780470891155},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Biemer, Paul P.},
	month = dec,
	year = {2010},
	doi = {10.1002/9780470891155},
}

@book{wolf_sage_2016,
	address = {1 Oliver's Yard, 55 City Road London EC1Y 1SP},
	title = {The {SAGE} {Handbook} of {Survey} {Methodology}},
	isbn = {978-1-4462-8266-3 978-1-4739-5789-3},
	url = {http://sk.sagepub.com/reference/the-sage-handbook-of-survey-methodology},
	urldate = {2021-10-17},
	publisher = {SAGE Publications Ltd},
	author = {Wolf, Christof and Joye, Dominique and Smith, Tom and Fu, Yang-chih},
	year = {2016},
	doi = {10.4135/9781473957893},
}

@book{de_leeuw_international_2012,
	edition = {0},
	title = {International {Handbook} of {Survey} {Methodology}},
	isbn = {978-1-136-91063-0},
	url = {https://www.taylorfrancis.com/books/9781136910630},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Routledge},
	editor = {de Leeuw, Edith  D. and Hox, Joop and Dillman, Don},
	month = oct,
	year = {2012},
	doi = {10.4324/9780203843123},
}

@book{gideon_handbook_2012,
	address = {New York, NY},
	title = {Handbook of {Survey} {Methodology} for the {Social} {Sciences}},
	isbn = {978-1-4614-3875-5 978-1-4614-3876-2},
	url = {http://link.springer.com/10.1007/978-1-4614-3876-2},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer New York},
	editor = {Gideon, Lior},
	year = {2012},
	doi = {10.1007/978-1-4614-3876-2},
}

@book{groves_survey_2013,
	address = {Hoboken},
	edition = {2nd ed.},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Survey {Methodology}},
	isbn = {978-1-118-21134-2},
	language = {English},
	publisher = {Wiley},
	author = {Groves, Robert M. and Fowler, Floyd J. and Couper, Mick P. and Lepkowski, James M. and Singer, Eleanor and Tourangeau, Roger},
	year = {2013},
	note = {OCLC: 861474163},
}

@book{lumley_complex_2010,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Complex {Surveys}},
	isbn = {978-0-470-58006-6 978-0-470-28430-8},
	url = {http://doi.wiley.com/10.1002/9780470580066},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Lumley, Thomas},
	month = feb,
	year = {2010},
	doi = {10.1002/9780470580066},
}

@book{fuller_sampling_2009,
	edition = {1},
	title = {Sampling {Statistics}},
	isbn = {978-0-470-45460-2 978-0-470-52355-1},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470523551},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Wiley},
	author = {Fuller, Wayne A.},
	month = jul,
	year = {2009},
	doi = {10.1002/9780470523551},
}

@book{madans_question_2011,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Question {Evaluation} {Methods}: {Contributing} to the {Science} of {Data} {Quality}},
	isbn = {978-1-118-03700-3 978-0-470-76948-5},
	shorttitle = {Question {Evaluation} {Methods}},
	url = {http://doi.wiley.com/10.1002/9781118037003},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Madans, Jennifer and Miller, Kristen and Maitland, Aaron and Willis, Gordon},
	month = jul,
	year = {2011},
	doi = {10.1002/9781118037003},
}

@book{groves_nonresponse_1998,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Nonresponse in {Household} {Interview} {Surveys}},
	isbn = {978-1-118-49008-2 978-0-471-18245-0},
	shorttitle = {Nonresponse in {Household} {Interview} {Surveys}},
	url = {http://doi.wiley.com/10.1002/9781118490082},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Groves, Robert M. and Couper, Mick P.},
	month = apr,
	year = {1998},
	doi = {10.1002/9781118490082},
}

@book{hundepool_statistical_2012,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Statistical {Disclosure} {Control}},
	isbn = {978-1-118-34823-9 978-1-119-97815-2},
	shorttitle = {Statistical {Disclosure} {Control}},
	url = {http://doi.wiley.com/10.1002/9781118348239},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Hundepool, Anco and Domingo-Ferrer, Josep and Franconi, Luisa and Giessing, Sarah and Nordholt, Eric Schulte and Spicer, Keith and de Wolf, Peter-Paul},
	month = aug,
	year = {2012},
	doi = {10.1002/9781118348239},
}

@book{snijkers_designing_2013,
	address = {Hoboken, New Jersey},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Designing and {Conducting} {Business} {Surveys}},
	isbn = {978-1-118-44789-5 978-0-470-90304-9},
	url = {http://doi.wiley.com/10.1002/9781118447895},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Snijkers, Ger and Haraldsen, Gustav and Jones, Jacqui and Willimack, Diane K.},
	month = jul,
	year = {2013},
	doi = {10.1002/9781118447895},
}

@book{kreuter_improving_2013,
	address = {Hoboken, New Jersey},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Improving {Surveys} with {Paradata}: {Analytic} {Uses} of {Process} {Information}},
	isbn = {978-1-118-59686-9 978-0-470-90541-8},
	shorttitle = {Improving {Surveys} with {Paradata}},
	url = {http://doi.wiley.com/10.1002/9781118596869},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Kreuter, Frauke},
	month = may,
	year = {2013},
	doi = {10.1002/9781118596869},
}

@book{callegaro_online_2014,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Online {Panel} {Research}: {Data} {Quality} {Perspective}, {A}},
	isbn = {978-1-118-76352-0 978-1-119-94177-4},
	shorttitle = {Online {Panel} {Research}},
	url = {http://doi.wiley.com/10.1002/9781118763520},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {Callegaro, Mario and Baker, Reg and Bethlehem, Jelke and Göritz, Anja S. and Krosnick, Jon A. and Lavrakas, Paul J.},
	month = may,
	year = {2014},
	doi = {10.1002/9781118763520},
}

@book{biemer_total_2017,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Total {Survey} {Error} in {Practice}},
	isbn = {978-1-119-04170-2 978-1-119-04167-2},
	url = {http://doi.wiley.com/10.1002/9781119041702},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Biemer, Paul P. and de Leeuw, Edith and Eckman, Stephanie and Edwards, Brad and Kreuter, Frauke and Lyberg, Lars E. and Tucker, N. Clyde and West, Brady T.},
	month = jan,
	year = {2017},
	doi = {10.1002/9781119041702},
}

@book{lietz_implementation_2017,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Implementation of {Large}-{Scale} {Education} {Assessments}},
	isbn = {978-1-118-76246-2 978-1-118-33609-0},
	url = {http://doi.wiley.com/10.1002/9781118762462},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {Lietz, Petra and Cresswell, John C. and Rust, Keith F. and Adams, Raymond J.},
	month = mar,
	year = {2017},
	doi = {10.1002/9781118762462},
}

@book{johnson_advances_2018,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Advances in {Comparative} {Survey} {Methods}: {Multinational}, {Multiregional}, and {Multicultural} {Contexts} ({3MC})},
	isbn = {978-1-118-88499-7 978-1-118-88498-0},
	shorttitle = {Advances in {Comparative} {Survey} {Methods}},
	url = {http://doi.wiley.com/10.1002/9781118884997},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Johnson, Timothy P. and Pennell, Beth-Ellen and Stoop, Ineke A.L. and Dorer, Brita},
	month = oct,
	year = {2018},
	doi = {10.1002/9781118884997},
}

@book{lavrakas_experimental_2019,
	edition = {1},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Experimental {Methods} in {Survey} {Research}: {Techniques} that {Combine} {Random} {Sampling} with {Random} {Assignment}},
	isbn = {978-1-119-08374-0 978-1-119-08377-1},
	shorttitle = {Experimental {Methods} in {Survey} {Research}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119083771},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Wiley},
	editor = {Lavrakas, Paul and Traugott, Michael and Kennedy, Courtney and Holbrook, Allyson and de Leeuw, Edith and West, Brady},
	month = oct,
	year = {2019},
	doi = {10.1002/9781119083771},
}

@book{sarndal_model_2003,
	address = {New York Berlin Heidelberg},
	edition = {1. softcover print},
	series = {Springer series in statistics},
	title = {Model assisted survey sampling},
	isbn = {978-0-387-40620-6},
	abstract = {Aka},
	language = {eng},
	publisher = {Springer},
	author = {Särndal, Carl-Erik and Swensson, Bengt and Wretman, Jan Håkan},
	year = {2003},
}

@book{tille_sampling_2020,
	edition = {1},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Sampling and {Estimation} from {Finite} {Populations}},
	isbn = {978-0-470-68205-0 978-1-119-07125-9},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119071259},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Wiley},
	author = {Tillé, Yves},
	month = apr,
	year = {2020},
	doi = {10.1002/9781119071259},
}

@book{chun_administrative_2021,
	edition = {1},
	title = {Administrative {Records} for {Survey} {Methodology}},
	isbn = {978-1-119-27204-5 978-1-119-27207-6},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119272076},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Wiley},
	editor = {Chun, Asaph Young and D. Larsen, Michael and Durrant, Gabriele and P. Reiter, Jerome},
	month = apr,
	year = {2021},
	doi = {10.1002/9781119272076},
}

@book{p_johnson_health_2014,
	edition = {1},
	series = {Wiley {Handbooks} in {Survey} {Methodology}},
	title = {Health {Survey} {Methods}},
	isbn = {978-1-118-00232-2 978-1-118-59462-9},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118594629},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Wiley},
	editor = {P. Johnson, Timothy},
	month = oct,
	year = {2014},
	doi = {10.1002/9781118594629},
}

@book{bethlehem_applied_2009,
	address = {Hoboken, NJ, USA},
	title = {Applied {Survey} {Methods}},
	isbn = {978-0-470-49499-8 978-0-470-37308-8},
	url = {http://doi.wiley.com/10.1002/9780470494998},
	language = {en},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Bethlehem, Jelke},
	month = jun,
	year = {2009},
	doi = {10.1002/9780470494998},
}

@book{bethlehem_handbook_2011,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Handbook of {Nonresponse} in {Household} {Surveys}},
	isbn = {978-0-470-89105-6 978-0-470-54279-8},
	shorttitle = {Handbook of {Nonresponse} in {Household} {Surveys}},
	url = {http://doi.wiley.com/10.1002/9780470891056},
	urldate = {2021-10-17},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Bethlehem, Jelke and Cobben, Fannie and Schouten, Barry},
	month = jan,
	year = {2011},
	doi = {10.1002/9780470891056},
}

@misc{00_seminar_2018,
	title = {Seminar {Syllabus}},
	author = {00},
	year = {2018},
}

@article{barocas_data_2014,
	title = {{DATA} {MINING} {AND} {THE} {DISCOURSE} {ON} {DISCRIMINATION}},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=rEjgIskAAAAJ&cstart=20&pagesize=80&citation_for_view=rEjgIskAAAAJ:Tyk-4Ss8FVUC},
	abstract = {This paper surveys and brings some order to the broad set of charges that commentators have begun to levy against data mining, all expressed in the language of discrimination. It maps the myriad kinds of discrimination ascribed to data mining, clarifies the precise mechanisms the commentators see as giving rise to these objectionable forms of discrimination, and specifies the principles or policies that such discrimination seems to contravene.},
	language = {en},
	author = {Barocas, Solon},
	year = {2014},
	pages = {4},
}

@article{valentino-devries_websites_2012,
	title = {Websites {Vary} {Prices}, {Deals} {Based} on {Users}' {Information}},
	url = {https://online.wsj.com/article/SB10001424127887323777204578189391813881534.html?reflink=desktopwebshare_permalink},
	journal = {Wall Street Journal},
	author = {Valentino-DeVries, Jennifer and Singer-Vine, Jeremy and Soltani, Ashkan},
	month = dec,
	year = {2012},
}

@article{tufekci_engineering_2014,
	title = {Engineering the public: {Big} data, surveillance and computational politics},
	issn = {1396-0466},
	shorttitle = {Engineering the public},
	url = {https://journals.uic.edu/ojs/index.php/fm/article/view/4901},
	doi = {10.5210/fm.v19i7.4901},
	abstract = {Digital technologies have given rise to a new combination of big data and computational practices which allow for massive, latent data collection and sophisticated computational modeling, increasing the capacity of those with resources and access to use these tools to carry out highly effective, opaque and unaccountable campaigns of persuasion and social engineering in political, civic and commercial spheres. I examine six intertwined dynamics that pertain to the rise of computational politics: the rise of big data, the shift away from demographics to individualized targeting, the opacity and power of computational modeling, the use of persuasive behavioral science, digital media enabling dynamic real-time experimentation, and the growth of new power brokers who own the data or social media environments. I then examine the consequences of these new mechanisms on the public sphere and political campaigns.},
	urldate = {2021-10-17},
	journal = {First Monday},
	author = {Tufekci, Zeynep},
	month = jul,
	year = {2014},
}

@techreport{dieterich_compas_2016,
	title = {{COMPAS} {Risk} {Scales}: {Demonstrating} {Accuracy} {Equity} and {Predictive} {Parity}},
	url = {https://university.pretrial.org/viewdocument/compas-risk-scales-demonstrating-a},
	abstract = {Performance of the COMPAS Risk Scales
in Broward County},
	author = {Dieterich, William and Mendoza, Christina and Brennan, Tim},
	month = jul,
	year = {2016},
}

@article{friedman_bias_1996,
	title = {Bias in computer systems},
	volume = {14},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/230538.230561},
	doi = {10.1145/230538.230561},
	abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
	number = {3},
	urldate = {2021-10-17},
	journal = {ACM Transactions on Information Systems},
	author = {Friedman, Batya and Nissenbaum, Helen},
	month = jul,
	year = {1996},
	pages = {330--347},
}

@article{agre_surveillance_1994,
	title = {Surveillance and capture: {Two} models of privacy},
	volume = {10},
	issn = {0197-2243, 1087-6537},
	shorttitle = {Surveillance and capture},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01972243.1994.9960162},
	doi = {10.1080/01972243.1994.9960162},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {The Information Society},
	author = {Agre, Philip E.},
	month = apr,
	year = {1994},
	pages = {101--127},
}

@article{zliobaite_using_2016,
	title = {Using sensitive personal data may be necessary for avoiding discrimination in data-driven decision models},
	volume = {24},
	issn = {0924-8463, 1572-8382},
	url = {http://link.springer.com/10.1007/s10506-016-9182-5},
	doi = {10.1007/s10506-016-9182-5},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Artificial Intelligence and Law},
	author = {Žliobaitė, Indrė and Custers, Bart},
	month = jun,
	year = {2016},
	pages = {183--201},
}

@article{zarsky_trouble_2016,
	title = {The {Trouble} with {Algorithmic} {Decisions}: {An} {Analytic} {Road} {Map} to {Examine} {Efficiency} and {Fairness} in {Automated} and {Opaque} {Decision} {Making}},
	volume = {41},
	issn = {0162-2439, 1552-8251},
	shorttitle = {The {Trouble} with {Algorithmic} {Decisions}},
	url = {http://journals.sagepub.com/doi/10.1177/0162243915605575},
	doi = {10.1177/0162243915605575},
	abstract = {We are currently witnessing a sharp rise in the use of algorithmic decision-making tools. In these instances, a new wave of policy concerns is set forth. This article strives to map out these issues, separating the wheat from the chaff. It aims to provide policy makers and scholars with a comprehensive framework for approaching these thorny issues in their various capacities. To achieve this objective, this article focuses its attention on a general analytical framework, which will be applied to a specific subset of the overall discussion. The analytical framework will reduce the discussion to two dimensions, every one of which addressing two central elements. These four factors call for a distinct discussion, which is at times absent in the existing literature. The two dimensions are (1) the specific and novel problems the process assumedly generates and (2) the specific attributes which exacerbate them. While the problems are articulated in a variety of ways, they most likely could be reduced to two broad categories: efficiency and fairness-based concerns. In the context of this discussion, such problems are usually linked to two salient attributes the algorithmic processes feature—its opaque and automated nature.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Science, Technology, \& Human Values},
	author = {Zarsky, Tal},
	month = jan,
	year = {2016},
	pages = {118--132},
}

@article{zarsky_transparent_2013,
	title = {{TRANSPARENT} {PREDICTIONS}},
	volume = {2013},
	language = {en},
	number = {4},
	journal = {UNIVERSITY OF ILLINOIS LAW REVIEW},
	author = {Zarsky, Tal Z},
	year = {2013},
	pages = {68},
}

@article{wu_responses_2017,
	title = {Responses to {Critiques} on {Machine} {Learning} of {Criminality} {Perceptions} ({Addendum} of {arXiv}:1611.04135)},
	shorttitle = {Responses to {Critiques} on {Machine} {Learning} of {Criminality} {Perceptions} ({Addendum} of {arXiv}},
	url = {http://arxiv.org/abs/1611.04135},
	abstract = {We study, for the ﬁrst time, automated inference on criminality based solely on still face images, which is free of any biases of subjective judgments of human observers. Via supervised machine learning, we build four classiﬁers (logistic regression, KNN, SVM, CNN) using facial images of 1856 real persons controlled for race, gender, age and facial expressions, nearly half of whom were convicted criminals, for discriminating between criminals and noncriminals. All four classiﬁers perform consistently well and empirically establish the validity of automated face-induced inference on criminality, despite the historical controversy surrounding this line of enquiry. Also, some discriminating structural features for predicting criminality have been found by machine learning. Above all, the most important discovery of this research is that criminal and non-criminal face images populate two quite distinctive manifolds. The variation among criminal faces is signiﬁcantly greater than that of the non-criminal faces. The two manifolds consisting of criminal and non-criminal faces appear to be concentric, with the non-criminal manifold lying in the kernel with a smaller span, exhibiting a law of ”normality” for faces of non-criminals. In other words, the faces of general law-biding public have a greater degree of resemblance compared with the faces of criminals, or criminals have a higher degree of dissimilarity in facial appearance than non-criminals.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1611.04135 [cs]},
	author = {Wu, Xiaolin and Zhang, Xi},
	month = may,
	year = {2017},
	note = {arXiv: 1611.04135},
}

@article{vedder_kdd_nodate,
	title = {{KDD}: {The} challenge to individualism},
	abstract = {KDD (Knowledge Discovery in Databases) confronts us with phenomena that can intuitively be grasped as highly problematic, but are nevertheless difﬁcult to understand and articulate. Many of these problems have to do with what I call the “deindividualization of the person”: a tendency of judging and treating persons on the basis of group characteristics instead of on their own individual characteristics and merits. This tendency will be one of the consequences of the production and use of group proﬁles with the help of KDD. Current privacy law and regulations, as well as current ethical theory concerning privacy, start from too narrow a deﬁnition of “personal data” to capture these problems. In this paper, I introduce the notion of “categorical privacy” as a starting point for a possible remedy for the failures of the current conceptions of privacy. I discuss some ways in which the problems relating to group proﬁles deﬁnitely cannot be solved and I suggest a possible way out of these problems. Finally, I suggest that it may take us a step forward if we would begin to question the predominance of privacy norms in the social debate on information technologies and if we would be prepared to introduce normative principles other than privacy rules for the assessment of new information technologies. If we do not succeed in articulating the problems relating to KDD clearly, one day we may ﬁnd ourselves in a situation where KDD appears to have undermined the methodic and normative individualism which pervades the mainstream of morality and moral theory.},
	language = {en},
	author = {Vedder, Anton},
	pages = {8},
}

@article{tene_taming_nodate,
	title = {{TAMING} {THE} {GOLEM}: {CHALLENGES} {OF} {ETHICAL} {ALGORITHMIC} {DECISION}-{MAKING}},
	volume = {19},
	language = {en},
	author = {Tene, Omer and Polonetsky, Jules},
	pages = {49},
}

@article{simon_ideological_nodate,
	title = {The {Ideological} {Effects} of {Actuarial} {Practices}},
	language = {en},
	author = {Simon, Jonathan},
	pages = {31},
}

@article{sweeney_discrimination_nodate,
	title = {Discrimination in {Online} {Ad} {Delivery}},
	language = {en},
	author = {Sweeney, Latanya},
	pages = {36},
}

@article{barocas_regulating_nodate,
	title = {Regulating {Inscrutable} {Systems}},
	language = {en},
	author = {Barocas, Solon},
	pages = {39},
}

@article{swedloff_risk_2014,
	title = {{RISK} {CLASSIFICATION}’{S} {BIG} {DATA} ({R}){EVOLUTION}},
	volume = {21},
	language = {en},
	author = {Swedloff, Rick},
	year = {2014},
	pages = {36},
}

@article{sandvig_auditing_nodate,
	title = {Auditing {Algorithms}: {Research} {Methods} for {Detecting} {Discrimination} on {Internet} {Platforms}},
	language = {en},
	author = {Sandvig, Christian and Hamilton, Kevin and Karahalios, Karrie and Langbort, Cedric},
	pages = {23},
}

@article{pager_sociology_2008,
	title = {The {Sociology} of {Discrimination}: {Racial} {Discrimination} in {Employment}, {Housing}, {Credit}, and {Consumer} {Markets}},
	volume = {34},
	issn = {0360-0572, 1545-2115},
	shorttitle = {The {Sociology} of {Discrimination}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.soc.33.040406.131740},
	doi = {10.1146/annurev.soc.33.040406.131740},
	abstract = {Persistent racial inequality in employment, housing, and a wide range of other social domains has renewed interest in the possible role of discrimination. And yet, unlike in the pre–civil rights era, when racial prejudice and discrimination were overt and widespread, today discrimination is less readily identiﬁable, posing problems for social scientiﬁc conceptualization and measurement. This article reviews the relevant literature on discrimination, with an emphasis on racial discrimination in employment, housing, credit markets, and consumer interactions. We begin by deﬁning discrimination and discussing relevant methods of measurement. We then provide an overview of major ﬁndings from studies of discrimination in each of the four domains; and, ﬁnally, we turn to a discussion of the individual, organizational, and structural mechanisms that may underlie contemporary forms of discrimination. This discussion seeks to orient readers to some of the key debates in the study of discrimination and to provide a roadmap for those interested in building upon this long and important line of research.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Annual Review of Sociology},
	author = {Pager, Devah and Shepherd, Hana},
	month = aug,
	year = {2008},
	pages = {181--209},
}

@article{moor_what_1985,
	title = {{WHAT} {IS} {COMPUTER} {ETHICS}?},
	volume = {16},
	issn = {0026-1068, 1467-9973},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9973.1985.tb00173.x},
	doi = {10.1111/j.1467-9973.1985.tb00173.x},
	language = {en},
	number = {4},
	urldate = {2021-10-17},
	journal = {Metaphilosophy},
	author = {Moor, James H.},
	month = oct,
	year = {1985},
	pages = {266--275},
}

@article{lum_predict_2016,
	title = {To predict and serve?},
	volume = {13},
	issn = {17409705},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x},
	doi = {10.1111/j.1740-9713.2016.00960.x},
	language = {en},
	number = {5},
	urldate = {2021-10-17},
	journal = {Significance},
	author = {Lum, Kristian and Isaac, William},
	month = oct,
	year = {2016},
	pages = {14--19},
}

@article{lipton_mythos_2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspeciﬁed. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to reﬁne the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, ﬁnding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1606.03490 [cs, stat]},
	author = {Lipton, Zachary C.},
	month = mar,
	year = {2017},
	note = {arXiv: 1606.03490},
}

@article{lippert-rasmussen_we_2011,
	title = {“{We} are all {Different}”: {Statistical} {Discrimination} and the {Right} to be {Treated} as an {Individual}},
	volume = {15},
	issn = {1382-4554, 1572-8609},
	shorttitle = {“{We} are all {Different}”},
	url = {http://link.springer.com/10.1007/s10892-010-9095-6},
	doi = {10.1007/s10892-010-9095-6},
	abstract = {There are many objections to statistical discrimination in general and racial proﬁling in particular. One objection appeals to the idea that people have a right to be treated as individuals. Statistical discrimination violates this right because, presumably, it involves treating people simply on the basis of statistical facts about groups to which they belong while ignoring non-statistical evidence about them. While there is something to this objection—there are objectionable ways of treating others that seem aptly described as failing to treat them as individuals—it needs to be articulated carefully. First, most people accept that many forms of statistical discrimination are morally unproblematic, let alone morally justiﬁed all things considered. Second, even treating people on the basis of putative non-statistical evidence relies on generalizations. Once we construe treating someone as an individual in a way that respects this fact, it becomes apparent: (1) that statistical discrimination is compatible with treating people as individuals, and (2) that one may fail to treat people as individuals even without engaging in statistical discrimination. Finally, there are situations involving the expression of messages of inclusion where we think it is good, morally speaking, that we are not treated as individuals.},
	language = {en},
	number = {1-2},
	urldate = {2021-10-17},
	journal = {The Journal of Ethics},
	author = {Lippert-Rasmussen, Kasper},
	month = jun,
	year = {2011},
	pages = {47--59},
}

@article{lerman_big_2013,
	title = {Big {Data} and {Its} {Exclusions}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2293765},
	doi = {10.2139/ssrn.2293765},
	language = {en},
	urldate = {2021-10-17},
	journal = {SSRN Electronic Journal},
	author = {Lerman, Jonas},
	year = {2013},
}

@article{kroll_accountable_nodate,
	title = {Accountable {Algorithms}},
	volume = {165},
	language = {en},
	journal = {University of Pennsylvania Law Review},
	author = {Kroll, Joshua A and Huey, Joanna and Barocas, Solon and Felten, Edward W and Reidenberg, Joel R and Robinson, David G and Yu, Harlan},
	pages = {74},
}

@article{kosinski_private_2013,
	title = {Private traits and attributes are predictable from digital records of human behavior},
	volume = {110},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1218772110},
	doi = {10.1073/pnas.1218772110},
	language = {en},
	number = {15},
	urldate = {2021-10-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kosinski, M. and Stillwell, D. and Graepel, T.},
	month = apr,
	year = {2013},
	pages = {5802--5805},
}

@article{kochelek_data_nodate,
	title = {{DATA} {MINING} {AND} {ANTITRUST}},
	volume = {22},
	language = {en},
	number = {2},
	author = {Kochelek, Douglas M},
	pages = {21},
}

@article{kleinberg_inherent_2016,
	title = {Inherent {Trade}-{Offs} in the {Fair} {Determination} of {Risk} {Scores}},
	url = {http://arxiv.org/abs/1609.05807},
	abstract = {Recent discussion in the public sphere about algorithmic classiﬁcation has involved tension between competing notions of what it means for a probabilistic classiﬁcation to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identiﬁed by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1609.05807 [cs, stat]},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	month = nov,
	year = {2016},
	note = {arXiv: 1609.05807},
}

@incollection{hutchison_selecting_2010,
	address = {Berlin, Heidelberg},
	title = {Selecting {Effective} {Means} to {Any} {End}: {Futures} and {Ethics} of {Persuasion} {Profiling}},
	volume = {6137},
	isbn = {978-3-642-13225-4 978-3-642-13226-1},
	shorttitle = {Selecting {Effective} {Means} to {Any} {End}},
	url = {http://link.springer.com/10.1007/978-3-642-13226-1_10},
	abstract = {Interactive persuasive technologies can and do adapt to individuals. Existing systems identify and adapt to user preferences within a speciﬁc domain: e.g., a music recommender system adapts its recommended songs to user preferences. This paper is concerned with adaptive persuasive systems that adapt to individual diﬀerences in the effectiveness of particular means, rather than selecting diﬀerent ends. We give special attention to systems that implement persuasion proﬁling —adapting to individual diﬀerences in the eﬀects of inﬂuence strategies. We argue that these systems are worth separate consideration and raise unique ethical issues for two reasons: (1) their end-independence implies that systems trained in one context can be used in other, unexpected contexts and (2) they do not rely on — and are generally disadvantaged by — disclosing that they are adapting to individual diﬀerences. We use examples of these systems to illustrate some ethically and practically challenging futures that these characteristics make possible.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Persuasive {Technology}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kaptein, Maurits and Eckles, Dean},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ploug, Thomas and Hasle, Per and Oinas-Kukkonen, Harri},
	year = {2010},
	doi = {10.1007/978-3-642-13226-1_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {82--93},
}

@article{joseph_fairness_nodate,
	title = {Fairness in {Learning}: {Classic} and {Contextual} {Bandits}},
	abstract = {We introduce the study of fairness in multi-armed bandit problems. Our fairness deﬁnition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm’s uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on “chained” conﬁdence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.},
	language = {en},
	author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie H and Roth, Aaron},
	pages = {9},
}

@article{jones_right_2017,
	title = {The right to a human in the loop: {Political} constructions of computer automation and personhood},
	volume = {47},
	issn = {0306-3127, 1460-3659},
	shorttitle = {The right to a human in the loop},
	url = {http://journals.sagepub.com/doi/10.1177/0306312717699716},
	doi = {10.1177/0306312717699716},
	abstract = {Contributing to recent scholarship on the governance of algorithms, this article explores the role of dignity in data protection law addressing automated decision-making. Delving into the historical roots of contemporary disputes between information societies, notably European Union and Council of Europe countries and the United States, reveals that the regulation of algorithms has a rich, culturally entrenched, politically relevant backstory. The article compares the making of law concerning data protection and privacy, focusing on the role automation has played in the two regimes. By situating diverse policy treatments within the cultural contexts from which they emerged, the article uncovers and examines two different legal constructions of automated data processing, one that has furnished a right to a human in the loop that is intended to protect the dignity of the data subject and the other that promotes and fosters full automation to establish and celebrate the fairness and objectivity of computers. The existence of a subtle right across European countries and its absence in the US will no doubt continue to be relevant to international technology policy as smart technologies are introduced in more and more areas of society.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Social Studies of Science},
	author = {Jones, Meg Leta},
	month = apr,
	year = {2017},
	pages = {216--239},
}

@article{hurley_credit_2016,
	title = {{CREDIT} {SCORING} {IN} {THE} {ERA} {OF} {BIG} {DATA}},
	volume = {18},
	abstract = {For most Americans, access to credit is an essential requirement for upward mobility and financial success. A favorable credit rating is necessary to purchase a home or car, to start a new business, to seek higher education, or to pursue other important goals. For many consumers, strong credit is also necessary to gain access to employment, rental housing, and essential services such as insurance. At present, however, individuals have very little control over how they are scored and have even less ability to contest inaccurate, biased, or unfair assessments of their credit. Traditional, automated creditscoring tools raise longstanding concerns of accuracy and unfairness. The recent advent of new "big-data"credit-scoring products heightens these concerns. The credit-scoring industry has experienced a recent explosion of start-ups that take an "all data is credit data" approach, combining conventional credit information with thousands of data points mined from consumers' offline and online activities. Big-data scoring tools may now base credit decisions on where people shop, the purchases they make, their online social media networks, and various other factors that are not intuitively related to creditworthiness. While the details of many of these products remain closely guardedtrade secrets, the proponents of big-data credit scoring argue that these tools can reach millions of underserved consumers by using complex algorithms to detect patterns and signals within a vast sea of information. While alternative credit scoring may ultimately benefit some consumers, it also poses significant risks.},
	language = {en},
	journal = {Big Data},
	author = {Hurley, Mikella and Adebayo, Julius},
	year = {2016},
	pages = {70},
}

@article{helveston_consumer_nodate,
	title = {Consumer {Protection} in the {Age} of {Big} {Data}},
	volume = {93},
	language = {en},
	author = {Helveston, Max N},
	pages = {60},
}

@article{hardt_equality_nodate,
	title = {Equality of {Opportunity} in {Supervised} {Learning}},
	abstract = {We propose a criterion for discrimination against a speciﬁed sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our deﬁnition. Our framework also improves incentives by shifting the cost of poor classiﬁcation from disadvantaged groups to the decision maker, who can respond by improving the classiﬁcation accuracy.},
	language = {en},
	author = {Hardt, Moritz and Price, Eric},
	pages = {9},
}

@inproceedings{hannak_measuring_2014,
	address = {Vancouver BC Canada},
	title = {Measuring {Price} {Discrimination} and {Steering} on {E}-commerce {Web} {Sites}},
	isbn = {978-1-4503-3213-2},
	url = {https://dl.acm.org/doi/10.1145/2663716.2663744},
	doi = {10.1145/2663716.2663744},
	abstract = {Today, many e-commerce websites personalize their content, including Netﬂix (movie recommendations), Amazon (product suggestions), and Yelp (business reviews). In many cases, personalization provides advantages for users: for example, when a user searches for an ambiguous query such as “router,” Amazon may be able to suggest the woodworking tool instead of the networking device. However, personalization on e-commerce sites may also be used to the user’s disadvantage by manipulating the products shown (price steering) or by customizing the prices of products (price discrimination). Unfortunately, today, we lack the tools and techniques necessary to be able to detect such behavior.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 2014 {Conference} on {Internet} {Measurement} {Conference}},
	publisher = {ACM},
	author = {Hannak, Aniko and Soeller, Gary and Lazer, David and Mislove, Alan and Wilson, Christo},
	month = nov,
	year = {2014},
	pages = {305--318},
}

@article{hand_deconstructing_1994,
	title = {Deconstructing {Statistical} {Questions}},
	volume = {157},
	issn = {09641998},
	url = {https://www.jstor.org/stable/10.2307/2983526?origin=crossref},
	doi = {10.2307/2983526},
	abstract = {Too much current statistical work takes a superficial view of the client's research question, adopting techniques which have a solid history, a sound mathematical basis or readily available software, but without considering in depth whether the questions being answered are in fact those which should be asked. Examples, some familiar and others less so, are given to illustrate this assertion. It is clear that establishing the mapping from the client's domain to a statistical question is one of the most difficult parts of a statistical analysis. It is a part in which the responsibility is shared by both client and statistician. A plea is made for more research effort to go in this direction and some suggestions are made for ways to tackle the problem.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Hand, David J.},
	year = {1994},
	pages = {317},
}

@article{hand_classifier_2006,
	title = {Classifier {Technology} and the {Illusion} of {Progress}},
	volume = {21},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-21/issue-1/Classifier-Technology-and-the-Illusion-of-Progress/10.1214/088342306000000060.full},
	doi = {10.1214/088342306000000060},
	abstract = {A great many tools have been developed for supervised classi fication, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vec tor machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important as pects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typi cally yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm. Key words and phrases: Supervised classification, error rate, misclassifica tion rate, simplicity, principle of parsimony, population drift, selectivity bias, flat maximum effect, problem uncertainty, empirical comparisons.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Statistical Science},
	author = {Hand, David J.},
	month = feb,
	year = {2006},
}

@article{haggerty_methodology_2009,
	title = {Methodology as a {Knife} {Fight}: {The} {Process}, {Politics} and {Paradox} of {Evaluating} {Surveillance}},
	volume = {17},
	issn = {1205-8629, 1572-9877},
	shorttitle = {Methodology as a {Knife} {Fight}},
	url = {http://link.springer.com/10.1007/s10612-009-9083-y},
	doi = {10.1007/s10612-009-9083-y},
	abstract = {This paper uses the analogy of an unregulated ﬁght to examine the rhetorical politics of evaluation research pertaining to surveillance measures. It outlines how, in addition to being standard fare in social scientiﬁc debates, methodological issues have a parallel existence as part of the rhetorical politics of surveillance and crime control. After brieﬂy sketching some of the ways that advocates try and accentuate methodological concerns in attempts to undermine the position of their adversary the paper considers how certain groups are comparatively advantaged and disadvantaged in such exchanges. The concluding section takes a larger view of these dynamics to address some of the risks inherent in engaging in this style of discursive politics.},
	language = {en},
	number = {4},
	urldate = {2021-10-17},
	journal = {Critical Criminology},
	author = {Haggerty, Kevin D.},
	month = dec,
	year = {2009},
	pages = {277--291},
}

@article{james_grimmelmann_incomprehensible_2017,
	title = {Incomprehensible {Discrimination}},
	url = {https://lawcat.berkeley.edu/record/1128018},
	doi = {10.15779/Z38707WN47},
	language = {en},
	urldate = {2021-10-17},
	author = {James Grimmelmann, Daniel Westreich},
	year = {2017},
	note = {Publisher: California Law Review},
}

@article{grgic-hlaca_case_nodate,
	title = {The {Case} for {Process} {Fairness} in {Learning}: {Feature} {Selection} for {Fair} {Decision} {Making}},
	abstract = {Machine learning methods are increasingly being used to inform, or sometimes even directly to make, important decisions about humans. A number of recent works have focussed on the fairness of the outcomes of such decisions, particularly on avoiding decisions that affect users of different sensitive groups (e.g., race, gender) disparately. In this paper, we propose to consider the fairness of the process of decision making. Process fairness can be measured by estimating the degree to which people consider various features to be fair to use when making an important legal decision. We examine the task of predicting whether or not a prisoner is likely to commit a crime again once released by analyzing the dataset considered by ProPublica relating to the COMPAS system. We introduce new measures of people’s discomfort with using various features, show how these measures can be estimated, and consider the effect of removing the uncomfortable features on prediction accuracy and on outcome fairness. Our empirical analysis suggests that process fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
	language = {en},
	author = {Grgic-Hlacˇa, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P and Weller, Adrian},
	pages = {11},
}

@article{goodman_economic_nodate,
	title = {Economic {Models} of ({Algorithmic}) {Discrimination}},
	abstract = {Organizations ranging from consumer ﬁnance to criminal justice increasingly rely on data mining and algorithmic decision-making as a compliment to or, in some cases, substitute for human judgement. This transition is motivated by improved efﬁciency, accuracy and reduced cost. However, it also raises important concerns. Chief among them is algorithmic discrimination, which occurs when certain groups or individuals unfairly receive unfavorable treatment as a result of algorithmic decision-making. Different algorithms discriminate for different reasons; identifying and rectifying algorithmic discrimination requires attention to these key differences. To that end, this paper draws upon literature from economics to differentiate between various sources and consequences of algorithmic discrimination. In particular, I show how the economic theories of taste-based and statistical discrimination apply in an algorithmic setting. The contribution of the work is threefold: to bridge work on algorithmic discrimination in computer and social sciences, to develop generalizable mathematical models for different types of algorithmic discrimination, and to identify sources of algorithmic discrimination that are unaddressed in current literature.},
	language = {en},
	author = {Goodman, Bryce W},
	pages = {10},
}

@incollection{gillespie_relevance_2014,
	title = {The {Relevance} of {Algorithms}},
	isbn = {978-0-262-52537-4},
	url = {http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262525374.001.0001/upso-9780262525374-chapter-9},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Media {Technologies}},
	publisher = {The MIT Press},
	author = {Gillespie, Tarleton},
	editor = {Gillespie, Tarleton and Boczkowski, Pablo J. and Foot, Kirsten A.},
	month = feb,
	year = {2014},
	doi = {10.7551/mitpress/9780262525374.003.0009},
	pages = {167--194},
}

@article{friedler_impossibility_2016,
	title = {On the (im)possibility of fairness},
	url = {http://arxiv.org/abs/1609.07236},
	abstract = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the “observed” space) and outputs (the “decision” space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1609.07236 [cs, stat]},
	author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.07236},
}

@article{feldman_certifying_2015,
	title = {Certifying and removing disparate impact},
	url = {http://arxiv.org/abs/1412.3756},
	abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a deﬁnition of a protected class (ethnicity, gender) and an explicit description of the process.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1412.3756 [cs, stat]},
	author = {Feldman, Michael and Friedler, Sorelle and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	month = jul,
	year = {2015},
	note = {arXiv: 1412.3756},
}

@techreport{edwards_slave_2017,
	type = {preprint},
	title = {Slave to the {Algorithm}? {Why} a 'right to an explanation' is probably not the remedy you are looking for},
	shorttitle = {Slave to the {Algorithm}?},
	url = {https://osf.io/97upg},
	abstract = {Algorithms, particularly machine learning (ML) algorithms, are increasingly important to individuals’ lives, but have caused a range of concerns revolving mainly around unfairness, discrimination and opacity. Transparency in the form of a “right to an explanation” has emerged as a compellingly attractive remedy since it intuitively promises to open the algorithmic “black box” to promote challenge, redress, and hopefully heightened accountability. Amidst the general furore over algorithmic bias we describe, any remedy in a storm has looked attractive.},
	language = {en},
	urldate = {2021-10-17},
	institution = {LawArXiv},
	author = {Edwards, Lilian and Veale, Michael},
	month = nov,
	year = {2017},
	doi = {10.31228/osf.io/97upg},
}

@article{dwork_its_nodate,
	title = {It's {Not} {Privacy}, and {It}'s {Not} {Fair}},
	volume = {66},
	language = {en},
	author = {Dwork, Cynthia and Mulligan, Deirdre K},
	pages = {7},
}

@article{dwork_fairness_2011,
	title = {Fairness {Through} {Awareness}},
	url = {http://arxiv.org/abs/1104.3913},
	abstract = {We study fairness in classiﬁcation, where individuals are classiﬁed, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classiﬁer (the university). The main conceptual contribution of this paper is a framework for fair classiﬁcation comprising (1) a (hypothetical) task-speciﬁc metric for determining the degree to which individuals are similar with respect to the classiﬁcation task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of “fair aﬃrmative action,” which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classiﬁcation are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of diﬀerential privacy may be applied to fairness.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1104.3913 [cs]},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Rich},
	month = nov,
	year = {2011},
	note = {arXiv: 1104.3913},
}

@incollection{starkman_24_2013,
	title = {24. {How} {Companies} {Learn} {Your} {Secrets}},
	isbn = {978-0-231-16075-9},
	url = {https://www.degruyter.com/document/doi/10.7312/star16075-025/html},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {The {Best} {Business} {Writing} 2013},
	publisher = {Columbia University Press},
	author = {Duhigg, Charles},
	editor = {Starkman, Dean},
	month = dec,
	year = {2013},
	doi = {10.7312/star16075-025},
	pages = {421--444},
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08608},
}

@article{diakopoulos_algorithmic_2015,
	title = {Algorithmic {Accountability}: {Journalistic} investigation of computational power structures},
	volume = {3},
	issn = {2167-0811, 2167-082X},
	shorttitle = {Algorithmic {Accountability}},
	url = {http://www.tandfonline.com/doi/full/10.1080/21670811.2014.976411},
	doi = {10.1080/21670811.2014.976411},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {Digital Journalism},
	author = {Diakopoulos, Nicholas},
	month = may,
	year = {2015},
	pages = {398--415},
}

@article{datta_automated_2015,
	title = {Automated {Experiments} on {Ad} {Privacy} {Settings}: {A} {Tale} of {Opacity}, {Choice}, and {Discrimination}},
	volume = {2015},
	issn = {2299-0984},
	shorttitle = {Automated {Experiments} on {Ad} {Privacy} {Settings}},
	url = {https://www.sciendo.com/article/10.1515/popets-2015-0007},
	doi = {10.1515/popets-2015-0007},
	abstract = {To partly address people’s concerns over web tracking, Google has created the Ad Settings webpage to provide information about and some choice over the proﬁles Google creates on users. We present AdFisher, an automated tool that explores how user behaviors, Google’s ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and signiﬁcance tests. Our tool uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results. We use AdFisher to ﬁnd that the Ad Settings was opaque about some features of a user’s proﬁle, that it does provide some choice on ads, and that these choices can lead to seemingly discriminatory ads. In particular, we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page. We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male. We cannot determine who caused these ﬁndings due to our limited visibility into the ad ecosystem, which includes Google, advertisers, websites, and users. Nevertheless, these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Datta, Amit and Tschantz, Michael Carl and Datta, Anupam},
	month = apr,
	year = {2015},
	pages = {92--112},
}

@article{crawford_big_nodate,
	title = {Big {Data} and {Due} {Process}: {Toward} a {Framework} to {Redress} {Predictive} {Privacy} {Harms}},
	volume = {55},
	abstract = {The rise of “Big Data” analytics in the private sector poses new challenges for privacy advocates. Through its reliance on existing data and predictive analysis to create detailed individual profiles, Big Data has exploded the scope of personally identifiable information (“PII”). It has also effectively marginalized regulatory schema by evading current privacy protections with its novel methodology. Furthermore, poor execution of Big Data methodology may create additional harms by rendering inaccurate profiles that nonetheless impact an individual’s life and livelihood. To respond to Big Data’s evolving practices, this Article examines several existing privacy regimes and explains why these approaches inadequately address current Big Data challenges. This Article then proposes a new approach to mitigating predictive privacy harms—that of a right to procedural data due process. Although current privacy regimes offer limited nominal due process-like mechanisms, a more rigorous framework is needed to address their shortcomings. By examining due process’s role in the Anglo-American legal system and building on previous scholarship about due process for public administrative computer systems, this Article argues that individuals affected by Big Data should have similar rights to those in the legal system with respect to how their personal data is used in such adjudications. Using these principles, this Article analogizes a system of regulation that would provide such rights against private Big Data actors.},
	language = {en},
	author = {Crawford, Kate and Schultz, Jason},
	pages = {37},
}

@article{citron_scored_nodate,
	title = {The {Scored} {Society}: {Due} {Process} for {Automated} {Predictions}},
	volume = {89},
	abstract = {Big Data is increasingly mined to rank and rate individuals. Predictive algorithms assess whether we are good credit risks, desirable employees, reliable tenants, valuable customers—or deadbeats, shirkers, menaces, and “wastes of time.” Crucial opportunities are on the line, including the ability to obtain loans, work, housing, and insurance. Though automated scoring is pervasive and consequential, it is also opaque and lacking oversight. In one area where regulation does prevail—credit—the law focuses on credit history, not the derivation of scores from data.},
	language = {en},
	journal = {WASHINGTON LAW REVIEW},
	author = {Citron, Danielle Keats and Pasquale, Frank A},
	pages = {33},
}

@article{chouldechova_fair_2017,
	title = {Fair {Prediction} with {Disparate} {Impact}: {A} {Study} of {Bias} in {Recidivism} {Prediction} {Instruments}},
	volume = {5},
	issn = {2167-6461, 2167-647X},
	shorttitle = {Fair {Prediction} with {Disparate} {Impact}},
	url = {http://www.liebertpub.com/doi/10.1089/big.2016.0047},
	doi = {10.1089/big.2016.0047},
	abstract = {Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisﬁed when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance.},
	language = {en},
	number = {2},
	urldate = {2021-10-17},
	journal = {Big Data},
	author = {Chouldechova, Alexandra},
	month = jun,
	year = {2017},
	pages = {153--163},
}

@article{chen_enhancing_2017,
	title = {Enhancing {Transparency} and {Control} {When} {Drawing} {Data}-{Driven} {Inferences} {About} {Individuals}},
	volume = {5},
	issn = {2167-6461, 2167-647X},
	url = {http://www.liebertpub.com/doi/10.1089/big.2017.0074},
	doi = {10.1089/big.2017.0074},
	abstract = {Recent studies show the remarkable power of ﬁne-grained information disclosed by users on social network sites to infer users’ personal characteristics via predictive modeling. Similar ﬁne-grained data are being used successfully in other commercial applications. In response, attention is turning increasingly to the transparency that organizations provide to users as to what inferences are drawn and why, as well as to what sort of control users can be given over inferences that are drawn about them. In this article, we focus on inferences about personal characteristics based on information disclosed by users’ online actions. As a use case, we explore personal inferences that are made possible from ‘‘Likes’’ on Facebook. We ﬁrst present a means for providing transparency into the information responsible for inferences drawn by data-driven models. We then introduce the ‘‘cloaking device’’—a mechanism for users to inhibit the use of particular pieces of information in inference. Using these analytical tools we ask two main questions: (1) How much information must users cloak to signiﬁcantly affect inferences about their personal traits? We ﬁnd that usually users must cloak only a small portion of their actions to inhibit inference. We also ﬁnd that, encouragingly, false-positive inferences are signiﬁcantly easier to cloak than true-positive inferences. (2) Can ﬁrms change their modeling behavior to make cloaking more difﬁcult? The answer is a deﬁnitive yes. We demonstrate a simple modeling change that requires users to cloak substantially more information to affect the inferences drawn. The upshot is that organizations can provide transparency and control even into complicated, predictive model-driven inferences, but they also can make control easier or harder for their users.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {Big Data},
	author = {Chen, Daizhuo and Fraiberger, Samuel P. and Moakler, Robert and Provost, Foster},
	month = sep,
	year = {2017},
	pages = {197--212},
}

@article{calo_digital_2013,
	title = {Digital {Market} {Manipulation}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2309703},
	doi = {10.2139/ssrn.2309703},
	abstract = {In 1999, Jon Hanson and Douglas Kysar coined the term “market manipulation” to describe how companies exploit the cognitive limitations of consumers. For example, everything costs \$9.99 because consumers see the price as closer to \$9 than \$10. Although widely cited by academics, the concept of market manipulation has had only a modest impact on consumer protection law.},
	language = {en},
	urldate = {2021-10-17},
	journal = {SSRN Electronic Journal},
	author = {Calo, M. Ryan},
	year = {2013},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	language = {en},
	number = {6334},
	urldate = {2021-10-17},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pages = {183--186},
}

@article{burrell_how_2016,
	title = {How the machine ‘thinks’: {Understanding} opacity in machine learning algorithms},
	volume = {3},
	issn = {2053-9517, 2053-9517},
	shorttitle = {How the machine ‘thinks’},
	url = {http://journals.sagepub.com/doi/10.1177/2053951715622512},
	doi = {10.1177/2053951715622512},
	abstract = {This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Big Data \& Society},
	author = {Burrell, Jenna},
	month = jun,
	year = {2016},
	pages = {205395171562251},
}

@article{boyd_critical_2012,
	title = {{CRITICAL} {QUESTIONS} {FOR} {BIG} {DATA}: {Provocations} for a cultural, technological, and scholarly phenomenon},
	volume = {15},
	issn = {1369-118X, 1468-4462},
	shorttitle = {{CRITICAL} {QUESTIONS} {FOR} {BIG} {DATA}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.678878},
	doi = {10.1080/1369118X.2012.678878},
	language = {en},
	number = {5},
	urldate = {2021-10-17},
	journal = {Information, Communication \& Society},
	author = {boyd, danah and Crawford, Kate},
	month = jun,
	year = {2012},
	pages = {662--679},
}

@book{bowker_sorting_1999,
	address = {Cambridge, Mass},
	series = {Inside technology},
	title = {Sorting things out: classification and its consequences},
	isbn = {978-0-262-02461-7},
	shorttitle = {Sorting things out},
	language = {en},
	publisher = {MIT Press},
	author = {Bowker, Geoffrey C. and Star, Susan Leigh},
	year = {1999},
}

@article{bornstein_is_nodate,
	title = {Is {Artificial} {Intelligence} {Permanently} {Inscrutable}?},
	language = {en},
	author = {Bornstein, Aaron M and Polanco, Illustration Emmanuel},
	pages = {11},
}

@article{bertrand_are_2004,
	title = {Are {Emily} and {Greg} {More} {Employable} than {Lakisha} and {Jamal}? {A} {Field} {Experiment} on {Labor} {Market} {Discrimination}},
	language = {en},
	journal = {THE AMERICAN ECONOMIC REVIEW},
	author = {Bertrand, Marianne and Mullainathan, Sendhil},
	year = {2004},
	pages = {24},
}

@article{barocas_big_2016,
	title = {Big {Data}'s {Disparate} {Impact}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=2477899},
	doi = {10.2139/ssrn.2477899},
	language = {en},
	urldate = {2021-10-17},
	journal = {SSRN Electronic Journal},
	author = {Barocas, Solon and Selbst, Andrew D.},
	year = {2016},
}

@article{barocas_big_2014,
	title = {Big data's end run around procedural privacy protections},
	volume = {57},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2668897},
	doi = {10.1145/2668897},
	abstract = {Recognizing the inherent limitations of consent and anonymity.},
	language = {en},
	number = {11},
	urldate = {2021-10-17},
	journal = {Communications of the ACM},
	author = {Barocas, Solon and Nissenbaum, Helen},
	month = oct,
	year = {2014},
	pages = {31--33},
}

@article{ananny_seeing_2018,
	title = {Seeing without knowing: {Limitations} of the transparency ideal and its application to algorithmic accountability},
	volume = {20},
	issn = {1461-4448, 1461-7315},
	shorttitle = {Seeing without knowing},
	url = {http://journals.sagepub.com/doi/10.1177/1461444816676645},
	doi = {10.1177/1461444816676645},
	abstract = {Models for understanding and holding systems accountable have long rested upon ideals and logics of transparency. Being able to see a system is sometimes equated with being able to know how it works and govern it—a pattern that recurs in recent work about transparency and computational systems. But can “black boxes’ ever be opened, and if so, would that ever be sufficient? In this article, we critically interrogate the ideal of transparency, trace some of its roots in scientific and sociotechnical epistemological cultures, and present 10 limitations to its application. We specifically focus on the inadequacy of transparency for understanding and governing algorithmic systems and sketch an alternative typology of algorithmic accountability grounded in constructive engagements with the limitations of transparency ideals.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {New Media \& Society},
	author = {Ananny, Mike and Crawford, Kate},
	month = mar,
	year = {2018},
	pages = {973--989},
}

@inproceedings{ethics_in_natural_language_processing_ethics_2017,
	address = {Valencia, Spain},
	title = {Ethics in {Natural} {Language} {Processing}},
	author = {Ethics in Natural Language Processing},
	month = apr,
	year = {2017},
}

@article{lan_discriminatory_2017,
	title = {Discriminatory {Transfer}},
	url = {http://arxiv.org/abs/1707.00780},
	abstract = {We observe standard transfer learning can improve prediction accuracies of target tasks at the cost of lowering their prediction fairness – a phenomenon we named discriminatory transfer. We examine prediction fairness of a standard hypothesis transfer algorithm and a standard multi-task learning algorithm, and show they both suﬀer discriminatory transfer on the real-world Communities and Crime data set. The presented case study introduces an interaction between fairness and transfer learning, as an extension of existing fairness studies that focus on single task learning.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.00780 [cs, stat]},
	author = {Lan, Chao and Huan, Jun},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.00780},
}

@article{lakkaraju_interpretable_2017,
	title = {Interpretable \& {Explorable} {Approximations} of {Black} {Box} {Models}},
	url = {http://arxiv.org/abs/1707.01154},
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classi er by simultaneously optimizing for delity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-de ned regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in di erent subspaces that are of interest to the user. To the best of our knowledge, this is the rst approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, delity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with realworld datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.01154 [cs]},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01154},
}

@article{kleinberg_guide_nodate,
	title = {A {Guide} to {Solving} {Social} {Problems} with {Machine} {Learning}},
	language = {en},
	author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil},
	pages = {11},
}

@article{khandani_consumer_nodate,
	title = {Consumer {Credit} {Risk} {Models} via {Machine}-{Learning} {Algorithms}},
	abstract = {We apply machine-learning techniques to construct nonlinear nonparametric forecasting models of consumer credit risk. By combining customer transactions and credit bureau data from January 2005 to April 2009 for a sample of a major commercial bank’s customers, we are able to construct out-of-sample forecasts that signiﬁcantly improve the classiﬁcation rates of credit-card-holder delinquencies and defaults, with linear regression R2’s of forecasted/realized delinquencies of 85\%. Using conservative assumptions for the costs and beneﬁts of cutting credit lines based on machine-learning forecasts, we estimate the cost savings to range from 6\% to 25\% of total losses. Moreover, the time-series patterns of estimated delinquency rates from this model over the course of the recent ﬁnancial crisis suggests that aggregated consumer-credit risk analytics may have important applications in forecasting systemic risk.},
	language = {en},
	author = {Khandani, Amir E and Kim, Adlar J and Lo, Andrew W},
	pages = {55},
}

@article{joseph_better_nodate,
	title = {Better {Fair} {Algorithms} for {Contextual} {Bandits}},
	abstract = {We study fairness in the linear bandit setting. Starting from the notion of meritocratic fairness introduced in Joseph et al. [11], we introduce a suﬃciently more general model in which meritocratic fairness can be imposed and satisﬁed. We then perform a more ﬁne-grained analysis which achieves better performance guarantees in this more general model. Our work therefore studies fairness for a more general problem and provides tighter performance guarantees than previous work in the simpler setting.},
	language = {en},
	author = {Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
	pages = {5},
}

@article{grgic-hlaca_fairness_2017,
	title = {On {Fairness}, {Diversity} and {Randomness} in {Algorithmic} {Decision} {Making}},
	url = {http://arxiv.org/abs/1706.10208},
	abstract = {Consider a binary decision making process where a single machine learning classifier replaces a multitude of humans. We raise questions about the resulting loss of diversity in the decision making process. We study the potential benefits of using random classifier ensembles instead of a single classifier in the context of fairness-aware learning and demonstrate various attractive properties: (i) an ensemble of fair classifiers is guaranteed to be fair, for several different measures of fairness, (ii) an ensemble of unfair classifiers can still achieve fair outcomes, and (iii) an ensemble of classifiers can achieve better accuracy-fairness trade-offs than a single classifier. Finally, we introduce notions of distributional fairness to characterize further potential benefits of random classifier ensembles.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1706.10208 [cs, stat]},
	author = {Grgić-Hlača, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P. and Weller, Adrian},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.10208},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
}

@article{garcia-martin_is_2017,
	title = {Is it ethical to avoid error analysis?},
	url = {http://arxiv.org/abs/1706.10237},
	abstract = {Machine learning algorithms tend to create more accurate models with the availability of large datasets. In some cases, highly accurate models can hide the presence of bias in the data. There are several studies published that tackle the development of discriminatoryaware machine learning algorithms. We center on the further evaluation of machine learning models by doing error analysis, to understand under what conditions the model is not working as expected. We focus on the ethical implications of avoiding error analysis, from a falsi cation of results and discrimination perspective. Finally, we show di erent ways to approach error analysis in non-interpretable machine learning algorithms such as deep learning.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1706.10237 [cs]},
	author = {García-Martín, Eva and Lavesson, Niklas},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.10237},
}

@article{ensign_decision_2017,
	title = {Decision making with limited feedback:{Error} bounds for recidivism prediction and predictive policing},
	abstract = {When models are trained for deployment in decision-making in various real-world settings, they are typically trained in batch mode. Historical data is used to train and validate the models prior to deployment. However, in many settings, feedback changes the nature of the training process. Either the learner does not get full feedback on its actions, or the decisions made by the trained model influence what future training data it will see. We focus on the problems of recidivism prediction and predictive policing, showing that both problems (and others like these) can be abstracted into a general reinforcement learning framework called partial monitoring. We then design algorithms that yield provable guarantees on regret for these problems, and discuss the policy implications of these solutions.},
	language = {en},
	journal = {Nova Scotia},
	author = {Ensign, Danielle and Friedler, Sorelle A and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	year = {2017},
	pages = {5},
}

@article{dwork_decoupled_nodate,
	title = {Decoupled {Classiﬁers} for {Group}-{Fair} and {Eﬃcient} {Machine} {Learning}},
	abstract = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the na¨ıve application of machine learning algorithms using sensitive attributes leads to an inherent tradeoﬀ in accuracy between groups. We provide a simple and eﬃcient decoupling technique, which can be added on top of any black-box machine learning algorithm, to learn diﬀerent classiﬁers for diﬀerent groups. Transfer learning is used to mitigate the problem of having too little data on any one group.},
	language = {en},
	author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
	pages = {15},
}

@article{dwork_decoupled_nodate-1,
	title = {Decoupled classiﬁers for fair and eﬃcient machine learning},
	abstract = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoﬀ in accuracy between groups. We provide a simple and eﬃcient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn diﬀerent classiﬁers for diﬀerent groups. The method can apply to a range of fairness criteria. In particular, we require the application designer to specify as joint loss function that makes explicit the trade-oﬀ between fairness and accuracy. Our reduction is shown to eﬃciently ﬁnd the global optimum loss as long as the objective has a certain natural monotonicity property. Monotonicity may be of independent interest in the study of fairness in algorithms.},
	language = {en},
	author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
	pages = {7},
}

@inproceedings{datta_algorithmic_2016,
	address = {San Jose, CA},
	title = {Algorithmic {Transparency} via {Quantitative} {Input} {Influence}: {Theory} and {Experiments} with {Learning} {Systems}},
	isbn = {978-1-5090-0824-7},
	shorttitle = {Algorithmic {Transparency} via {Quantitative} {Input} {Influence}},
	url = {http://ieeexplore.ieee.org/document/7546525/},
	doi = {10.1109/SP.2016.42},
	abstract = {Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque—it is difﬁcult to explain why a certain decision was made. We develop a formal foundation to improve the transparency of such decision-making systems. Speciﬁcally, we introduce a family of Quantitative Input Inﬂuence (QII) measures that capture the degree of inﬂuence of inputs on outputs of systems. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a speciﬁc credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination).},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {2016 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Datta, Anupam and Sen, Shayak and Zick, Yair},
	month = may,
	year = {2016},
	pages = {598--617},
}

@article{corbett-davies_measure_2018,
	title = {The {Measure} and {Mismeasure} of {Fairness}: {A} {Critical} {Review} of {Fair} {Machine} {Learning}},
	shorttitle = {The {Measure} and {Mismeasure} of {Fairness}},
	url = {http://arxiv.org/abs/1808.00023},
	abstract = {The nascent ﬁeld of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal deﬁnitions of fairness have gained prominence: (1) anti-classiﬁcation, meaning that protected attributes—like race, gender, and their proxies—are not explicitly used to make decisions; (2) classiﬁcation parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups deﬁned by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness deﬁnitions suﬀer from signiﬁcant statistical limitations. Requiring anticlassiﬁcation or classiﬁcation parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classiﬁcation and classiﬁcation parity. In practice, it requires signiﬁcant eﬀort to construct suitable risk estimates. One must carefully deﬁne and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these diﬃculties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1808.00023 [cs]},
	author = {Corbett-Davies, Sam and Goel, Sharad},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.00023},
}

@article{craven_extracting_nodate,
	title = {Extracting {Tree}-{Structured} {Representations} of {Trained} {Networks}},
	abstract = {A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm , TREPAN, for extracting comprehensible , symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that TREPAN is able to produce decision trees that maintain a high level of fidelity to their respective networks while being comprehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large networks and problems with high-dimensional input spaces.},
	language = {en},
	author = {Craven, Mark and Shavlik, Jude W},
	pages = {7},
}

@article{chouldechova_fairer_2017,
	title = {Fairer and more accurate, but for whom?},
	url = {http://arxiv.org/abs/1707.00046},
	abstract = {Complex statistical machine learning models are increasingly being used or considered for use in highstakes decision-making pipelines in domains such as nancial services, health care, criminal justice and human services. ese models are o en investigated as possible improvements over more classical tools such as regression models or human judgement. While the modeling approach may be new, the practice of using some form of risk assessment to inform decisions is not. When determining whether a new model should be adopted, it is therefore essential to be able to compare the proposed model to the existing approach across a range of task-relevant accuracy and fairness metrics. Looking at overall performance metrics, however, may be misleading. Even when two models have comparable overall performance, they may nevertheless disagree in their classi cations on a considerable fraction of cases. In this paper we introduce a model comparison framework for automatically identifying subgroups in which the di erences between models are most pronounced. Our primary focus is on identifying subgroups where the models di er in terms of fairness-related quantities such as racial or gender disparities. We present experimental results from a recidivism prediction task and a hypothetical lending example.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.00046 [cs, stat]},
	author = {Chouldechova, Alexandra and G'Sell, Max},
	month = jun,
	year = {2017},
	note = {arXiv: 1707.00046},
}

@article{chierichetti_fair_nodate,
	title = {Fair {Clustering} {Through} {Fairlets}},
	abstract = {We study the question of fair clustering under the disparate impact doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions—for instance a point may no longer be assigned to its nearest cluster center! En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective. We show that any fair clustering problem can be decomposed into ﬁrst ﬁnding good fairlets, and then using existing machinery for traditional clustering algorithms. While ﬁnding good fairlets can be NP-hard, we proceed to obtain eﬃcient approximation algorithms based on minimum cost ﬂow.},
	language = {en},
	author = {Chierichetti, Flavio and Kumar, Ravi and Lattanzi, Silvio and Vassilvitskii, Sergei},
	pages = {8},
}

@article{celis_fair_2017,
	title = {Fair {Personalization}},
	url = {http://arxiv.org/abs/1707.02260},
	abstract = {Personalization is pervasive in the online space as, when combined with learning, it leads to higher efficiency and revenue by allowing the most relevant content to be served to each user. However, recent studies suggest that such personalization can propagate societal or systemic biases, which has led to calls for regulatory mechanisms and algorithms to combat inequality. Here we propose a rigorous algorithmic framework that allows for the possibility to control biased or discriminatory personalization with respect to sensitive attributes of users without losing all of the benefits of personalization.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.02260 [cs]},
	author = {Celis, L. Elisa and Vishnoi, Nisheeth K.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.02260},
}

@inproceedings{caruana_intelligible_2015,
	address = {Sydney NSW Australia},
	title = {Intelligible {Models} for {HealthCare}: {Predicting} {Pneumonia} {Risk} and {Hospital} 30-day {Readmission}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {Intelligible {Models} for {HealthCare}},
	url = {https://dl.acm.org/doi/10.1145/2783258.2788613},
	doi = {10.1145/2783258.2788613},
	abstract = {In machine learning often a tradeoﬀ must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have signiﬁcantly worse accuracy. This tradeoﬀ sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being ﬁelded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
	month = aug,
	year = {2015},
	pages = {1721--1730},
}

@article{burke_multisided_2017,
	title = {Multisided {Fairness} for {Recommendation}},
	url = {http://arxiv.org/abs/1707.00093},
	abstract = {Recent work on machine learning has begun to consider issues of fairness. In this paper, we extend the concept of fairness to recommendation. In particular, we show that in some recommendation contexts, fairness may be a multisided concept, in which fair outcomes for multiple individuals need to be considered. Based on these considerations, we present a taxonomy of classes of fairness-aware recommender systems and suggest possible fairness-aware recommendation architectures.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.00093 [cs]},
	author = {Burke, Robin},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.00093},
}

@article{bower_fair_2017,
	title = {Fair {Pipelines}},
	url = {http://arxiv.org/abs/1707.00391},
	abstract = {This work facilitates ensuring fairness of machine learning in the real world by decoupling fairness considerations in compound decisions. In particular, this work studies how fairness propagates through a compound decision-making processes, which we call a pipeline. Prior work in algorithmic fairness only focuses on fairness with respect to one decision. However, many decision-making processes require more than one decision. For instance, hiring is at least a two stage model: deciding who to interview from the applicant pool and then deciding who to hire from the interview pool. Perhaps surprisingly, we show that the composition of fair components may not guarantee a fair pipeline under a (1 + ε)-equal opportunity definition of fair. However, we identify circumstances that do provide that guarantee. We also propose numerous directions for future work on more general compound machine learning decisions.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.00391 [cs, stat]},
	author = {Bower, Amanda and Kitchen, Sarah N. and Niss, Laura and Strauss, Martin J. and Vargas, Alexander and Venkatasubramanian, Suresh},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.00391},
}

@article{blodgett_racial_2017,
	title = {Racial {Disparity} in {Natural} {Language} {Processing}: {A} {Case} {Study} of {Social} {Media} {African}-{American} {English}},
	shorttitle = {Racial {Disparity} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1707.00061},
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of di erent social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identi cation for tweets wri en in African-American English, and discuss implications of disparity in NLP.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.00061 [cs]},
	author = {Blodgett, Su Lin and O'Connor, Brendan},
	month = jun,
	year = {2017},
	note = {arXiv: 1707.00061},
}

@article{beutel_data_2017,
	title = {Data {Decisions} and {Theoretical} {Implications} when {Adversarially} {Learning} {Fair} {Representations}},
	url = {http://arxiv.org/abs/1707.00075},
	abstract = {How can we learn a classi er that is “fair” for a protected or sensitive group, when we do not know if the input to the classi er belongs to the protected group? How can we train such a classi er when data on the protected group is di cult to a ain? In many settings, nding out the sensitive input a ribute can be prohibitively expensive even during model training, and sometimes impossible during model serving. For example, in recommender systems, if we want to predict if a user will click on a given recommendation, we o en do not know many a ributes of the user, e.g., race or age, and many a ributes of the content are hard to determine, e.g., the language or topic. us, it is not feasible to use a di erent classi er calibrated based on knowledge of the sensitive a ribute.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.00075 [cs]},
	author = {Beutel, Alex and Chen, Jilin and Zhao, Zhe and Chi, Ed H.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.00075},
}

@article{berk_convex_nodate,
	title = {A {Convex} {Framework} for {Fair} {Regression}},
	abstract = {We introduce a ﬂexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and span the range from group fairness to strong individual fairness. We study the accuracy-fairness trade-oﬀ on any given dataset, and we measure the severity of this trade-oﬀ via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six diﬀerent datasets in which fairness is a primary consideration.},
	language = {en},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
	pages = {5},
}

@book{berk_criminal_2012,
	address = {New York, NY},
	series = {{SpringerBriefs} in {Computer} {Science}},
	title = {Criminal {Justice} {Forecasts} of {Risk}},
	isbn = {978-1-4614-3084-1 978-1-4614-3085-8},
	url = {http://link.springer.com/10.1007/978-1-4614-3085-8},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer New York},
	author = {Berk, Richard},
	year = {2012},
	doi = {10.1007/978-1-4614-3085-8},
}

@article{benton_multi-task_nodate,
	title = {Multi-{Task} {Learning} for {Mental} {Health} using {Social} {Media} {Text}},
	abstract = {We introduce initial groundwork for estimating suicide risk and mental health in a deep learning framework. By modeling multiple conditions, the system learns to make predictions about suicide risk and mental health at a low false positive rate. Conditions are modeled as tasks in a multitask learning (MTL) framework, with gender prediction as an additional auxiliary task. We demonstrate the effectiveness of multi-task learning by comparison to a well-tuned single-task baseline with the same number of parameters. Our best MTL model predicts potential suicide attempt, as well as the presence of atypical mental health, with AUC {\textgreater} 0.8. We also ﬁnd additional large improvements using multi-task learning on mental health tasks with limited training data.},
	language = {en},
	author = {Benton, Adrian and Mitchell, Margaret and Hovy, Dirk},
	pages = {11},
}

@article{bastani_interpretability_2018,
	title = {Interpretability via {Model} {Extraction}},
	url = {http://arxiv.org/abs/1706.09773},
	abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1706.09773 [cs, stat]},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	month = mar,
	year = {2018},
	note = {arXiv: 1706.09773},
}

@article{bantilan_themis-ml_2017,
	title = {Themis-ml: {A} {Fairness}-aware {Machine} {Learning} {Interface} for {End}-to-end {Discrimination} {Discovery} and {Mitigation}},
	shorttitle = {Themis-ml},
	url = {http://arxiv.org/abs/1710.06921},
	abstract = {As more industries integrate machine learning into socially sensitive decision processes like hiring, loan-approval, and parole-granting, we are at risk of perpetuating historical and contemporary socioeconomic disparities. This is a critical problem because on the one hand, organizations who use but do not understand the discriminatory potential of such systems will facilitate the widening of social disparities under the assumption that algorithms are categorically objective. On the other hand, the responsible use of machine learning can help us measure, understand, and mitigate the implicit historical biases in socially sensitive data by expressing implicit decision-making mental models in terms of explicit statistical models. In this paper we specify, implement, and evaluate a “fairness-aware” machine learning interface called themis-ml, which is intended for use by individual data scientists and engineers, academic research teams, or larger product teams who use machine learning in production systems.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1710.06921 [cs]},
	author = {Bantilan, Niels},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06921},
}

@article{baehrens_how_nodate,
	title = {How to {Explain} {Individual} {Classiﬁcation} {Decisions}},
	abstract = {After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method.},
	language = {en},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja},
	pages = {29},
}

@article{athey_beyond_2017,
	title = {Beyond prediction: {Using} big data for policy problems},
	volume = {355},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Beyond prediction},
	url = {https://www.science.org/doi/10.1126/science.aal4321},
	doi = {10.1126/science.aal4321},
	language = {en},
	number = {6324},
	urldate = {2021-10-17},
	journal = {Science},
	author = {Athey, Susan},
	month = feb,
	year = {2017},
	pages = {483--485},
}

@book{craglia_artificial_2018,
	address = {Luxembourg},
	series = {{EUR} / {Joint} {Research} {Centre}},
	title = {Artificial intelligence: a european perspective},
	isbn = {978-92-79-97217-1 978-92-79-97219-5},
	shorttitle = {Artificial intelligence},
	language = {en},
	number = {29425 EN},
	publisher = {Publications Office of the European Union},
	editor = {Craglia, Max and Europäische Gemeinschaften},
	year = {2018},
	doi = {10.2760/936974},
}

@article{agarwal_areductionsapproachtofairclassification_2017,
	title = {{AReductionsApproachtoFairClassification}},
	language = {en},
	journal = {Nova Scotia},
	author = {Agarwal, Alekh and Beygelzimer, Alina and Dudík, Miroslav and Langford, John},
	year = {2017},
	pages = {5},
}

@article{adler_auditing_2016,
	title = {Auditing {Black}-box {Models} for {Indirect} {Influence}},
	url = {http://arxiv.org/abs/1602.07043},
	abstract = {Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior, and in particular how different features inﬂuence the model prediction. This is important when interpreting the behavior of complex models, or asserting that certain problematic attributes (like race or gender) are not unduly inﬂuencing decisions.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1602.07043 [cs, stat]},
	author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A. and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
	month = nov,
	year = {2016},
	note = {arXiv: 1602.07043},
}

@article{adebayo_iterative_2016,
	title = {Iterative {Orthogonal} {Feature} {Projection} for {Diagnosing} {Bias} in {Black}-{Box} {Models}},
	url = {http://arxiv.org/abs/1611.04967},
	abstract = {Predictive models are increasingly deployed for the purpose of determining access to services such as credit, insurance, and employment. Despite potential gains in productivity and efﬁciency, several potential problems have yet to be addressed, particularly the potential for unintentional discrimination. We present an iterative procedure, based on orthogonal projection of input attributes, for enabling interpretability of black-box predictive models. Through our iterative procedure, one can quantify the relative dependence of a black-box model on its input attributes.The relative signiﬁcance of the inputs to a predictive model can then be used to assess the fairness (or discriminatory extent) of such a model.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1611.04967 [cs, stat]},
	author = {Adebayo, Julius and Kagal, Lalana},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.04967},
}

@article{zhang_identifying_2017,
	title = {Identifying {Significant} {Predictive} {Bias} in {Classifiers}},
	url = {http://arxiv.org/abs/1611.08292},
	abstract = {We present a novel subset scan method to detect if a probabilistic binary classi er has statistically signi cant bias —over or under predicting the risk — for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of- t test provides a way to interpretably detect the presence of classi er bias or regions of poor classi er t. This allows consideration of not just subgroups of a priori interest or small dimensions, but the space of all possible subgroups of features. To address the di culty of considering these exponentially many possible subgroups, we use subset scan and parametric bootstrap-based methods. Extending this method, we can penalize the complexity of the detected subgroup and also identify subgroups with high classi cation errors. We demonstrate these methods and nd interesting results on the COMPAS crime recidivism and credit delinquency data.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1611.08292 [cs, stat]},
	author = {Zhang, Zhe and Neill, Daniel B.},
	month = jul,
	year = {2017},
	note = {arXiv: 1611.08292},
}

@article{zemel_learning_nodate,
	title = {Learning {Fair} {Representations}},
	abstract = {We propose a learning algorithm for fair classiﬁcation that achieves both group fairness (the proportion of members in a protected group receiving positive classiﬁcation is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of ﬁnding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classiﬁcation tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can ﬁnd important dimensions of the data for classiﬁcation.},
	language = {en},
	author = {Zemel, Richard},
	pages = {9},
}

@article{zafar_fairness_2017,
	title = {Fairness {Constraints}: {Mechanisms} for {Fair} {Classification}},
	shorttitle = {Fairness {Constraints}},
	url = {http://arxiv.org/abs/1507.05259},
	abstract = {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as oﬄine services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and proﬁtability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, beneﬁt) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a ﬂexible mechanism to design fair classiﬁers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classiﬁers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a ﬁne-grained control on the degree of fairness, often at a small cost in terms of accuracy.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1507.05259 [cs, stat]},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P.},
	month = mar,
	year = {2017},
	note = {arXiv: 1507.05259},
}

@article{zafar_parity_nodate,
	title = {From {Parity} to {Preference}-based {Notions} of {Fairness} in {Classiﬁcation}},
	language = {en},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P},
	pages = {5},
}

@article{yao_new_2017,
	title = {New {Fairness} {Metrics} for {Recommendation} that {Embrace} {Differences}},
	url = {http://arxiv.org/abs/1706.09838},
	abstract = {We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative filtering methods to make unfair predictions against minority groups of users. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1706.09838 [cs]},
	author = {Yao, Sirui and Huang, Bert},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.09838},
}

@techreport{wachter_right_2018,
	type = {preprint},
	title = {A {Right} to {Reasonable} {Inferences}: {Re}-{Thinking} {Data} {Protection} {Law} in the {Age} of {Big} {Data} and {AI}},
	shorttitle = {A {Right} to {Reasonable} {Inferences}},
	url = {https://osf.io/mu2kf},
	abstract = {Big Data analytics and artificial intelligence (AI) draw non-intuitive and unverifiable inferences and predictions about the behaviors, preferences, and private lives of individuals. These inferences draw on highly diverse and feature-rich data of unpredictable value, and create new opportunities for discriminatory, biased, and invasive decision-making. Concerns about algorithmic accountability are often actually concerns about the way in which these technologies draw privacy invasive and non-verifiable inferences about us that we cannot predict, understand, or refute.Data protection law is meant to protect people’s privacy, identity, reputation, and autonomy, but is currently failing to protect data subjects from the novel risks of inferential analytics. The broad concept of personal data in Europe could be interpreted to include inferences, predictions, and assumptions that refer to or impact on an individual. If seen as personal data, individuals are granted numerous rights under data protection law. However, the legal status of inferences is heavily disputed in legal scholarship, and marked by inconsistencies and contradictions within and between the views of the Article 29 Working Party and the European Court of Justice.As we show in this paper, individuals are granted little control and oversight over how their personal data is used to draw inferences about them. Compared to other types of personal data, inferences are effectively ‘economy class’ personal data in the General Data Protection Regulation (GDPR). Data subjects’ rights to know about (Art 13-15), rectify (Art 16), delete (Art 17), object to (Art 21), or port (Art 20) personal data are significantly curtailed when it comes to inferences, often requiring a greater balance with controller’s interests (e.g. trade secrets, intellectual property) than would otherwise be the case. Similarly, the GDPR provides insufficient protection against sensitive inferences (Art 9) or remedies to challenge inferences or important decisions based on them (Art 22(3)).This situation is not accidental. In standing jurisprudence the European Court of Justice (ECJ; Bavarian Lager, YS. and M. and S., and Nowak) and the Advocate General (AG; YS. and M. and S. and Nowak) have consistently restricted the remit of data protection law to assessing the legitimacy of input personal data undergoing processing, and to rectify, block, or erase it. Critically, the ECJ has likewise made clear that data protection law is not intended to ensure the accuracy of decisions and decision-making processes involving personal data, or to make these processes fully transparent.Conflict looms on the horizon in Europe that will further weaken the protection afforded to data subjects against inferences. Current policy proposals addressing privacy protection (the ePrivacy Regulation and the EU Digital Content Directive) fail to close the GDPR’s accountability gaps concerning inferences. At the same time, the GDPR and Europe’s new Copyright Directive aim to facilitate data mining, knowledge discovery, and Big Data analytics by limiting data subjects’ rights over personal data. And lastly, the new Trades Secrets Directive provides extensive protection of commercial interests attached to the outputs of these processes (e.g. models, algorithms and inferences).In this paper we argue that a new data protection right, the ‘right to reasonable inferences’, is needed to help close the accountability gap currently posed ‘high risk inferences’ , meaning inferences that are privacy invasive or reputation damaging and have low verifiability in the sense of being predictive or opinion-based. In cases where algorithms draw ‘high risk inferences’ about individuals, this right would require ex-ante justification to be given by the data controller to establish whether an inference is reasonable. This disclosure would address (1) why certain data is a relevant basis to draw inferences; (2) why these inferences are relevant for the chosen processing purpose or type of automated decision; and (3) whether the data and methods used to draw the inferences are accurate and statistically reliable. The ex-ante justification is bolstered by an additional ex-post mechanism enabling unreasonable inferences to be challenged. A right to reasonable inferences must, however, be reconciled with EU jurisprudence and counterbalanced with IP and trade secrets law as well as freedom of expression and Article 16 of the EU Charter of Fundamental Rights: the freedom to conduct a business.},
	language = {en},
	urldate = {2021-10-17},
	institution = {LawArXiv},
	author = {Wachter, Sandra and Mittelstadt, Brent},
	month = oct,
	year = {2018},
	doi = {10.31228/osf.io/mu2kf},
}

@article{veale_logics_2018,
	title = {Logics and practices of transparency and opacity in real-world applications of public sector machine learning},
	url = {http://arxiv.org/abs/1706.09249},
	abstract = {Machine learning systems are increasingly used to support public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short paper distils insights about transparency on the ground from interviews with 27 such actors, largely public servants and relevant contractors, across 5 OECD countries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1706.09249 [cs]},
	author = {Veale, Michael},
	month = nov,
	year = {2018},
	note = {arXiv: 1706.09249},
}

@inproceedings{ustun_optimized_2017,
	address = {Halifax NS Canada},
	title = {Optimized {Risk} {Scores}},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098161},
	doi = {10.1145/3097983.3098161},
	abstract = {Risk scores are simple classi cation models that let users quickly assess risk by adding, subtracting, and multiplying a few small numbers. Such models are widely used in healthcare and criminal justice, but are o en built ad hoc. In this paper, we present a principled approach to learn risk scores that are fully optimized for feature selection, integer coe cients, and operational constraints. We formulate the risk score problem as a mixed integer nonlinear program, and present a new cu ing plane algorithm to e ciently recover its optimal solution. Our approach can t optimized risk scores in a way that scales linearly with the sample size of a dataset, provides a proof of optimality, and obeys complex constraints without parameter tuning. We illustrate these bene ts through an extensive set of numerical experiments, and an application where we build a customized risk score for ICU seizure prediction.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ustun, Berk and Rudin, Cynthia},
	month = aug,
	year = {2017},
	pages = {1125--1134},
}

@article{skirpan_authority_2017,
	title = {The {Authority} of "{Fair}" in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1706.09976},
	abstract = {In this paper, we argue for the adoption of a normative definition of fairness within the machine learning community. After characterizing this definition, we review the current literature of Fair ML in light of its implications. We end by suggesting ways to incorporate a broader community and generate further debate around how to decide what is fair in ML.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1706.09976 [cs]},
	author = {Skirpan, Michael and Gorelick, Micha},
	month = jul,
	year = {2017},
	note = {arXiv: 1706.09976},
}

@inproceedings{schnoebelen_goal-oriented_2017,
	address = {Valencia, Spain},
	title = {Goal-{Oriented} {Design} for {Ethical} {Machine} {Learning} and {NLP}},
	url = {http://aclweb.org/anthology/W17-1611},
	doi = {10.18653/v1/W17-1611},
	abstract = {The argument made in this paper is that to act ethically in machine learning and NLP requires focusing on goals. NLP projects are often classificatory systems that deal with human subjects, which means that goals from people affected by the systems should be included. The paper takes as its core example a model that detects criminality, showing the problems of training data, categories, and outcomes. The paper is oriented to the kinds of critiques on power and the reproduction of inequality that are found in social theory, but it also includes concrete suggestions on how to put goal-oriented design into practice.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Schnoebelen, Tyler},
	year = {2017},
	pages = {88--93},
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco California USA},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {1135--1144},
}

@article{pearl_theoretical_nodate,
	title = {Theoretical {Impediments} to {Machine} {Learning}},
	abstract = {Current machine learning systems operate, almost exclusively, in a purely statistical mode, which puts severe theoretical limits on their performance. We consider the feasibility of leveraging counterfactual reasoning in machine learning tasks, and to identify areas where such reasoning could lead to major breakthroughs in machine learning applications.},
	language = {en},
	author = {Pearl, Judea},
	pages = {5},
}

@article{mullainathan_machine_nodate,
	title = {Machine {Learning} and {Development} {Policy}},
	language = {en},
	author = {Mullainathan, Sendhil},
	pages = {55},
}

@article{mohan_graphical_2019,
	title = {Graphical {Models} for {Processing} {Missing} {Data}},
	url = {http://arxiv.org/abs/1801.03583},
	abstract = {This paper reviews recent advances in missing data research using graphical models to represent multivariate dependencies. We ﬁrst examine the limitations of traditional frameworks from three diﬀerent perspectives: transparency, estimability and testability. We then show how procedures based on graphical models can overcome these limitations and provide meaningful performance guarantees even when data are Missing Not At Random (MNAR). In particular, we identify conditions that guarantee consistent estimation in broad categories of missing data problems, and derive procedures for implementing this estimation. Finally we derive testable implications for missing data models in both MAR (Missing At Random) and MNAR categories.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1801.03583 [stat]},
	author = {Mohan, Karthika and Pearl, Judea},
	month = nov,
	year = {2019},
	note = {arXiv: 1801.03583},
}

@article{misra_seeing_2016,
	title = {Seeing through the {Human} {Reporting} {Bias}: {Visual} {Classifiers} from {Noisy} {Human}-{Centric} {Labels}},
	shorttitle = {Seeing through the {Human} {Reporting} {Bias}},
	url = {http://arxiv.org/abs/1512.06974},
	abstract = {When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy “human-centric” annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classiﬁers. Such annotations do not use consistent vocabulary, and miss a signiﬁcant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting “what’s in the image” versus “what’s worth saying.” We demonstrate the algorithm’s efﬁcacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classiﬁcation and image captioning, doubling the performance of existing methods in some cases.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1512.06974 [cs]},
	author = {Misra, Ishan and Zitnick, C. Lawrence and Mitchell, Margaret and Girshick, Ross},
	month = apr,
	year = {2016},
	note = {arXiv: 1512.06974},
}

@article{menon_cost_nodate,
	title = {The {Cost} of {Fairness} in {Binary} {Classiﬁcation}},
	abstract = {Binary classiﬁers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive, e.g. race. We study the inherent tradeoﬀs in learning classiﬁers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairnessaware classiﬁers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such costsensitive fairness measures, the optimal classiﬁer is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoﬀ between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.},
	language = {en},
	author = {Menon, Aditya Krishna and Williamson, Robert C},
	pages = {12},
}

@article{lundberg_consistent_2019,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	url = {http://arxiv.org/abs/1802.03888},
	abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature’s assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique “supervised” clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1802.03888 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	month = mar,
	year = {2019},
	note = {arXiv: 1802.03888},
}

@article{liu_calibrated_2017,
	title = {Calibrated {Fairness} in {Bandits}},
	url = {http://arxiv.org/abs/1707.01875},
	abstract = {We study fairness within the stochastic, {\textbackslash}emph\{multi-armed bandit\} (MAB) decision making framework. We adapt the fairness framework of "treating similar individuals similarly" to this setting. Here, an `individual' corresponds to an arm and two arms are `similar' if they have a similar quality distribution. First, we adopt a \{{\textbackslash}em smoothness constraint\} that if two arms have a similar quality distribution then the probability of selecting each arm should be similar. In addition, we define the \{{\textbackslash}em fairness regret\}, which corresponds to the degree to which an algorithm is not calibrated, where perfect calibration requires that the probability of selecting an arm is equal to the probability with which the arm has the best quality realization. We show that a variation on Thompson sampling satisfies smooth fairness for total variation distance, and give an \${\textbackslash}tilde\{O\}((kT){\textasciicircum}\{2/3\})\$ bound on fairness regret. This complements prior work, which protects an on-average better arm from being less favored. We also explain how to extend our algorithm to the dueling bandit setting.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1707.01875 [cs]},
	author = {Liu, Yang and Radanovic, Goran and Dimitrakakis, Christos and Mandal, Debmalya and Parkes, David C.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01875},
}

@article{kern_multiaccurate_nodate,
	title = {Multiaccurate {Predictors} {Under} {Distributional} {Shifts}},
	language = {en},
	author = {Kern, Christoph},
	pages = {32},
}

@article{hebert-johnson_multicalibration_nodate,
	title = {Multicalibration: {Calibration} for the ({Computationally}-{Identifiable}) {Masses}},
	abstract = {We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identiﬁed within a speciﬁed class of computations. The speciﬁed class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.},
	language = {en},
	author = {Hébert-Johnson, Úrsula and Kim, Michael P and Reingold, Omer and Rothblum, Guy N},
	pages = {10},
}

@inproceedings{kamishima_recommendation_2018,
	title = {Recommendation {Independence}},
	url = {https://proceedings.mlr.press/v81/kamishima18a.html},
	abstract = {This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {187--201},
}

@inproceedings{ekstrand_all_2018,
	title = {All {The} {Cool} {Kids}, {How} {Do} {They} {Fit} {In}?: {Popularity} and {Demographic} {Biases} in {Recommender} {Evaluation} and {Effectiveness}},
	shorttitle = {All {The} {Cool} {Kids}, {How} {Do} {They} {Fit} {In}?},
	url = {https://proceedings.mlr.press/v81/ekstrand18b.html},
	abstract = {In the research literature, evaluations of recommender system effectiveness typically report results over a given data set, providing an aggregate measure of effectiveness over each instance (e.g. user) in the data set. Recent advances in information retrieval evaluation, however, demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes. For example, do users of different ages or genders obtain similar utility from the system, particularly if their group is a relatively small subset of the user base? We apply this consideration to recommender systems, using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy. We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains; these differences sometimes, but not always, correlate with the size of the user group in question. Demographic effects also have a complex—and likely detrimental—interaction with popularity bias, a known deficiency of recommender evaluation. These results demonstrate the need for recommender system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users, as well as the need for researchers and operators to move beyond naïve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Ekstrand, Michael D. and Tian, Mucun and Azpiazu, Ion Madrazo and Ekstrand, Jennifer D. and Anuyah, Oghenemaro and McNeill, David and Pera, Maria Soledad},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {172--186},
}

@inproceedings{binns_fairness_2018,
	title = {Fairness in {Machine} {Learning}: {Lessons} from {Political} {Philosophy}},
	shorttitle = {Fairness in {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v81/binns18a.html},
	abstract = {What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Binns, Reuben},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {149--159},
}

@inproceedings{dwork_decoupled_2018,
	title = {Decoupled {Classifiers} for {Group}-{Fair} and {Efficient} {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v81/dwork18a.html},
	abstract = {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {119--133},
}

@inproceedings{menon_cost_2018,
	title = {The cost of fairness in binary classification},
	url = {https://proceedings.mlr.press/v81/menon18a.html},
	abstract = {Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Menon, Aditya Krishna and Williamson, Robert C.},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {107--118},
}

@inproceedings{duarte_mixed_2018,
	title = {Mixed {Messages}? {The} {Limits} of {Automated} {Social} {Media} {Content} {Analysis}},
	shorttitle = {Mixed {Messages}?},
	url = {https://proceedings.mlr.press/v81/duarte18a.html},
	abstract = {Governments and companies are turning to automated tools to make sense of what people post on social media. Policymakers routinely call for social media companies to identify and take down hate speech, terrorist propaganda, harassment, “fake news” or disinformation. Other policy proposals have focused on mining social media to inform law enforcement and immigration decisions. But these proposals wrongly assume that automated technology can accomplish on a large scale the kind of nuanced analysis that humans can do on a small scale. Today’s tools for analyzing social media text have limited ability to parse the meaning of human communication or detect the intent of the speaker.  A knowledge gap exists between data scientists studying natural language processing (NLP) and policymakers advocating for wide adoption of automated social media analysis and moderation. Policymakers must understand the capabilities and limits of NLP before endorsing or adopting automated content analysis tools, particularly for making decisions that affect fundamental rights or access to government benefits. Without proper safeguards, these tools can facilitate overbroad censorship and biased enforcement of laws or terms of service.  This paper draws on existing research to explain the capabilities and limitations of text classifiers for social media posts and other online content. It is aimed at helping researchers and technical experts address the gaps in policymakers’ knowledge about what is possible with automated text analysis.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Duarte, Natasha and Llanso, Emma and Loup, Anna},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {106--106},
}

@inproceedings{madaan_analyze_2018,
	title = {Analyze, {Detect} and {Remove} {Gender} {Stereotyping} from {Bollywood} {Movies}},
	url = {https://proceedings.mlr.press/v81/madaan18a.html},
	abstract = {The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry ({\textbackslash}it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Madaan, Nishtha and Mehta, Sameep and Agrawaal, Taneea and Malhotra, Vrinda and Aggarwal, Aditi and Gupta, Yatin and Saxena, Mayank},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {92--105},
}

@inproceedings{burke_balanced_2018,
	title = {Balanced {Neighborhoods} for {Multi}-sided {Fairness} in {Recommendation}},
	url = {https://proceedings.mlr.press/v81/burke18a.html},
	abstract = {Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Burke, Robin and Sonboli, Nasim and Ordonez-Gauger, Aldo},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {202--214},
}

@inproceedings{ensign_runaway_2018,
	title = {Runaway {Feedback} {Loops} in {Predictive} {Policing}},
	url = {https://proceedings.mlr.press/v81/ensign18a.html},
	abstract = {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {160--171},
}

@inproceedings{chouldechova_case_2018,
	title = {A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
	url = {https://proceedings.mlr.press/v81/chouldechova18a.html},
	abstract = {Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {134--148},
}

@inproceedings{barabas_interventions_2018,
	title = {Interventions over {Predictions}: {Reframing} the {Ethical} {Debate} for {Actuarial} {Risk} {Assessment}},
	shorttitle = {Interventions over {Predictions}},
	url = {https://proceedings.mlr.press/v81/barabas18a.html},
	abstract = {Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Barabas, Chelsea and Virza, Madars and Dinakar, Karthik and Ito, Joichi and Zittrain, Jonathan},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {62--76},
}

@inproceedings{selbst_meaningful_2018,
	title = {“{Meaningful} {Information}” and the {Right} to {Explanation}},
	url = {https://proceedings.mlr.press/v81/selbst18a.html},
	abstract = {There is no single, neat statutory provision labeled the “right to explanation” in Europe’s new General Data Protection Regulation (GDPR). But nor is such a right illusory. Responding to two prominent papers that, in turn, conjure and critique the right to explanation in the context of automated decision-making, we advocate a return to the text of the GDPR. Articles 13–15 provide rights to “meaningful information about the logic involved” in automated decisions. This is a right to explanation, whether one uses the phrase or not. The right to explanation should be interpreted functionally, flexibly, and should, at a minimum, enable a data subject to exercise his or her rights under the GDPR and human rights law.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Selbst, Andrew and Powles, Julia},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {48--48},
}

@inproceedings{datta_discrimination_2018,
	title = {Discrimination in {Online} {Advertising}: {A} {Multidisciplinary} {Inquiry}},
	shorttitle = {Discrimination in {Online} {Advertising}},
	url = {https://proceedings.mlr.press/v81/datta18a.html},
	abstract = {We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Datta, Amit and Datta, Anupam and Makagon, Jael and Mulligan, Deirdre K. and Tschantz, Michael Carl},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {20--34},
}

@inproceedings{speicher_potential_2018,
	title = {Potential for {Discrimination} in {Online} {Targeted} {Advertising}},
	url = {https://proceedings.mlr.press/v81/speicher18a.html},
	abstract = {Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious.  We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising.  We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Speicher, Till and Ali, Muhammad and Venkatadri, Giridhari and Ribeiro, Filipe Nunes and Arvanitakis, George and Benevenuto, Fabrício and Gummadi, Krishna P. and Loiseau, Patrick and Mislove, Alan},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5--19},
}

@inproceedings{hellman_keynote_2018,
	title = {Keynote 2},
	url = {https://proceedings.mlr.press/v81/hellman18a.html},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Hellman, Deborah},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {4--4},
}

@inproceedings{friedler_preface_2018,
	title = {Preface},
	url = {https://proceedings.mlr.press/v81/friedler18a.html},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Friedler, Sorelle A. and Wilson, Christo},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1--2},
}

@inproceedings{buolamwini_gender_2018,
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	shorttitle = {Gender {Shades}},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {77--91},
}

@inproceedings{phillips_interpretable_2018,
	title = {Interpretable {Active} {Learning}},
	url = {https://proceedings.mlr.press/v81/phillips18a.html},
	abstract = {Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. These explanations can also be used to generate batches based on common sources of uncertainty. These regions of common uncertainty can be useful for understanding a model’s current weaknesses.  In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model’s predictions between one subgroup and another.  Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty. We also measure how the choice of initial labeled examples effects groups over time.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Phillips, Richard and Chang, Kyu Hyun and Friedler, Sorelle A.},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {49--61},
}

@inproceedings{ekstrand_privacy_2018,
	title = {Privacy for {All}: {Ensuring} {Fair} and {Equitable} {Privacy} {Protections}},
	shorttitle = {Privacy for {All}},
	url = {https://proceedings.mlr.press/v81/ekstrand18a.html},
	abstract = {In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Ekstrand, Michael D. and Joshaghani, Rezvan and Mehrpouyan, Hoda},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {35--47},
}

@inproceedings{sweeney_keynote_2018,
	title = {Keynote 1},
	url = {https://proceedings.mlr.press/v81/sweeney18a.html},
	language = {en},
	urldate = {2021-10-17},
	booktitle = {Proceedings of the 1st {Conference} on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Sweeney, Latanya},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3--3},
}

@article{authors_formalizing_nodate,
	title = {On {Formalizing} {Fairness} in {Prediction} with {Machine} {Learning}},
	language = {en},
	author = {Authors, Anonymous},
	pages = {5},
}

@article{benthall_racial_2019,
	title = {Racial categories in machine learning},
	url = {http://arxiv.org/abs/1811.11668},
	doi = {10.1145/3287560.3287575},
	abstract = {Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled “Black” it is an ascribed political category that has consequences for social diﬀerentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classiﬁcation can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reiﬁes race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratiﬁcation, without further anchoring status categories of disadvantage.},
	language = {en},
	urldate = {2021-10-17},
	journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	author = {Benthall, Sebastian and Haynes, Bruce D.},
	month = jan,
	year = {2019},
	note = {arXiv: 1811.11668},
	pages = {289--298},
}

@article{hu_exploring_2018,
	title = {Exploring {Stereotypes} and {Biased} {Data} with the {Crowd}},
	url = {http://arxiv.org/abs/1801.03261},
	abstract = {The goal of our research is to contribute information about how useful the crowd is at anticipating stereotypes that may be biasing a data set without a researcher's knowledge. The results of the crowd's prediction can potentially be used during data collection to help prevent the suspected stereotypes from introducing bias to the dataset. We conduct our research by asking the crowd on Amazon's Mechanical Turk (AMT) to complete two similar Human Intelligence Tasks (HITs) by suggesting stereotypes relating to their personal experience. Our analysis of these responses focuses on determining the level of diversity in the workers' suggestions and their demographics. Through this process we begin a discussion on how useful the crowd can be in tackling this difficult problem within machine learning data collection.},
	language = {en},
	urldate = {2021-10-17},
	journal = {arXiv:1801.03261 [cs]},
	author = {Hu, Zeyuan and Strout, Julia},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.03261},
}

@misc{mattu_machine_nodate,
	title = {Machine {Bias}},
	url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	abstract = {There’s software used across the country to predict future criminals. And it’s biased against blacks.},
	language = {en},
	urldate = {2021-10-17},
	journal = {ProPublica},
	author = {Mattu, Jeff Larson, Lauren Kirchner, Surya, Julia Angwin},
}

@misc{noauthor_excavating_nodate,
	title = {Excavating {AI}},
	url = {https://excavating.ai},
	abstract = {An investigation into the politics of training sets, and the fundamental problems with classifying humans.},
	language = {en-US},
	urldate = {2021-10-17},
	journal = {-},
}

@misc{noauthor_directive_nodate,
	title = {Directive 2004/9/{EC} of the {European} {Parliament} and of the {Council} of 11 {February} 2004 on the inspection and verification of good laboratory practice ({GLP})},
	shorttitle = {{EU} directive on the inspection and verification of good laboratory practice ({GLP})},
	url = {https://eur-lex.europa.eu/eli/dir/2004/9/2019-07-26},
	language = {en},
	urldate = {2021-10-17},
	note = {Doc ID: 02004L0009-20190726
Doc Sector: 0
Doc Title: Directive 2004/9/EC of the European Parliament and of the Council of 11 February 2004 on the inspection and verification of good laboratory practice (GLP) (Codified version) (Text with EEA relevance)Text with EEA relevance
Doc Type: L
Usr\_lan: en},
}

@misc{noauthor_eur-lex_nodate,
	title = {{EUR}-{Lex} - {02004L0009}-20190726 - {EN} - {EUR}-{Lex}},
	url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A02004L0009-20190726},
	language = {en},
	urldate = {2021-10-17},
	note = {Doc ID: 02004L0009-20190726
Doc Sector: 0
Doc Title: Directive 2004/9/EC of the European Parliament and of the Council of 11 February 2004 on the inspection and verification of good laboratory practice (GLP) (Codified version) (Text with EEA relevance)Text with EEA relevance
Doc Type: L
Usr\_lan: en},
}

@misc{wenzel_ethikkodex_2018,
	address = {Nürnberg},
	title = {Ethikkodex: {Kontext} und {Zielsetzung}},
	abstract = {Fachlicher Austausch},
	author = {Wenzel, Ulrich},
	month = jun,
	year = {2018},
}

@misc{noauthor_comparison_2018,
	title = {Comparison {Chart} of {FDA} and {EPA} {Good} {Laboratory} {Practice} ({GLP}) {Regulations} and the {OECD} {Principles} of {GLP}},
	url = {https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/fda-bioresearch-monitoring-information/comparison-chart-fda-and-epa-good-laboratory-practice-glp-regulations-and-oecd-principles-glp},
	abstract = {Comparison Chart of FDA and EPA Good Laboratory Practice (GLP) Regulations and the OECD Principles of GLP},
	language = {en},
	urldate = {2021-10-17},
	journal = {FDA},
	month = nov,
	year = {2018},
	note = {Publisher: FDA},
}

@techreport{international_committee_of_medical_journal_editors_recommendations_2019,
	title = {Recommendations for the {Conduct}, {Reporting}, {Editing}, and {Publication} of {Scholarly} {Work} in {Medical} {Journals}},
	url = {http://www.icmje.org/recommendations/},
	institution = {International Committee of Medical Journal Editors},
	author = {International Committee of Medical Journal Editors},
	month = dec,
	year = {2019},
}

@techreport{wma_-_the_world_medical_association-_wma_1964,
	address = {Helsinki, Fortaleza},
	title = {{WMA} {Declaration} of {Helsinki} – {Ethical} {Principles} for {Medical} {Research} {Involving} {Human} {Subjects}},
	url = {https://www.wma.net/policies-post/wma-declaration-of-helsinki-ethical-principles-for-medical-research-involving-human-subjects/},
	language = {en-US},
	urldate = {2021-10-17},
	institution = {WMA - The World Medical Association-},
	author = {WMA - The World Medical Association-},
	year = {1964},
}

@misc{widmer_good_2021,
	title = {Good {Data} {Science} {Practice}: {A} multi-perspective discussion on data science in the context of drug development},
	shorttitle = {Good {Data} {Science} {Practice}},
	language = {en},
	author = {Widmer, Lukas and Baillie, Mark and Dorn, Jonas and Krusche, Peter and Moloney, Conor and Ohlssen, David},
	month = jun,
	year = {2021},
}

@misc{noauthor_good_nodate,
	title = {Good {Laboratory} {Practice} ({GLP}) - {OECD}},
	url = {https://www.oecd.org/chemicalsafety/testing/good-laboratory-practiceglp.htm},
	abstract = {Data and research on test guidelines including chemical testing and assessment, chemical safety, animal welfare, endocrine disrupters, good laboratory practice (GLP), Mutual Acceptance of Data (MAD)., Read the countries' response to address the challenges posed by COVID-19 to GLP test facilities.},
	urldate = {2021-10-17},
}

@misc{noauthor_good_2021,
	title = {Good laboratory practice},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Good_laboratory_practice&oldid=1028821947},
	abstract = {In the experimental (non-clinical) research arena, good laboratory practice or GLP is a quality system of management controls for research laboratories and organizations to ensure the uniformity, consistency, reliability, reproducibility, quality, and integrity of products in development for human or animal health (including pharmaceuticals) through non-clinical safety tests; from physio-chemical properties through acute to chronic toxicity tests.GLP was first introduced in New Zealand and Denmark in 1972, and later in the US in 1978 in response to the Industrial BioTest Labs scandal.  It was followed a few years later by the Organization for Economic Co-operation and Development (OECD) Principles of GLP in 1992; the OECD has since helped promulgate GLP to many countries.
GLP applies to non-clinical studies conducted for the assessment of the safety or efficacy of products in development (including pharmaceuticals) for people, animals, and the environment. GLP, a data and operational quality system, is not the same as standards for laboratory safety - appropriate gloves, glasses and clothing to handle lab materials safely. The principles of GLP aim to ensure and promote safety, consistency, high quality, and reliability of chemicals in the process of non-clinical and laboratory testing. GLP is not limited to chemicals and also applies to medical devices, food additives, food packaging, colour additives, animal food additives, other non-pharmaceutical products or ingredients, biological products, and electronic products.},
	language = {en},
	urldate = {2021-10-17},
	journal = {Wikipedia},
	month = jun,
	year = {2021},
	note = {Page Version ID: 1028821947},
}

@techreport{american_psychological_association_ethical_2017,
	title = {Ethical {Principles} of {Psychologists} and {Code} of {Conduct}},
	url = {https://www.apa.org/ethics/code/ethics-code-2017.pdf},
	urldate = {2021-10-17},
	institution = {American Psychological Association},
	author = {American Psychological Association},
	month = jan,
	year = {2017},
}

@techreport{american_association_of_public_opinion_research_code_2015,
	title = {The {Code} of {Professional} {Ethics} and {Practices}},
	institution = {American Association of Public Opinion Research},
	author = {American Association of Public Opinion Research},
	month = nov,
	year = {2015},
}

@misc{noauthor_cognitive_nodate,
	title = {The {Cognitive} {Bias} {Codex}},
	url = {https://upload.wikimedia.org/wikipedia/commons/6/65/Cognitive_bias_codex_en.svg},
	urldate = {2021-10-17},
}

@misc{noauthor_simply_nodate,
	title = {Simply {Rational} {GmbH} – {Das} {Institut} für {Entscheidung} – {Simply} {Rational} {GmbH} – {Das} {Institut} für {Entscheidung}. {Empowering} {People} to {Make} {Better} {Decisions}},
	url = {https://update.simplyrational.de/},
	language = {de-DE},
	urldate = {2021-10-17},
}

@misc{noauthor_gerd_nodate,
	title = {Gerd {Gigerenzer}},
	url = {https://www.mpib-berlin.mpg.de/mitarbeiter/gerd-gigerenzer},
	language = {de},
	urldate = {2021-10-17},
}

@misc{noauthor_cynefin-framework_2021,
	title = {Cynefin-{Framework}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://de.wikipedia.org/w/index.php?title=Cynefin-Framework&oldid=213756681},
	abstract = {Das Cynefin-Framework [kə'nɛvɪn] ist ein Wissensmanagement-Modell mit der Aufgabe Probleme, Situationen und Systeme zu beschreiben. Das Modell liefert eine Typologie von Kontexten, die einen Anhaltspunkt bieten, welche Art von Erklärungen oder Lösungen zutreffen könnten.
Cynefin ist ein walisisches Wort, das üblicherweise im Deutschen mit 'Lebensraum' oder 'Platz' übersetzt wird, obwohl diese Übersetzung nicht seine volle Bedeutung vermitteln kann. Eine vollständige Übersetzung des Wortes würde aussagen, dass wir alle mehrere Vergangenheiten haben, derer wir nur teilweise bewusst sein können: kulturelle, religiöse, geographische, stammesgeschichtliche usw.
Der Begriff wurde von dem walisischen Gelehrten Dave Snowden gewählt, um die evolutionäre Natur komplexer Systeme zu veranschaulichen, einschließlich ihrer inhärenten Unsicherheit. Der Name ist eine Erinnerung daran, dass alle menschlichen Interaktionen stark von unseren Erfahrungen beeinflusst und häufig ganz davon bestimmt sind, sowohl durch den direkten Einfluss der persönlichen Erfahrung als auch durch kollektive Erfahrung wie Geschichten oder Musik.
Das Cynefin-Framework stützt sich auf Forschungen aus der Theorie komplexer adaptiver Systeme, Kognitionswissenschaft, Anthropologie und narrativer Muster sowie der evolutionären Psychologie. Es „untersucht die Beziehung zwischen Mensch, Erfahrung und Kontext“ und schlägt neue Wege vor für Kommunikation, Entscheidungsfindung, Richtlinienfindung und Wissensmanagement in einem komplexen sozialen Umfeld.},
	language = {de},
	urldate = {2021-10-17},
	journal = {Wikipedia},
	month = jul,
	year = {2021},
	note = {Page Version ID: 213756681},
}

@misc{karwa_differential_2017,
	title = {Differential {Privacy} and {Statistical} {Inference}: {A} statistician's perspective},
	author = {Karwa, Vishesh},
	month = may,
	year = {2017},
}

@article{oberski_differential_2020,
	title = {Differential {Privacy} and {Social} {Science}: {An} {Urgent} {Puzzle}},
	volume = {2},
	shorttitle = {Differential {Privacy} and {Social} {Science}},
	url = {https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/3},
	doi = {10.1162/99608f92.63a22079},
	abstract = {Accessing and combining large amounts of data is important for quantitative social scientists, but increasing amounts of data also increase privacy risks. To mitigate these risks, important players in official statistics, academia, and business see a solution in the concept of differential privacy. In this opinion piece, we ask how differential privacy can benefit from social-scientific insights, and, conversely, how differential privacy is likely to transform social science. First, we put differential privacy in the larger context of social science. We argue that the discussion on implementing differential privacy has been clouded by incompatible subjective beliefs about risk, each perspective having merit for different data types. Moreover, we point out existing social-scientific insights that suggest limitations to the premises of differential privacy as a data protection approach. Second, we examine the likely consequences for social science if differential privacy is widely implemented. Clearly, workflows must change, and common social science data collection will become more costly. However, in addition to data protection, differential privacy may bring other positive side effects. These could solve some issues social scientists currently struggle with, such as p-hacking, data peeking, or overfitting; after all, differential privacy is basically a robust method to analyze data. We conclude that, in the discussion around privacy risks and data protection, a large number of disciplines must band together to solve this urgent puzzle of our time, including social science, computer science, ethics, law, and statistics, as well as public and private policy.},
	language = {en},
	number = {1},
	urldate = {2021-10-17},
	journal = {Harvard Data Science Review},
	author = {Oberski, Daniel L. and Kreuter, Frauke},
	month = jan,
	year = {2020},
}

@article{dorazio_differential_2015,
	title = {Differential {Privacy} for {Social} {Science} {Inference}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=2676160},
	doi = {10.2139/ssrn.2676160},
	language = {en},
	urldate = {2021-10-17},
	journal = {SSRN Electronic Journal},
	author = {D'Orazio, Vito and Honaker, James and King, Gary},
	year = {2015},
}

@article{wood_differential_2018,
	title = {Differential {Privacy}: {A} {Primer} for a {Non}-{Technical} {Audience}},
	issn = {1556-5068},
	shorttitle = {Differential {Privacy}},
	url = {https://www.ssrn.com/abstract=3338027},
	doi = {10.2139/ssrn.3338027},
	language = {en},
	urldate = {2021-10-17},
	journal = {SSRN Electronic Journal},
	author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
	year = {2018},
}

@techreport{bundesagentur_fur_arbeit_statistikarbeitsberichterstattung_statistische_2021,
	address = {Nürnberg},
	type = {Grundlagen: {Definitionen}},
	title = {Statistische {Geheimhaltung}: {Rechtliche} {Grundlagen} und fachliche {Regelungen} der {Statistik} der {Bundesagentur} für {Arbeit}},
	url = {https://statistik.arbeitsagentur.de/DE/Navigation/Grundlagen/Rechtsgrundlagen/Statistische-Geheimhaltung/Statistische-Geheimhaltung-Nav.html},
	language = {de},
	urldate = {2021-10-16},
	institution = {Bundesagentur für Arbeit},
	author = {Bundesagentur für Arbeit: Statistik/Arbeitsberichterstattung},
	month = oct,
	year = {2021},
}

@misc{ros_datenschutz_2006,
	title = {Datenschutz und {Datensicherheit} beim {Umgang} mit {Sozialdaten}},
	abstract = {Informationsveranstaltung für neu angesetzte Mitarbeiter/innen des IAB},
	author = {Roß, Elisabeth},
	month = oct,
	year = {2006},
}

@book{hohne_verfahren_2010,
	address = {Wiesbaden},
	series = {Statistik und {Wissenschaft}},
	title = {Verfahren zur {Anonymisierung} von {Einzeldaten}},
	isbn = {978-3-8246-0901-7 978-3-8246-0902-4},
	language = {de},
	number = {Band 16},
	publisher = {Statistisches Bundesamt},
	author = {Höhne, Jörg},
	year = {2010},
}

@article{deutsche_vereinigung_fur_datenschutz_ev_big_2018,
	title = {Big {Data} und {KI}},
	volume = {41},
	issn = {0137-776},
	url = {https://www.datenschutzverein.de/dana/archiv/},
	number = {3/2018},
	journal = {Datenschutz Nachrichten},
	author = {Deutsche Vereinigung für Datenschutz e.V.},
	year = {2018},
}

@article{deutsche_vereinigung_fur_datenschutz_ev_rote_2015,
	title = {Rote {Linien} zur {EU}-{DSGVO}},
	issn = {0137-776},
	url = {https://www.datenschutzverein.de/dana/archiv/},
	number = {3/2015},
	journal = {Datenschutz Nachrichten},
	author = {Deutsche Vereinigung für Datenschutz e.V.},
	year = {2015},
}

@article{deutsche_vereinigung_fur_datenschutz_ev_rote_2016,
	title = {Rote {Linien} zur {EU}-{DSGVO}: {Was} ist daraus geworden?},
	volume = {39},
	issn = {0137-776},
	url = {https://www.datenschutzverein.de/dana/archiv/},
	number = {2/2016},
	journal = {Datenschutz Nachrichten},
	author = {Deutsche Vereinigung für Datenschutz e.V.},
	year = {2016},
}

@techreport{hochfellner_data_2012,
	address = {Nuremberg},
	type = {{FDZ}-{Methodenreport} 06/2012},
	title = {Data protection at the {Research} {Data} {Centre}},
	abstract = {Research data of the Federal Employment Agency as well as surveys of the Institute for Employment Research are highly relevant for the scientiﬁc community and policy consulting. These data help to ﬁnd answers to various research questions regarding employment and occupational research. The legal basis for data access is mainly Section 67 of the German Social Code Book X (SGB X). Since the establishment of the Research Data Centre (FDZ) of the Federal Employment Agency (BA) in the Institute of Employment Research (IAB) social data of the BA and the IAB are accessable for researchers using standardised and transparent principles. The remainder of this paper is to discuss the trade-off between the capability of research interests and data protection as well as the satisfaction of these demands.},
	language = {en},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Hochfellner, Daniela and Müller, Dana and Schmucker, Alexandra and Roß, Elisabeth},
	year = {2012},
	pages = {26},
}

@techreport{hoogland_data_2011,
	title = {Data editing: {Detection} and correction of errors},
	url = {https://www.cbs.nl/en-gb/onze-diensten/methods/statistical-methods/throughput/throughput/data-editing-detection-and-correction-of-errors},
	language = {en},
	institution = {Statistics Netherlands},
	author = {Hoogland, Jeffrey and Loo, Mark P. J. van der and Pannekoek, Jeroen and Scholtus, Sander},
	month = oct,
	year = {2011},
	pages = {75},
}

@inproceedings{oliver_measuring_2007,
	address = {Arlington},
	title = {Measuring {Edit} {Efficiency} in the {Economic} {Directorate} of the {U}.{S}. {Census} {Bureau}},
	url = {https://nces.ed.gov/FCSM/2007research.asp},
	publisher = {Federal Committee on Statistical Methodology},
	author = {Oliver, Broderick E. and Thompson, Katherine J.},
	month = nov,
	year = {2007},
}

@inproceedings{thompson_investigation_2007,
	address = {Montreal},
	title = {Investigation of {Macro} {Editing} {Techniques} for {Outlier} {Detection} in {Survey} {Data}},
	url = {https://ww2.amstat.org/meetings/ices/2007/proceedings/ices2007-000071.pdf},
	booktitle = {Proceedings of the {Third} {International} {Conference} on {Establishment} {Surveys}},
	publisher = {American Statistical Association},
	author = {Thompson, Katherine J.},
	year = {2007},
}

@article{granquist_new_1997,
	title = {The {New} {View} on {Editing}},
	volume = {65},
	issn = {03067734},
	url = {https://www.jstor.org/stable/1403378?origin=crossref},
	doi = {10.2307/1403378},
	abstract = {Numerous evaluation and other studies show that the heavy cost of editing cannot be justified by quality improvement. It is necessary to replace the old paradigm-the more and tighter the checks and recontacts, the better the quality-by a new-focus the editing on identifying and collecting data on errors, problem areas, and error causes to provide a basis for a continuous improvement of the whole survey vehicle. The corner stone of the new view on editing is that the entire set of the query edits should be designed meticulously, be focused on errors influencing the estimates, and be targeted on existing error types which can be identified by edits, and finally that the effects of the edits should be continuously evaluated by analysis of performance measures and other diagnostics, which the process should be designed to produce. The paper is a short introduction to modern editing under the new view. It presents, also, some facts to the low efficiency of traditional editing, and explains it by giving a historical review of editing.},
	language = {en},
	number = {3},
	urldate = {2021-10-17},
	journal = {International Statistical Review},
	author = {Granquist, Leopold},
	month = dec,
	year = {1997},
	pages = {381},
}

@article{elliott_use_2007,
	title = {Use of a web-based convenience sample to supplement a probability sample},
	volume = {33},
	url = {https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X200700210498},
	abstract = {In this paper we describe a methodology for combining a convenience sample with a probability sample in order to produce an estimator with a smaller mean squared error (MSE) than estimators based on only the probability sample. We then explore the properties of the resulting composite estimator, a linear combination of the convenience and probability sample estimators with weights that are a function of bias. We discuss the estimator's properties in the context of web-based convenience sampling. Our analysis demonstrates that the use of a convenience sample to supplement a probability sample for improvements in the MSE of estimation may be practical only under limited circumstances. First, the remaining bias of the estimator based on the convenience sample must be quite small, equivalent to no more than 0.1 of the outcome's population standard deviation. For a dichotomous outcome, this implies a bias of no more than five percentage points at 50 percent prevalence and no more than three percentage points at 10 percent prevalence. Second, the probability sample should contain at least 1,000-10,000 observations for adequate estimation of the bias of the convenience sample estimator. Third, it must be inexpensive and feasible to collect at least thousands (and probably tens of thousands) of web-based convenience observations. The conclusions about the limited usefulness of convenience samples with estimator bias of more than 0.1 standard deviations also apply to direct use of estimators based on that sample.},
	number = {2},
	journal = {Survey methodology},
	author = {Elliott, Marc N and Haviland, Amelia},
	month = dec,
	year = {2007},
	pages = {211--5},
}

@article{lohr_combining_2017,
	title = {Combining {Survey} {Data} with {Other} {Data} {Sources}},
	volume = {32},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-2/Combining-Survey-Data-with-Other-Data-Sources/10.1214/16-STS584.full},
	doi = {10.1214/16-STS584},
	abstract = {Collecting data using probability samples can be expensive, and response rates for many household surveys are decreasing. The increasing availability of large data sources opens new opportunities for statisticians to use the information in survey data more efficiently by combining survey data with information from these other sources. We review some of the work done to date on statistical methods for combining information from multiple data sources, discuss the limitations and challenges for different methods that have been proposed, and describe research that is needed for combining survey estimates.},
	number = {2},
	urldate = {2021-10-16},
	journal = {Statistical Science},
	author = {Lohr, Sharon L. and Raghunathan, Trivellore E.},
	month = may,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {293--312},
}

@article{bryan_excuse_2018,
	title = {Excuse {Me}, {Do} {You} {Have} a {Moment} to {Talk} {About} {Version} {Control}?},
	volume = {72},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1399928},
	doi = {10.1080/00031305.2017.1399928},
	abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many ﬁles! There are data ﬁles, source code, ﬁgures, tables, prepared reports, and much more. Most of these ﬁles evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the ﬁnal product. This unhappy result can be avoided by repurposing tools and workﬂows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientiﬁc workﬂows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
	language = {en},
	number = {1},
	urldate = {2021-10-16},
	journal = {The American Statistician},
	author = {Bryan, Jennifer},
	month = jan,
	year = {2018},
	pages = {20--27},
}

@article{rosenbaum_modern_2020,
	title = {Modern {Algorithms} for {Matching} in {Observational} {Studies}},
	volume = {7},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-031219-041058},
	doi = {10.1146/annurev-statistics-031219-041058},
	abstract = {Using a small example as an illustration, this article reviews multivariate matching from the perspective of a working scientist who wishes to make effective use of available methods. The several goals of multivariate matching are discussed. Matching tools are reviewed, including propensity scores, covariate distances, fine balance, and related methods such as near-fine and refined balance, exact and near-exact matching, tactics addressing missing covariate values, the entire number, and checks of covariate balance. Matching structures are described, such as matching with a variable number of controls, full matching, subset matching and risk-set matching. Software packages in R are described. A brief review is given of the theory underlying propensity scores and the associated sensitivity analysis concerning an unobserved covariate omitted from the propensity score.},
	language = {en},
	number = {1},
	urldate = {2021-10-16},
	journal = {Annual Review of Statistics and Its Application},
	author = {Rosenbaum, Paul R.},
	month = mar,
	year = {2020},
	pages = {143--176},
}

@article{zhang_reject_2018,
	title = {On {Reject} and {Refine} {Options} in {Multicategory} {Classification}},
	volume = {113},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1282372},
	doi = {10.1080/01621459.2017.1282372},
	abstract = {In many real applications of statistical learning, a decision made from misclassification can be too costly to afford; in this case, a reject option, which defers the decision until further investigation is conducted, is often preferred. In recent years, there has been much development for binary classification with a reject option. Yet, little progress has been made for the multicategory case. In this article, we propose margin-based multicategory classification methods with a reject option. In addition, and more importantly, we introduce a new and unique refine option for the multicategory problem, where the class of an observation is predicted to be from a set of class labels, whose cardinality is not necessarily one. The main advantage of both options lies in their capacity of identifying error-prone observations. Moreover, the refine option can provide more constructive information for classification by effectively ruling out implausible classes. Efficient implementations have been developed for the proposed methods. On the theoretical side, we offer a novel statistical learning theory and show a fast convergence rate of the excess -risk of our methods with emphasis on diverging dimensionality and number of classes. The results can be further improved under a low noise assumption and be generalized to the excess 0-d-1 risk. Finite-sample upper bounds for the reject and reject/refine rates are also provided. A set of comprehensive simulation and real data studies has shown the usefulness of the new learning tools compared to regular multicategory classifiers. Detailed proofs of theorems and extended numerical results are included in the supplemental materials available online.},
	language = {en},
	number = {522},
	urldate = {2021-10-16},
	journal = {Journal of the American Statistical Association},
	author = {Zhang, Chong and Wang, Wenbo and Qiao, Xingye},
	month = apr,
	year = {2018},
	pages = {730--745},
}

@article{strang_categorical_1990,
	title = {Categorical {Imperatives}: {The} {Structure} of {Job} {Titles} in {California} {State} {Agencies}},
	volume = {55},
	issn = {00031224},
	shorttitle = {Categorical {Imperatives}},
	url = {http://www.jstor.org/stable/2095802?origin=crossref},
	doi = {10.2307/2095802},
	language = {en},
	number = {4},
	urldate = {2021-10-16},
	journal = {American Sociological Review},
	author = {Strang, David and Baron, James N.},
	month = aug,
	year = {1990},
	pages = {479},
}

@article{mitnik_measuring_2018,
	title = {Measuring {Social} {Class} with {Changing} {Occupational} {Classifications}: {Reliability}, {Competing} {Measurement} {Strategies}, and the 1970–1980 {U}.{S}. {Classification} {Divide}},
	volume = {50},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Measuring {Social} {Class} with {Changing} {Occupational} {Classifications}},
	url = {http://journals.sagepub.com/doi/10.1177/0049124118769084},
	doi = {10.1177/0049124118769084},
	abstract = {Periodic changes in occupational classifications make it difficult to obtain consistent measures of social class over time, potentially jeopardizing research on class-based trends. The severity of this problem depends, in part, on the measurement strategies used to address those changes. The authors propose that when a sample has been coded partly with one occupational classification and partly with another, Krippendorff’s index α be used to identify the best strategy for measuring class consistently across the two classifications and to assess the reliability of the class measure employed in the final analyses. This index can be computed regardless of the metric of the class variable; it can be used to compare measures based on different class schemes or that use different metrics; and statistical inference is straightforward, even with a complex sampling design. The authors put the index to work in conducting a case study of the effects of the switch from the 1970 to the 1980 U.S. Census Bureau Classification of Occupations on the reliability of Erikson–Goldthorpe–Portocarero class measures. Their findings indicate that measurement strategies that seem a priori equally reasonable vary substantially in terms of their reliability, and that the bulk of this variation is accounted for by the extent to which the strategies rely on subjective judgments about the relationships between occupational and class classifications. Most importantly, as long as the best-performing measurement strategies are used, the switch in occupational classifications appears to be substantially less consequential than has been previously argued. A computer program made available as a companion to the paper makes estimation of Krippendorff’s α, and statistical inference, very simple endeavors for nominal class variables.},
	language = {en},
	number = {1},
	urldate = {2021-10-16},
	journal = {Sociological Methods \& Research},
	author = {Mitnik, Pablo A. and Cumberworth, Erin},
	year = {2018},
	pages = {1--45},
}

@article{roberts_model_2016,
	title = {A {Model} of {Text} for {Experimentation} in the {Social} {Sciences}},
	volume = {111},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of documentlevel covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	language = {en},
	number = {515},
	urldate = {2021-10-16},
	journal = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	month = jul,
	year = {2016},
	pages = {988--1003},
}

@inproceedings{zadrozny_obtaining_2001,
	address = {San Francisco, CA, USA},
	series = {{ICML} '01},
	title = {Obtaining calibrated probability estimates from decision trees and naive {Bayesian} classifiers},
	isbn = {978-1-55860-778-1},
	urldate = {2021-10-16},
	booktitle = {Proceedings of the {Eighteenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Zadrozny, Bianca and Elkan, Charles},
	month = jun,
	year = {2001},
	pages = {609--616},
}

@inproceedings{minka_bayesian_2000,
	title = {Bayesian {Model} {Averaging} is not {Model} {Combination}},
	url = {https://www.microsoft.com/en-us/research/publication/bayesian-model-averaging-not-model-combination/},
	abstract = {In a recent paper, Domingos (2000) compares Bayesian model averaging (BMA) to other model combination methods on some benchmark data sets, is surprised that BMA performs worst, and suggests that BMA may be flawed. These results are actually not surprising, especially in light of an earlier paper by Domingos (1997) where it was shown that model combination works by enriching the space of hypotheses, not by approximating a Bayesian model average. And the only flaw with BMA is the belief that it is an algorithm for model combination, when it is not.},
	publisher = {Microsoft},
	author = {Minka, Tom},
	month = jul,
	year = {2000},
}

@inproceedings{domingos_bayesian_2000,
	address = {San Francisco, CA, USA},
	series = {{ICML} '00},
	title = {Bayesian {Averaging} of {Classifiers} and the {Overfitting} {Problem}},
	isbn = {978-1-55860-707-1},
	urldate = {2021-10-16},
	booktitle = {Proceedings of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Domingos, Pedro},
	month = jun,
	year = {2000},
	pages = {223--230},
}

@inproceedings{siekmann_bayes_1993,
	address = {Berlin, Heidelberg},
	title = {Bayes and pseudo-{Bayes} estimates of conditional probabilities and their reliability},
	volume = {667},
	isbn = {978-3-540-56602-1 978-3-540-47597-2},
	url = {http://link.springer.com/10.1007/3-540-56602-3_133},
	abstract = {Various ways of estimating probabilities, mainly within the Bayesian framework, are discussed. Their relevance and application to machine learning is given, and their relative performance empirically evaluated. A method of accounting for noisy data is given and also applied. The reliability of estimates is measured by a signi cance measure, which is also empirically tested. We brie y discuss the use of likelihood ratio as a signi cance measure.},
	language = {en},
	urldate = {2021-10-16},
	booktitle = {Machine {Learning}: {ECML}-93},
	publisher = {Springer Berlin Heidelberg},
	author = {Cussens, James},
	editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Brazdil, Pavel B.},
	year = {1993},
	doi = {10.1007/3-540-56602-3_133},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {136--152},
}

@book{furnkranz_foundations_2012,
	address = {Berlin, Heidelberg},
	series = {Cognitive {Technologies}},
	title = {Foundations of {Rule} {Learning}},
	isbn = {978-3-540-75196-0 978-3-540-75197-7},
	url = {http://link.springer.com/10.1007/978-3-540-75197-7},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	author = {Fürnkranz, Johannes and Gamberger, Dragan and Lavrač, Nada},
	year = {2012},
	doi = {10.1007/978-3-540-75197-7},
}

@article{dzeroski_using_1993,
	title = {Using the \textit{m}-{Estimate} in {Rule} {Induction}},
	volume = {1},
	issn = {1330-1136},
	number = {1},
	journal = {J. Comput. Inf. Technol.},
	author = {Džeroski, Sašo and Cestnik, Bojan and Petrovski, Igor},
	month = mar,
	year = {1993},
	note = {Place: HRV
Publisher: CIT},
	pages = {37--46},
}

@inproceedings{domingos_bayesian_1997,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Bayesian {Model} {Averaging} in {Rule} {Induction}},
	volume = {R1},
	url = {https://proceedings.mlr.press/r1/domingos97a.html},
	abstract = {Bayesian model averaging (BMA) can be seen as the optimal approach to any induction task. It can reduce error by accounting for model uncertainty in a principled way, and its usefulness in several areas has been empirically verified. However, few attempts to apply it to rule induction have been made. This paper reports a series of experiments designed to test the utility of BMA in this field. BMA is applied to combining multiple rule sets learned from different subsets of the training data, to combining multiple rules covering a test example, to inducing technical rules for foreign exchange trading, and to inducing conjunctive concepts. In the first two cases, BMA is observed to produce lower accuracies than the ad hoc methods it is compared with. In the last two cases, BMA is observed to typically produce the same result as simply using the best (maximum-likelihood) rule, even though averaging is performed over all possible rules in the space, the domains are highly noisy, and the samples are medium- to small-sized. In all cases, this is observed to be due to BMA’s consistent tendency to assign highly asymmetric weights to different models, even when their accuracy differs by little, with most models (often all but one) effectively having no influence on the outcome. Thus the effective number of models being averaged is much smaller for BMA than for common ad hoc methods, leading to a smaller reduction in variance. This suggests that the success of the multiple models approach to rule induction is primarily due to this variance reduction, and not to its being a closer approximation to the Bayesian ideal.},
	booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Domingos, Pedro},
	editor = {Madigan, David and Smyth, Padhraic},
	month = jan,
	year = {1997},
	pages = {157--164},
}

@article{tomczak_probabilistic_2015,
	title = {Probabilistic combination of classification rules and its application to medical diagnosis},
	volume = {101},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-015-5508-x},
	doi = {10.1007/s10994-015-5508-x},
	abstract = {Application of machine learning to medical diagnosis entails facing two major issues, namely, a necessity of learning comprehensible models and a need of coping with imbalanced data phenomenon. The ﬁrst one corresponds to a problem of implementing interpretable models, e.g., classiﬁcation rules or decision trees. The second issue represents a situation in which the number of examples from one class (e.g., healthy patients) is signiﬁcantly higher than the number of examples from the other class (e.g., ill patients). Learning algorithms which are prone to the imbalance data return biased models towards the majority class. In this paper, we propose a probabilistic combination of soft rules, which can be seen as a probabilistic version of the classiﬁcation rules, by introducing new latent random variable called conjunctive feature. The conjunctive features represent conjunctions of values of attribute variables (features) and we assume that for given conjunctive feature the object and its label (class) become independent random variables. In order to deal with the between class imbalance problem, we present a new estimator which incorporates the knowledge about data imbalanceness into hyperparameters of initial probability of objects with ﬁxed class labels. Additionally, we propose a method for aggregating sufﬁcient statistics needed to estimate probabilities in a graph-based structure to speed up computations. At the end, we carry out two experiments: (1) using benchmark datasets, (2) using medical datasets. The results are discussed and the conclusions are drawn.},
	language = {en},
	number = {1-3},
	urldate = {2021-10-16},
	journal = {Machine Learning},
	author = {Tomczak, Jakub M. and Zięba, Maciej},
	month = oct,
	year = {2015},
	pages = {105--135},
}

@incollection{siekmann_rule_1991,
	address = {Berlin, Heidelberg},
	title = {Rule induction with {CN2}: {Some} recent improvements},
	volume = {482},
	isbn = {978-3-540-53816-5 978-3-540-46308-5},
	shorttitle = {Rule induction with {CN2}},
	url = {https://link.springer.com/10.1007/BFb0017011},
	abstract = {The CN2 algorithm induces an ordered list of classification rules from examples using entropy as its search heuristic. In this short paper, we describe two improvements to this algorithm. Firstly, we present the use of the Laplacian error estimate as an alternative evaluation function and secondly, we show how unordered as well as ordered rules can be generated. We experimentally demonstrate significantly improved performances resulting from these changes, thus enhancing the usefulness of CN2 as an inductive tool. Comparisons with Quinlan's C4.5 are also made.},
	language = {en},
	urldate = {2021-10-16},
	booktitle = {Machine {Learning} — {EWSL}-91},
	publisher = {Springer Berlin Heidelberg},
	author = {Clark, Peter and Boswell, Robin},
	editor = {Siekmann, J. and Goos, G. and Hartmanis, J. and Kodratoff, Yves},
	year = {1991},
	doi = {10.1007/BFb0017011},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {151--163},
}

@inproceedings{cestnik_estimating_1990,
	address = {USA},
	series = {{ECAI}'90},
	title = {Estimating {Probabilities}: {A} {Crucial} {Task} in {Machine} {Learning}},
	booktitle = {Proceedings of the 9th {European} {Conference} on {Artificial} {Intelligence}},
	publisher = {Pitman Publishing, Inc.},
	author = {Cestnik, Bojan},
	year = {1990},
	note = {event-place: Stockholm, Sweden},
	pages = {147--149},
}

@article{datta_small_2015,
	title = {Small {Area} {Estimation} {With} {Uncertain} {Random} {Effects}},
	volume = {110},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2015.1016526},
	doi = {10.1080/01621459.2015.1016526},
	language = {en},
	number = {512},
	urldate = {2021-10-16},
	journal = {Journal of the American Statistical Association},
	author = {Datta, Gauri Sankar and Mandal, Abhyuday},
	month = oct,
	year = {2015},
	pages = {1735--1744},
}

@article{granger_preface_2005,
	title = {Preface: {Some} {Thoughts} on the {Future} of {Forecasting}},
	volume = {67},
	issn = {0305-9049, 1468-0084},
	shorttitle = {Preface},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1468-0084.2005.00138.x},
	doi = {10.1111/j.1468-0084.2005.00138.x},
	language = {en},
	number = {s1},
	urldate = {2021-10-16},
	journal = {Oxford Bulletin of Economics and Statistics},
	author = {Granger, Clive W. J.},
	month = dec,
	year = {2005},
	pages = {707--711},
}

@misc{hothorn_torsten_r_nodate,
	title = {R package: partykit},
	url = {https://cran.r-project.org/web/packages/partykit/index.html},
	author = {Hothorn, Torsten and Seibold, Heidi and Zeileis, Achim},
}

@misc{hothorn_r_nodate,
	title = {R package: party},
	url = {http://party.r-forge.r-project.org/},
	author = {Hothorn, Torsten and Hornik, Kurt and Strobl, Carolin and Zeileis, Achim},
}

@article{domingos_unifying_1996,
	title = {Unifying instance-based and rule-based induction},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00058656},
	doi = {10.1007/BF00058656},
	abstract = {Several well-developed approaches to inductive learning now exist, but each has speciﬁc limitations that are hard to overcome. Multi-strategy learning attempts to tackle this problem by combining multiple methods in one algorithm. This article describes a uniﬁcation of two widely-used empirical approaches: rule induction and instance-based learning. In the new algorithm, instances are treated as maximally speciﬁc rules, and classiﬁcation is performed using a best-match strategy. Rules are learned by gradually generalizing instances until no improvement in apparent accuracy is obtained. Theoretical analysis shows this approach to be efﬁcient. It is implemented in the RISE 3.1 system. In an extensive empirical study, RISE consistently achieves higher accuracies than state-ofthe-art representatives of both its parent approaches (PEBLS and CN2), as well as a decision tree learner (C4.5). Lesion studies show that each of RISE’s components is essential to this performance. Most signiﬁcantly, in 14 of the 30 domains studied, RISE is more accurate than the best of PEBLS and CN2, showing that a signiﬁcant synergy can be obtained by combining multiple empirical methods.},
	language = {en},
	number = {2},
	urldate = {2021-10-16},
	journal = {Machine Learning},
	author = {Domingos, Pedro},
	month = aug,
	year = {1996},
	pages = {141--168},
}

@misc{cuturi_primer_2017,
	title = {A {Primer} on {Optimal} {Transport}},
	url = {https://optimaltransport.github.io/},
	urldate = {2021-10-16},
	author = {Cuturi, Marco and Salomon, Justin},
	year = {2017},
}

@book{peyre_computational_2019,
	address = {Norwell, MA},
	title = {Computational {Optimal} {Transport}: {With} {Applications} to {Data} {Science}.},
	isbn = {978-1-68083-551-9},
	shorttitle = {Computational {Optimal} {Transport}},
	url = {https://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5750369},
	abstract = {This book will be a valuable reference for researchers and students wishing to get a thorough understanding of Computational Optimal Transport, a mathematical gem at the interface of probability, analysis and optimization.},
	language = {English},
	urldate = {2021-10-16},
	publisher = {Now Publishers},
	author = {Peyré, Gabriel and Cuturi, Marco},
	year = {2019},
	note = {OCLC: 1097974423},
}

@book{wallgren_register-based_2014,
	address = {Chichester, West Sussex ; Hoboken, NJ},
	edition = {Second edition},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Register-based statistics: statistical methods for administrative data},
	isbn = {978-1-119-94213-9},
	shorttitle = {Register-based statistics},
	publisher = {John Wiley \& Sons Inc},
	author = {Wallgren, Anders and Wallgren, Britt},
	year = {2014},
}

@book{staab_handbook_2009,
	address = {Berlin, Heidelberg},
	title = {Handbook on {Ontologies}},
	isbn = {978-3-540-70999-2 978-3-540-92673-3},
	url = {http://link.springer.com/10.1007/978-3-540-92673-3},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	editor = {Staab, Steffen and Studer, Rudi},
	year = {2009},
	doi = {10.1007/978-3-540-92673-3},
}

@book{schutt_doing_2013,
	address = {Sebastopol},
	edition = {First edition},
	title = {Doing data science},
	isbn = {978-1-4493-5865-5},
	publisher = {O'Reilly Media},
	author = {Schutt, Rachel and O'Neil, Cathy},
	year = {2013},
	note = {OCLC: ocn827841776},
}

@book{jensen_forschungsdatenmanagement_2019,
	title = {Forschungsdatenmanagement sozialwissenschaftlicher {Umfragedaten}},
	isbn = {978-3-8474-2233-4},
	url = {https://shop.budrich-academic.de/produkt/forschungsdatenmanagement-sozialwissenschaftlicher-umfragedaten},
	urldate = {2021-10-16},
	publisher = {Verlag Barbara Budrich},
	editor = {Jensen, Uwe and Netscher, Sebastian and Weller, Katrin},
	month = jan,
	year = {2019},
	doi = {10.3224/84742233},
}

@book{petkovic_security_2007,
	address = {Berlin, Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Security, {Privacy}, and {Trust} in {Modern} {Data} {Management}},
	isbn = {978-3-540-69860-9 978-3-540-69861-6},
	url = {http://link.springer.com/10.1007/978-3-540-69861-6},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	editor = {Petković, Milan and Jonker, Willem},
	year = {2007},
	doi = {10.1007/978-3-540-69861-6},
}

@book{vaisman_data_2014,
	address = {Berlin, Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Data {Warehouse} {Systems}},
	isbn = {978-3-642-54654-9 978-3-642-54655-6},
	url = {http://link.springer.com/10.1007/978-3-642-54655-6},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	author = {Vaisman, Alejandro and Zimányi, Esteban},
	year = {2014},
	doi = {10.1007/978-3-642-54655-6},
}

@book{ceri_web_2013,
	address = {Berlin, Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Web {Information} {Retrieval}},
	isbn = {978-3-642-39313-6 978-3-642-39314-3},
	url = {http://link.springer.com/10.1007/978-3-642-39314-3},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	author = {Ceri, Stefano and Bozzon, Alessandro and Brambilla, Marco and Della Valle, Emanuele and Fraternali, Piero and Quarteroni, Silvia},
	year = {2013},
	doi = {10.1007/978-3-642-39314-3},
}

@book{liu_web_2011,
	address = {Berlin, Heidelberg},
	edition = {2nd ed.},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Web {Data} {Mining}},
	isbn = {978-3-642-19459-7 978-3-642-19460-3},
	url = {http://link.springer.com/10.1007/978-3-642-19460-3},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	author = {Liu, Bing},
	year = {2011},
	doi = {10.1007/978-3-642-19460-3},
}

@book{sciore_database_2020,
	address = {Cham},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Database {Design} and {Implementation}: {Second} {Edition}},
	isbn = {978-3-030-33835-0 978-3-030-33836-7},
	shorttitle = {Database {Design} and {Implementation}},
	url = {http://link.springer.com/10.1007/978-3-030-33836-7},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer International Publishing},
	author = {Sciore, Edward},
	year = {2020},
	doi = {10.1007/978-3-030-33836-7},
}

@book{badia_sql_2020,
	address = {Cham},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {{SQL} for {Data} {Science}: {Data} {Cleaning}, {Wrangling} and {Analytics} with {Relational} {Databases}},
	isbn = {978-3-030-57591-5 978-3-030-57592-2},
	shorttitle = {{SQL} for {Data} {Science}},
	url = {http://link.springer.com/10.1007/978-3-030-57592-2},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer International Publishing},
	author = {Badia, Antonio},
	year = {2020},
	doi = {10.1007/978-3-030-57592-2},
}

@book{batini_data_2006,
	address = {Berlin ; New York},
	series = {Data-centric systems and applications},
	title = {Data quality: concepts, methodologies and techniques},
	isbn = {978-3-540-33172-8},
	shorttitle = {Data quality},
	publisher = {Springer},
	author = {Batini, Carlo and Scannapieca, Monica},
	year = {2006},
	note = {OCLC: ocm71336300},
}

@book{manning_introduction_2008,
	address = {New York},
	title = {Introduction to information retrieval},
	isbn = {978-0-521-86571-5},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	year = {2008},
	note = {OCLC: ocn190786122},
}

@book{kipker_sozialdatenschutz_2021,
	address = {Baden-Baden},
	edition = {1. Auflage},
	series = {{NomosPraxis}},
	title = {Sozialdatenschutz in der {Praxis}},
	isbn = {978-3-8487-5843-2},
	language = {ger},
	publisher = {Nomos},
	author = {Brüggemann, Sebastian},
	editor = {Kipker, Dennis-Kenji and Voskamp, Friederike},
	year = {2021},
}

@book{pyle_data_1999,
	address = {San Francisco, Calif},
	title = {Data preparation for data mining},
	isbn = {978-1-55860-529-9},
	publisher = {Morgan Kaufmann Publishers},
	author = {Pyle, Dorian},
	year = {1999},
}

@book{rao_small_2015,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Small {Area} {Estimation}},
	isbn = {978-1-118-73585-5 978-1-118-73578-7},
	shorttitle = {Small {Area} {Estimation}},
	url = {http://doi.wiley.com/10.1002/9781118735855},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc},
	author = {Rao, J.N.K. and Molina, Isabel},
	month = oct,
	year = {2015},
	doi = {10.1002/9781118735855},
}

@book{efron_large-scale_2010,
	address = {Cambridge},
	title = {Large-{Scale} {Inference}: {Empirical} {Bayes} {Methods} for {Estimation}, {Testing}, and {Prediction}},
	isbn = {978-0-511-76136-2},
	shorttitle = {Large-{Scale} {Inference}},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511761362},
	urldate = {2021-10-16},
	publisher = {Cambridge University Press},
	author = {Efron, Bradley},
	year = {2010},
	doi = {10.1017/CBO9780511761362},
}

@book{vaart_asymptotic_1998,
	edition = {1},
	title = {Asymptotic {Statistics}},
	isbn = {978-0-521-49603-2 978-0-521-78450-4 978-0-511-80225-6},
	url = {https://www.cambridge.org/core/product/identifier/9780511802256/type/book},
	urldate = {2021-10-16},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	month = oct,
	year = {1998},
	doi = {10.1017/CBO9780511802256},
}

@book{nesterov_introductory_2004,
	address = {Boston, MA},
	series = {Applied {Optimization}},
	title = {Introductory {Lectures} on {Convex} {Optimization}},
	volume = {87},
	isbn = {978-1-4613-4691-3 978-1-4419-8853-9},
	url = {http://link.springer.com/10.1007/978-1-4419-8853-9},
	urldate = {2021-10-16},
	publisher = {Springer US},
	author = {Nesterov, Yurii},
	editor = {Pardalos, Panos M. and Hearn, Donald W.},
	year = {2004},
	doi = {10.1007/978-1-4419-8853-9},
}

@book{tsybakov_introduction_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Introduction to {Nonparametric} {Estimation}},
	isbn = {978-0-387-79051-0 978-0-387-79052-7},
	url = {http://link.springer.com/10.1007/b13794},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Tsybakov, Alexandre B.},
	year = {2009},
	doi = {10.1007/b13794},
}

@book{kuhn_feature_2019,
	edition = {1},
	title = {Feature {Engineering} and {Selection}: {A} {Practical} {Approach} for {Predictive} {Models}},
	isbn = {978-1-315-10823-0},
	shorttitle = {Feature {Engineering} and {Selection}},
	url = {http://www.feat.engineering/},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Chapman and Hall/CRC},
	author = {Kuhn, Max and Johnson, Kjell},
	month = jul,
	year = {2019},
	doi = {10.1201/9781315108230},
}

@book{leskovec_mining_2020,
	address = {New York, NY},
	edition = {Third edition},
	title = {Mining of massive datasets},
	isbn = {978-1-108-47634-8},
	url = {http://www.mmds.org/},
	abstract = {"The Web, social media, mobile activity, sensors, Internet commerce, and many other modern applications provide many extremely large datasets from which information can be gleaned by data mining. This book focuses on practical algorithms that have been used to solve key problems in data mining and can be used on even the largest datasets. It begins with a discussion of the MapReduce framework and related techniques for efficient parallel programming. The tricks of locality-sensitive hashing are explained. This body of knowledge, which deserves to be more widely known, is essential when seeking similar objects in a very large collection without having to compare each pair of objects. Stream-processing algorithms for mining data that arrives too fast for exhaustive processing are also explained. The PageRank idea and related tricks for organizing the Web are covered next. Other chapters cover the problems of finding frequent itemsets and clustering, each from the point of view that the data is too large to fit in main memory. Two applications: recommendation systems and Web advertising, each vital in e-commerce, are treated in detail. Later chapters cover algorithms for analyzing social-network graphs, compressing large-scale data, and machine learning. This third edition includes new and extended coverage on decision trees, deep learning, and mining social-network graphs. Written by leading authorities in database and Web technologies, it is essential reading for students and practitioners alike"--},
	publisher = {Cambridge University Press},
	author = {Leskovec, Jurij and Rajaraman, Anand and Ullman, Jeffrey D.},
	year = {2020},
}

@book{witten_data_2011,
	address = {Burlington, MA},
	edition = {3rd ed},
	series = {Morgan {Kaufmann} series in data management systems},
	title = {Data mining: practical machine learning tools and techniques},
	isbn = {978-0-12-374856-0},
	shorttitle = {Data mining},
	url = {https://www.cs.waikato.ac.nz/ml/weka/book.html},
	publisher = {Morgan Kaufmann},
	author = {Witten, I. H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	note = {OCLC: ocn262433473},
}

@book{williams_data_2011,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Data {Mining} with {Rattle} and {R}},
	isbn = {978-1-4419-9889-7 978-1-4419-9890-3},
	url = {http://link.springer.com/10.1007/978-1-4419-9890-3},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Williams, Graham},
	year = {2011},
	doi = {10.1007/978-1-4419-9890-3},
}

@article{stigler_citation_1994,
	title = {Citation {Patterns} in the {Journals} of {Statistics} and {Probability}},
	volume = {9},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-9/issue-1/Citation-Patterns-in-the-Journals-of-Statistics-and-Probability/10.1214/ss/1177010655.full},
	doi = {10.1214/ss/1177010655},
	number = {1},
	urldate = {2021-10-16},
	journal = {Statistical Science},
	author = {Stigler, Stephen M.},
	month = feb,
	year = {1994},
}

@book{van_der_laan_targeted_2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Learning}},
	isbn = {978-1-4419-9781-4 978-1-4419-9782-1},
	url = {http://link.springer.com/10.1007/978-1-4419-9782-1},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1},
}

@book{li_behavioral_2012,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Behavioral {Research} {Data} {Analysis} with {R}},
	isbn = {978-1-4614-1237-3 978-1-4614-1238-0},
	url = {http://link.springer.com/10.1007/978-1-4614-1238-0},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Li, Yuelin and Baron, Jonathan},
	year = {2012},
	doi = {10.1007/978-1-4614-1238-0},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	url = {http://incompleteideas.net/book/the-book-2nd.html},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	url = {http://www.gaussianprocess.org/gpml/},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	note = {OCLC: ocm61285753},
}

@book{segaran_programming_2007,
	address = {Beijing ; Sebastapol [CA]},
	edition = {1st ed},
	title = {Programming collective intelligence: building smart web 2.0 applications},
	isbn = {978-0-596-52932-1},
	shorttitle = {Programming collective intelligence},
	publisher = {O'Reilly},
	author = {Segaran, Toby},
	year = {2007},
	note = {OCLC: ocn166886837},
}

@techreport{simon_vorlesungsskript_2004,
	address = {New York University},
	type = {Vorlesungsskript},
	title = {Vorlesungsskript {Regression} (enthält {Berechnung} der {Elastizität}!)},
	institution = {self-published},
	author = {Simon, Gary},
	year = {2004},
}

@techreport{schild_vorlesungsskript_nodate,
	address = {Marburg},
	title = {Vorlesungsskript {Instrumentalvariablen} und {2SLS}},
	institution = {self-published},
	author = {Schild, Karl-Heinz},
}

@book{hojsgaard_graphical_2012,
	address = {Boston, MA},
	series = {Use {R}!},
	title = {Graphical {Models} with {R}},
	isbn = {978-1-4614-2298-3 978-1-4614-2299-0},
	url = {http://link.springer.com/10.1007/978-1-4614-2299-0},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer US},
	author = {Højsgaard, Søren and Edwards, David and Lauritzen, Steffen},
	year = {2012},
	doi = {10.1007/978-1-4614-2299-0},
}

@book{hastie_statistical_2015,
	edition = {0},
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}},
	isbn = {978-0-429-17158-1},
	shorttitle = {Statistical {Learning} with {Sparsity}},
	url = {https://www.taylorfrancis.com/books/9781498712170},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Chapman and Hall/CRC},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	month = may,
	year = {2015},
	doi = {10.1201/b18401},
}

@book{daume_iii_course_2017,
	title = {A {Course} in {Machine} {Learning}},
	url = {http://ciml.info/},
	abstract = {The purpose of this book is to provide a gentle and pedagogically orga-
nized introduction to the ﬁeld. This is in contrast to most existing ma-
chine learning texts, which tend to organize things topically, rather


7

than pedagogically (an exception is Mitchell’s book1, but unfortu- 1 Mitchell 1997
nately that is getting more and more outdated). This makes sense for
researchers in the ﬁeld, but less sense for learners. A second goal of
this book is to provide a view of machine learning that focuses on
ideas and models, not on math.},
	language = {en},
	urldate = {2021-10-16},
	publisher = {self-published},
	author = {Daumé III, Hal},
	year = {2017},
}

@book{efron_computer_2016,
	edition = {1},
	title = {Computer {Age} {Statistical} {Inference}: {Algorithms}, {Evidence}, and {Data} {Science}},
	isbn = {978-1-107-14989-2 978-1-316-57653-3},
	shorttitle = {Computer {Age} {Statistical} {Inference}},
	url = {https://www.cambridge.org/core/product/identifier/9781316576533/type/book},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Cambridge University Press},
	author = {Efron, Bradley and Hastie, Trevor},
	month = jul,
	year = {2016},
	doi = {10.1017/CBO9781316576533},
}

@book{cowles_applied_2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Applied {Bayesian} {Statistics}: {With} {R} and {OpenBUGS} examples},
	volume = {98},
	isbn = {978-1-4614-5695-7 978-1-4614-5696-4},
	url = {http://link.springer.com/10.1007/978-1-4614-5696-4},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Cowles, Mary Kathryn},
	year = {2013},
	doi = {10.1007/978-1-4614-5696-4},
}

@book{perner_machine_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Machine {Learning} and {Data} {Mining} in {Pattern} {Recognition}: 8th {International} {Conference}, {MLDM} 2012, {Berlin}, {Germany}, {July} 13-20, 2012. {Proceedings}},
	volume = {7376},
	isbn = {978-3-642-31536-7 978-3-642-31537-4},
	shorttitle = {Machine {Learning} and {Data} {Mining} in {Pattern} {Recognition}},
	url = {http://link.springer.com/10.1007/978-3-642-31537-4},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer Berlin Heidelberg},
	editor = {Perner, Petra and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2012},
	doi = {10.1007/978-3-642-31537-4},
}

@book{boos_essential_2013,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Essential {Statistical} {Inference}},
	volume = {120},
	isbn = {978-1-4614-4817-4 978-1-4614-4818-1},
	url = {http://link.springer.com/10.1007/978-1-4614-4818-1},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Boos, Dennis D and Stefanski, L. A},
	year = {2013},
	doi = {10.1007/978-1-4614-4818-1},
}

@book{alpaydin_introduction_2020,
	address = {Cambridge, Massachusetts},
	edition = {Fourth edition},
	series = {Adaptive computation and machine learning series},
	title = {Introduction to machine learning, 4th edition},
	isbn = {978-0-262-04379-3},
	abstract = {"Since the third edition of this text appeared in 2014, most recent advances in machine learning, both in theory and application, are related to neural networks and deep learning. In this new edition, the author has extended the discussion of multilayer perceptrons. He has also added a new chapter on deep learning including training deep neural networks, regularizing them so they learn better, structuring them to improve learning, e.g., through convolutional layers, and their recurrent extensions with short-term memory necessary for learning sequences. There is a new section on generative adversarial networks that have found an impressive array of applications in recent years. Alpaydin has also extended the chapter on reinforcement learning to discuss the use of deep networks in reinforcement learning. There is a new section on the policy gradient method that has been used frequently in recent years with neural networks, and two additional sections on two examples of deep reinforcement learning, which both made headlines when they were announced in 2015 and 2016 respectively. One is a network that learns to play arcade video games, and the other one learns to play Go. There are also revisions in other chapters reflecting new approaches, such as embedding methods for dimensionality reduction, and multi-label classification. In response to requests from instructors, this new edition contains two new appendices on linear algebra and optimization, to remind the reader of the basics of those topics that find use in machine learning"--},
	publisher = {The MIT Press},
	author = {Alpaydin, Ethem},
	year = {2020},
}

@book{alpaydin_introduction_2010,
	address = {Cambridge, Mass},
	edition = {2nd ed},
	series = {Adaptive computation and machine learning},
	title = {Introduction to machine learning, 2nd edition with lecture slides},
	isbn = {978-0-262-01243-0},
	url = {https://www.cmpe.boun.edu.tr/~ethem/i2ml2e/},
	publisher = {MIT Press},
	author = {Alpaydin, Ethem},
	year = {2010},
	note = {OCLC: ocn317698631},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
}

@book{mohri_foundations_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Foundations of machine learning},
	isbn = {978-0-262-03940-6},
	url = {https://giving.mit.edu/taxonomy/term/162#3920880},
	publisher = {The MIT Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2018},
}

@book{schapire_boosting_2012,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning series},
	title = {Boosting: foundations and algorithms},
	isbn = {978-0-262-01718-3},
	shorttitle = {Boosting},
	url = {https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/titlepage.html},
	publisher = {MIT Press},
	author = {Schapire, Robert E. and Freund, Yoav},
	year = {2012},
}

@book{kuhn_applied_2013,
	address = {New York, NY},
	title = {Applied {Predictive} {Modeling}},
	isbn = {978-1-4614-6848-6 978-1-4614-6849-3},
	url = {http://link.springer.com/10.1007/978-1-4614-6849-3},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2013},
	doi = {10.1007/978-1-4614-6849-3},
}

@book{murty_pattern_2011,
	address = {London},
	series = {Undergraduate {Topics} in {Computer} {Science}},
	title = {Pattern {Recognition}},
	volume = {0},
	isbn = {978-0-85729-494-4 978-0-85729-495-1},
	url = {http://link.springer.com/10.1007/978-0-85729-495-1},
	urldate = {2021-10-16},
	publisher = {Springer London},
	author = {Murty, M. Narasimha and Devi, V. Susheela},
	editor = {Mackie, Ian},
	year = {2011},
	doi = {10.1007/978-0-85729-495-1},
}

@book{kearns_introduction_1994,
	address = {Cambridge, Mass},
	title = {An introduction to computational learning theory},
	isbn = {978-0-262-11193-5},
	publisher = {MIT Press},
	author = {Kearns, Michael J. and Vazirani, Umesh Virkumar},
	year = {1994},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
}

@book{murphy_probabilistic_2022,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning series},
	title = {Probabilistic machine learning: an introduction},
	isbn = {978-0-262-04682-4},
	shorttitle = {Probabilistic machine learning},
	abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"--},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2022},
}

@book{peters_elements_2017,
	address = {Cambridge, Massachuestts},
	series = {Adaptive computation and machine learning series},
	title = {Elements of causal inference: foundations and learning algorithms},
	isbn = {978-0-262-03731-0},
	shorttitle = {Elements of causal inference},
	publisher = {The MIT Press},
	author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2017},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	url = {https://www.deeplearningbook.org/},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2021-10-16},
	publisher = {Springer New York},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
}

@book{baldi_bioinformatics_2001,
	address = {Cambridge, Mass},
	edition = {2nd ed},
	series = {Adaptive computation and machine learning},
	title = {Bioinformatics: the machine learning approach},
	isbn = {978-0-262-02506-5},
	shorttitle = {Bioinformatics},
	language = {en},
	publisher = {MIT Press},
	author = {Baldi, Pierre and Brunak, Søren},
	year = {2001},
}

@book{barber_bayesian_2011,
	address = {Cambridge},
	title = {Bayesian {Reasoning} and {Machine} {Learning}},
	isbn = {978-0-511-80477-9},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511804779},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	year = {2011},
	doi = {10.1017/CBO9780511804779},
}

@book{feller_introduction_1957,
	address = {New York},
	edition = {2d ed},
	series = {A {Wiley} publication in mathematical statistics},
	title = {An introduction to probability theory and its applications},
	isbn = {978-0-471-25709-7},
	publisher = {Wiley},
	author = {Feller, William},
	year = {1957},
}

@book{morgan_counterfactuals_2014,
	address = {Cambridge},
	edition = {2},
	title = {Counterfactuals and {Causal} {Inference}: {Methods} and {Principles} for {Social} {Research}},
	isbn = {978-1-107-58799-1},
	shorttitle = {Counterfactuals and {Causal} {Inference}},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781107587991},
	urldate = {2021-10-16},
	publisher = {Cambridge University Press},
	author = {Morgan, Stephen L. and Winship, Christopher},
	year = {2014},
	doi = {10.1017/CBO9781107587991},
}

@book{russell_artificial_2021,
	address = {Hoboken},
	edition = {Fourth edition},
	series = {Pearson series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-461099-3},
	shorttitle = {Artificial intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	year = {2021},
}

@book{imbens_causal_2015,
	edition = {1},
	title = {Causal {Inference} for {Statistics}, {Social}, and {Biomedical} {Sciences}: {An} {Introduction}},
	isbn = {978-0-521-88588-1 978-1-139-02575-1},
	shorttitle = {Causal {Inference} for {Statistics}, {Social}, and {Biomedical} {Sciences}},
	url = {https://www.cambridge.org/core/product/identifier/9781139025751/type/book},
	urldate = {2021-10-16},
	publisher = {Cambridge University Press},
	author = {Imbens, Guido W. and Rubin, Donald B.},
	month = apr,
	year = {2015},
	doi = {10.1017/CBO9781139025751},
}

@book{gelman_bayesian_2014,
	address = {Boca Raton},
	title = {Bayesian {Data} {Analysis}, {Third} {Edition}},
	isbn = {978-1-4398-9820-8},
	abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
	language = {English},
	publisher = {Chapman and Hall/CRC Press},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	year = {2014},
}

@book{box_time_2008,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Time {Series} {Analysis}},
	isbn = {978-0-470-27284-8 978-1-118-61919-3},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118619193},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C.},
	month = jun,
	year = {2008},
	doi = {10.1002/9781118619193},
}

@book{wiedermann_statistics_2016,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistics and {Causality}: {Methods} for {Applied} {Empirical} {Research}},
	isbn = {978-1-118-94704-3 978-1-118-94707-4},
	shorttitle = {Statistics and {Causality}},
	url = {http://doi.wiley.com/10.1002/9781118947074},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Wiedermann, Wolfgang and von Eye, Alexander},
	month = jun,
	year = {2016},
	doi = {10.1002/9781118947074},
}

@book{rosenberger_randomization_2016,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Randomization in {Clinical} {Trials}},
	isbn = {978-1-118-74211-2 978-1-118-74224-2},
	shorttitle = {Randomization in {Clinical} {Trials}},
	url = {http://doi.wiley.com/10.1002/9781118742112},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc},
	author = {Rosenberger, William F. and Lachin, John M.},
	month = jan,
	year = {2016},
	doi = {10.1002/9781118742112},
}

@book{rubinstein_simulation_2016,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Simulation and the {Monte} {Carlo} {Method}},
	isbn = {978-1-118-63198-0 978-1-118-63216-1},
	url = {http://doi.wiley.com/10.1002/9781118631980},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Rubinstein, Reuven Y. and Kroese, Dirk P.},
	month = nov,
	year = {2016},
	doi = {10.1002/9781118631980},
}

@book{rencher_methods_2012,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Methods of {Multivariate} {Analysis}},
	isbn = {978-1-118-39168-6 978-0-470-17896-6},
	shorttitle = {Methods of {Multivariate} {Analysis}},
	url = {http://doi.wiley.com/10.1002/9781118391686},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Rencher, Alvin C. and Christensen, William F.},
	month = jul,
	year = {2012},
	doi = {10.1002/9781118391686},
}

@book{thompson_sampling_2012,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Sampling},
	isbn = {978-1-118-16293-4 978-0-470-40231-3},
	shorttitle = {Sampling},
	url = {http://doi.wiley.com/10.1002/9781118162934},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Thompson, Steven K.},
	month = feb,
	year = {2012},
	doi = {10.1002/9781118162934},
}

@book{thompson_empirical_2011,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Empirical {Model} {Building}: {Data}, {Models}, and {Reality}},
	isbn = {978-0-470-46703-9 978-1-118-10965-6},
	shorttitle = {Empirical {Model} {Building}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118109656},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Thompson, James R.},
	month = oct,
	year = {2011},
	doi = {10.1002/9781118109656},
}

@book{fitzmaurice_applied_2011,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Applied {Longitudinal} {Analysis}},
	isbn = {978-1-119-51346-9 978-0-470-38027-7},
	url = {http://doi.wiley.com/10.1002/9781119513469},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Fitzmaurice, Garrett M. and Laird, Nan M. and Ware, James H.},
	month = aug,
	year = {2011},
	doi = {10.1002/9781119513469},
}

@book{bartholomew_latent_2011,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Latent {Variable} {Models} and {Factor} {Analysis}: {A} {Unified} {Approach}},
	isbn = {978-1-119-97058-3 978-0-470-97192-5},
	shorttitle = {Latent {Variable} {Models} and {Factor} {Analysis}},
	url = {http://doi.wiley.com/10.1002/9781119970583},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Bartholomew, David and Knott, Martin and Moustaki, Irini},
	month = jul,
	year = {2011},
	doi = {10.1002/9781119970583},
}

@book{everitt_cluster_2011,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Cluster {Analysis}},
	isbn = {978-0-470-97781-1 978-0-470-74991-3},
	shorttitle = {Cluster {Analysis}},
	url = {http://doi.wiley.com/10.1002/9780470977811},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Everitt, Brian S. and Landau, Sabine and Leese, Morven and Stahl, Daniel},
	month = jan,
	year = {2011},
	doi = {10.1002/9780470977811},
}

@book{bendat_random_2010,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Random {Data}: {Analysis} and {Measurement} {Procedures}},
	isbn = {978-1-118-03242-8 978-0-470-24877-5},
	shorttitle = {Random {Data}},
	url = {http://doi.wiley.com/10.1002/9781118032428},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Bendat, Julius S. and Piersol, Allan G.},
	month = jan,
	year = {2010},
	doi = {10.1002/9781118032428},
}

@book{jackman_bayesian_2009,
	address = {Chichester, UK},
	title = {Bayesian {Analysis} for the {Social} {Sciences}},
	isbn = {978-0-470-68662-1 978-0-470-01154-6},
	url = {http://doi.wiley.com/10.1002/9780470686621},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Jackman, Simon},
	month = oct,
	year = {2009},
	doi = {10.1002/9780470686621},
}

@book{pena_statistical_2021,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Learning} for {Big} {Dependent} {Data}},
	isbn = {978-1-119-41740-8},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119417408},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Peña, Daniel and S. Tsay, Ruey},
	month = may,
	year = {2021},
	doi = {10.1002/9781119417408},
}

@book{lynn_advances_2021,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Advances in {Longitudinal} {Survey} {Methodology}},
	isbn = {978-1-119-37696-5},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119376965},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	editor = {Lynn, Peter},
	month = mar,
	year = {2021},
	doi = {10.1002/9781119376965},
}

@book{wu_experiments_2021,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Experiments: {Planning}, {Analysis}, and {Optimization}},
	isbn = {978-1-119-47010-6 978-1-119-47000-7},
	shorttitle = {Experiments},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119470007},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Wu, C. F. Jeff and Hamada, Michael},
	month = feb,
	year = {2021},
	doi = {10.1002/9781119470007},
}

@book{wang_structural_2019,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Structural {Equation} {Modeling}: {Applications} {Using} {Mplus}},
	isbn = {978-1-119-42270-9 978-1-119-42273-0},
	shorttitle = {Structural {Equation} {Modeling}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119422730},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Wang, Jichuan and Wang, Xiaoqian},
	month = oct,
	year = {2019},
	doi = {10.1002/9781119422730},
}

@book{little_statistical_2019,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Analysis} with {Missing} {Data}, {Third} {Edition}},
	isbn = {978-0-470-52679-8 978-1-119-48226-0},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119482260},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Little, Roderick and Rubin, Donald},
	month = apr,
	year = {2019},
	doi = {10.1002/9781119482260},
}

@book{magnus_matrix_2019,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Matrix {Differential} {Calculus} with {Applications} in {Statistics} and {Econometrics}},
	isbn = {978-1-119-54120-2 978-1-119-54121-9},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119541219},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Magnus, Jan R.},
	month = feb,
	year = {2019},
	doi = {10.1002/9781119541219},
}

@book{saleh_theory_2019,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Theory of {Ridge} {Regression} {Estimation} with {Applications}},
	isbn = {978-1-118-64447-8 978-1-118-64461-4},
	url = {http://doi.wiley.com/10.1002/9781118644478},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Saleh, A.K. Md. Ehsanes and Arashi, Mohammad and Kibria, B.M. Golam},
	month = jan,
	year = {2019},
	doi = {10.1002/9781118644478},
}

@book{paolella_fundamental_2018,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Fundamental {Statistical} {Inference}: {A} {Computational} {Approach}},
	isbn = {978-1-119-41789-7 978-1-119-41786-6},
	shorttitle = {Fundamental {Statistical} {Inference}},
	url = {http://doi.wiley.com/10.1002/9781119417897},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons Ltd},
	editor = {Paolella, Marc S},
	month = aug,
	year = {2018},
	doi = {10.1002/9781119417897},
}

@book{clarke_robustness_2018,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Robustness {Theory} and {Application}},
	isbn = {978-1-118-66947-1 978-1-118-66930-3},
	url = {http://doi.wiley.com/10.1002/9781118669471},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Clarke, Brenton R.},
	month = jun,
	year = {2018},
	doi = {10.1002/9781118669471},
}

@book{knox_machine_2018,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Machine {Learning}: a {Concise} {Introduction}},
	isbn = {978-1-119-43919-6 978-1-119-43986-8},
	shorttitle = {Machine {Learning}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119439868},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Knox, Steven W.},
	month = apr,
	year = {2018},
	doi = {10.1002/9781119439868},
}

@book{shortle_fundamentals_2018,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Fundamentals of {Queueing} {Theory}},
	isbn = {978-1-119-45376-5 978-1-118-94352-6},
	url = {http://doi.wiley.com/10.1002/9781119453765},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Shortle, John F and Thompson, James M and Gross, Donald and Harris, Carl M},
	month = jan,
	year = {2018},
	doi = {10.1002/9781119453765},
}

@book{choudhary_measuring_2017,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Measuring {Agreement}: {Models}, {Methods}, and {Applications}},
	isbn = {978-1-118-55328-2 978-1-118-07858-7},
	shorttitle = {Measuring {Agreement}},
	url = {http://doi.wiley.com/10.1002/9781118553282},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Choudhary, Pankaj K and Nagaraja, Haikady N},
	month = aug,
	year = {2017},
	doi = {10.1002/9781118553282},
}

@book{hirotsu_advanced_2017,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Advanced {Analysis} of {Variance}},
	isbn = {978-1-119-30337-4 978-1-119-30333-6},
	url = {http://doi.wiley.com/10.1002/9781119303374},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Hirotsu, Chihiro},
	month = aug,
	year = {2017},
	doi = {10.1002/9781119303374},
}

@book{nagel_probability_2017,
	address = {Oxford, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Probability and {Conditional} {Expectation}: {Fundamentals} for the {Empirical} {Sciences}},
	isbn = {978-1-119-24349-6 978-1-119-24352-6},
	shorttitle = {Probability and {Conditional} {Expectation}},
	url = {http://doi.wiley.com/10.1002/9781119243496},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Nagel, Werner and Steyer, Rolf},
	month = apr,
	year = {2017},
	doi = {10.1002/9781119243496},
}

@book{meeker_statistical_2017,
	address = {Hoboken, NJ, USA},
	edition = {2nd ed.},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Intervals}: {A} {Guide} for {Practitioners} and {Researchers}},
	isbn = {978-1-118-59484-1 978-0-471-68717-7},
	shorttitle = {Statistical {Intervals}},
	url = {http://doi.wiley.com/10.1002/9781118594841},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Meeker, William Q. and Hahn, Gerald J. and Escobar, Luis A.},
	month = mar,
	year = {2017},
	doi = {10.1002/9781118594841},
}

@book{de_finetti_theory_2017,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Theory of {Probability}: {A} {Critical} {Introductory} {Treatment}},
	isbn = {978-1-119-28638-7 978-1-119-28637-0},
	shorttitle = {Theory of {Probability}},
	url = {http://doi.wiley.com/10.1002/9781119286387},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {de Finetti, Bruno},
	editor = {Machí, Antonio and Smith, Adrian},
	month = feb,
	year = {2017},
	doi = {10.1002/9781119286387},
}

@book{dryden_statistical_2016,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Shape} {Analysis}, with {Applications} in {R}},
	isbn = {978-1-119-07249-2 978-0-470-69962-1},
	url = {http://doi.wiley.com/10.1002/9781119072492},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Dryden, Ian L. and Mardia, Kanti V.},
	month = sep,
	year = {2016},
	doi = {10.1002/9781119072492},
}

@book{harron_methodological_2015,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Methodological {Developments} in {Data} {Linkage}},
	isbn = {978-1-119-07245-4 978-1-118-74587-8},
	shorttitle = {Methodological {Developments} in {Data} {Linkage}},
	url = {http://doi.wiley.com/10.1002/9781119072454},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {Harron, Katie and Goldstein, Harvey and Dibben, Chris},
	month = nov,
	year = {2015},
	doi = {10.1002/9781119072454},
}

@book{huber_data_2011,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Data {Analysis}: {What} {Can} {Be} {Learned} {From} the {Past} 50 {Years}},
	isbn = {978-1-118-01064-8 978-1-118-01825-5},
	shorttitle = {Data {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118018255},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {Huber, Peter J.},
	month = apr,
	year = {2011},
	doi = {10.1002/9781118018255},
}

@book{van_belle_statistical_2008,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Rules} of {Thumb}, {Second} {Edition}},
	isbn = {978-0-470-37796-3 978-0-470-14448-0},
	url = {http://doi.wiley.com/10.1002/9780470377963},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {van Belle, Gerald},
	month = aug,
	year = {2008},
	doi = {10.1002/9780470377963},
}

@book{mclachlan_em_2008,
	address = {Hoboken, NJ, USA},
	edition = {2nd ed.},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {The {EM} {Algorithm} and {Extensions}},
	isbn = {978-0-470-19161-3 978-0-471-20170-0},
	url = {http://doi.wiley.com/10.1002/9780470191613},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {McLachlan, Geoffrey J. and Krishnan, Thriyambakam},
	month = feb,
	year = {2008},
	doi = {10.1002/9780470191613},
}

@book{ryan_modern_2007,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Modern {Experimental} {Design}},
	isbn = {978-0-470-07435-0 978-0-471-21077-1},
	shorttitle = {Modern {Experimental} {Design}},
	url = {http://doi.wiley.com/10.1002/0470074353},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Ryan, Thomas P.},
	month = jan,
	year = {2007},
	doi = {10.1002/0470074353},
}

@book{box_response_2007,
	address = {Hoboken, NJ, USA},
	title = {Response {Surfaces}, {Mixtures}, and {Ridge} {Analyses}},
	isbn = {978-0-470-07276-9 978-0-470-05357-7},
	url = {http://doi.wiley.com/10.1002/0470072768},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Box, George E. P. and Draper, Norman R.},
	month = mar,
	year = {2007},
	doi = {10.1002/0470072768},
}

@book{chatterjee_regression_2006,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Regression {Analysis} by {Example}},
	isbn = {978-0-470-05546-5 978-0-471-74696-6},
	shorttitle = {Regression {Analysis} by {Example}},
	url = {http://doi.wiley.com/10.1002/0470055464},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Chatterjee, Samprit and Hadi, Ali S.},
	month = sep,
	year = {2006},
	doi = {10.1002/0470055464},
}

@book{johnson_univariate_2005,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Univariate {Discrete} {Distributions}},
	isbn = {978-0-471-71581-8 978-0-471-27246-5},
	shorttitle = {Univariate {Discrete} {Distributions}},
	url = {http://doi.wiley.com/10.1002/0471715816},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Johnson, Norman L. and Kemp, Adrienne W. and Kotz, Samuel},
	month = aug,
	year = {2005},
	doi = {10.1002/0471715816},
}

@book{congdon_bayesian_2005,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Bayesian {Models} for {Categorical} {Data}},
	isbn = {978-0-470-09239-2 978-0-470-09237-8},
	shorttitle = {Bayesian {Models} for {Categorical} {Data}},
	url = {http://doi.wiley.com/10.1002/0470092394},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Congdon, Peter},
	month = may,
	year = {2005},
	doi = {10.1002/0470092394},
}

@book{demaris_regression_2004,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Regression with {Social} {Data}: {Modeling} {Continuous} and {Limited} {Response} {Variables}},
	isbn = {978-0-471-22337-5 978-0-471-67756-7},
	shorttitle = {Regression with {Social} {Data}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0471677566},
	language = {en},
	urldate = {2021-10-16},
	publisher = {Wiley},
	author = {DeMaris, Alfred},
	month = aug,
	year = {2004},
	doi = {10.1002/0471677566},
}

@book{biemer_measurement_2004,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Measurement {Errors} in {Surveys}},
	isbn = {978-1-118-15038-2 978-0-471-69280-5},
	shorttitle = {Measurement {Errors} in {Surveys}},
	url = {http://doi.wiley.com/10.1002/9781118150382},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Biemer, Paul P. and Groves, Robert M. and Lyberg, Lars E. and Mathiowetz, Nancy A. and Sudman, Seymour},
	month = aug,
	year = {2004},
	doi = {10.1002/9781118150382},
}

@book{gelman_applied_2004,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Applied {Bayesian} {Modeling} and {Causal} {Inference} from {Incomplete}-{Data} {Perspectives}: {An} {Essential} {Journey} with {Donald} {Rubin}'s {Statistical} {Family}},
	isbn = {978-0-470-09045-9 978-0-470-09043-5},
	shorttitle = {Applied {Bayesian} {Modeling} and {Causal} {Inference} from {Incomplete}-{Data} {Perspectives}},
	url = {http://doi.wiley.com/10.1002/0470090456},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {Gelman, Andrew and Meng, Xiao-Li},
	month = jul,
	year = {2004},
	doi = {10.1002/0470090456},
}

@book{press_subjective_2002,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Subjective and {Objective} {Bayesian} {Statistics}},
	isbn = {978-0-470-31710-5 978-0-471-34843-6},
	url = {http://doi.wiley.com/10.1002/9780470317105},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Press, S. James},
	month = nov,
	year = {2002},
	doi = {10.1002/9780470317105},
}

@book{lawless_statistical_2002,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Models} and {Methods} for {Lifetime} {Data}},
	isbn = {978-1-118-03300-5 978-0-471-37215-8},
	shorttitle = {Statistical {Models} and {Methods} for {Lifetime} {Data}},
	url = {http://doi.wiley.com/10.1002/9781118033005},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Lawless, Jerald F.},
	month = nov,
	year = {2002},
	doi = {10.1002/9781118033005},
}

@book{khuri_advanced_2002,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Advanced {Calculus} with {Applications} in {Statistics}},
	isbn = {978-0-471-39104-3 978-0-471-39488-4},
	url = {http://doi.wiley.com/10.1002/0471394882},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Khuri, André I.},
	editor = {Balding, David J. and Bloomfield, Peter and Cressie, Noel A. C. and Fisher, Nicholas I. and Johnstone, Iain M. and Kadane, J. B. and Ryan, Louise M. and Scott, David W. and Smith, Adrian F. M. and Teugels, Jozef L.},
	month = nov,
	year = {2002},
	doi = {10.1002/0471394882},
}

@book{little_statistical_2002,
	address = {Hoboken, NJ, USA},
	title = {Statistical {Analysis} with {Missing} {Data}},
	isbn = {978-1-119-01356-3 978-0-471-18386-0},
	shorttitle = {Statistical {Analysis} with {Missing} {Data}},
	url = {http://doi.wiley.com/10.1002/9781119013563},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Little, Roderick J. A. and Rubin, Donald B.},
	month = aug,
	year = {2002},
	doi = {10.1002/9781119013563},
}

@book{rencher_methods_2002,
	address = {New York, NY, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Methods of {Multivariate} {Analysis}},
	isbn = {978-0-471-41889-4 978-0-471-27135-2},
	url = {http://doi.wiley.com/10.1002/0471271357},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Rencher, Alvin C.},
	month = feb,
	year = {2002},
	doi = {10.1002/0471271357},
}

@book{hald_history_1990,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {History of {Probability} and {Statistics} and {Their} {Applications} before 1750},
	isbn = {978-0-471-72516-9 978-0-471-50230-2},
	shorttitle = {History of {Probability} and {Statistics} and {Their} {Applications} before 1750},
	url = {http://doi.wiley.com/10.1002/0471725161},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Hald, Anders},
	month = jan,
	year = {1990},
	doi = {10.1002/0471725161},
}

@book{groves_survey_1989,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Survey {Errors} and {Survey} {Costs}},
	isbn = {978-0-471-72527-5 978-0-471-61171-4},
	shorttitle = {Survey {Errors} and {Survey} {Costs}},
	url = {http://doi.wiley.com/10.1002/0471725277},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Groves, Robert M.},
	month = jul,
	year = {1989},
	doi = {10.1002/0471725277},
}

@book{fuller_measurement_1987,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Measurement {Error} {Models}},
	isbn = {978-0-470-31666-5 978-0-471-86187-4},
	url = {http://doi.wiley.com/10.1002/9780470316665},
	language = {en},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Fuller, Wayne A.},
	month = jun,
	year = {1987},
	doi = {10.1002/9780470316665},
}

@book{berzuini_causality_2012,
	address = {Chichester, UK},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Causality: {Statistical} {Perspectives} and {Applications}},
	isbn = {978-1-119-94571-0 978-0-470-66556-5},
	shorttitle = {Causality},
	url = {http://doi.wiley.com/10.1002/9781119945710},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {Berzuini, Carlo and Dawid, Philip and Bernardinelli, Luisa},
	month = jul,
	year = {2012},
	doi = {10.1002/9781119945710},
}

@book{opp_methodologie_2014,
	address = {Wiesbaden},
	edition = {7},
	title = {Methodologie der {Sozialwissenschaften}},
	isbn = {978-3-658-01910-5 978-3-658-01911-2},
	url = {http://link.springer.com/10.1007/978-3-658-01911-2},
	language = {de},
	urldate = {2021-10-16},
	publisher = {Springer VS},
	author = {Opp, Karl-Dieter},
	year = {2014},
	doi = {10.1007/978-3-658-01911-2},
}

@misc{noauthor_browsevignettes_nodate,
	title = {{browseVignettes}: {List} {Vignettes} in an {HTML} {Browser}},
	shorttitle = {{browseVignettes}},
	url = {https://rdrr.io/r/utils/browseVignettes.html},
	abstract = {List available vignettes in an HTML browser with links to PDF,
LaTeX/noweb source, and (tangled) R code (if available).},
	language = {en},
	urldate = {2021-10-16},
}

@misc{noauthor_cran_nodate,
	title = {{CRAN} {Task} {Views}},
	url = {https://cran.r-project.org/web/views/},
	urldate = {2021-10-16},
}

@misc{schenck_resources_2019,
	title = {Resources on {Robust} {Regression}: in general, in {R}, in {STATA}.},
	author = {Schenck, Patrick},
	month = apr,
	year = {2019},
}

@misc{jann_robust_2017,
	address = {London},
	title = {Robust {Statistics} in {Stata}},
	url = {https://www.stata.com/meeting/uk17/slides/uk17_Jann2.pdf},
	language = {en},
	urldate = {2021-10-15},
	author = {Jann, Ben},
	month = sep,
	year = {2017},
}

@book{rousseeuw_robust_1987,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Robust {Regression} and {Outlier} {Detection}},
	isbn = {978-0-471-85233-9 978-0-471-72538-1},
	shorttitle = {Robust {Regression} and {Outlier} {Detection}},
	url = {http://doi.wiley.com/10.1002/0471725382},
	urldate = {2021-10-16},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Rousseeuw, Peter J. and Leroy, Annick M.},
	month = oct,
	year = {1987},
	doi = {10.1002/0471725382},
}

@article{verardi_robust_2009,
	title = {Robust {Regression} in {Stata}},
	volume = {9},
	issn = {1536-867X, 1536-8734},
	url = {http://journals.sagepub.com/doi/10.1177/1536867X0900900306},
	doi = {10.1177/1536867X0900900306},
	abstract = {In regression analysis, the presence of outliers in the dataset can strongly distort the classical least-squares estimator and lead to unreliable results. To deal with this, several robust-to-outliers methods have been proposed in the statistical literature. In Stata, some of these methods are available through the rreg and qreg commands. Unfortunately, these methods resist only some specific types of outliers and turn out to be ineffective under alternative scenarios. In this article, we present more effective robust estimators that we implemented in Stata. We also present a graphical tool that recognizes the type of detected outliers.},
	language = {en},
	number = {3},
	urldate = {2021-10-15},
	journal = {The Stata Journal: Promoting communications on statistics and Stata},
	author = {Verardi, Vincenzo and Croux, Christophe},
	month = sep,
	year = {2009},
	pages = {439--453},
}

@book{king_designing_1994,
	address = {Princeton, N.J},
	title = {Designing social inquiry: scientific inference in qualitative research},
	isbn = {978-0-691-03470-6 978-0-691-03471-3},
	shorttitle = {Designing social inquiry},
	language = {en},
	publisher = {Princeton University Press},
	author = {King, Gary and Keohane, Robert O. and Verba, Sidney},
	year = {1994},
}

@book{kennedy_guide_2008,
	address = {Malden, MA},
	edition = {6th ed},
	title = {A guide to econometrics},
	isbn = {978-1-4051-8258-4 978-1-4051-8257-7},
	publisher = {Blackwell Pub},
	author = {Kennedy, Peter},
	year = {2008},
	note = {OCLC: ocn173299317},
}

@book{de_veaux_stats_2019,
	address = {Place of publication not identified},
	title = {{STATS}: data and models, books a la carte edition.},
	isbn = {978-0-13-516383-2},
	shorttitle = {{STATS}},
	language = {English},
	publisher = {PRENTICE HALL},
	author = {De Veaux, RICHARD D and Velleman and Bock},
	year = {2019},
	note = {OCLC: 1090754578},
}

@book{salganik_bit_2018,
	address = {Princeton},
	title = {Bit by bit: social research in the digital age},
	isbn = {978-0-691-15864-8 978-0-691-19610-7},
	shorttitle = {Bit by bit},
	url = {https://www.bitbybitbook.com/en/1st-ed/preface/},
	abstract = {An innovative and accessible guide to doing social research in the digital age. In just the past several years, we have witnessed the birth and rapid spread of social media, mobile phones, and numerous other digital marvels. In addition to changing how we live, these tools enable us to collect and process data about human behavior on a scale never before imaginable, offering entirely new approaches to core questions about social behavior. Bit by Bit is the key to unlocking these powerful methods-a landmark book that will fundamentally change how the next generation of social scientists and data scientists explores the world around us. Bit by Bit is the essential guide to mastering the key principles of doing social research in this fast-evolving digital age. In this comprehensive yet accessible book, Matthew Salganik explains how the digital revolution is transforming how social scientists observe behavior, ask questions, run experiments, and engage in mass collaborations. He provides a wealth of real-world examples throughout, and also lays out a principles-based approach to handling ethical challenges in the era of social media. Bit by Bit is an invaluable resource for social scientists who want to harness the research potential of big data and a must-read for data scientists interested in applying the lessons of social science to tomorrow's technologies},
	publisher = {Princeton University Press},
	author = {Salganik, Matthew J.},
	year = {2018},
	note = {OCLC: on1012406622},
}

@misc{rickert_r_nodate,
	title = {An {R} "meta" book},
	url = {https://blog.revolutionanalytics.com/2014/03/an-r-meta-book.html},
	abstract = {I am a book person. I collect books on all sorts of subjects that interest me and consequently I have a fairly extensive collection of R books, many of which I find to be of great value. Nevertheless, when I am asked to recommend an R book to someone new to R I am usually flummoxed. R is growing at a fantastic rate, and people coming to R for the first time span I wide range of sophistication. And besides, owning a book is kind of personal. It is one thing to go out and buy a...},
	urldate = {2021-10-15},
	journal = {Revolutions},
	author = {Rickert, Joseph},
}

@book{aitchison_statistical_1975,
	edition = {1},
	title = {Statistical {Prediction} {Analysis}},
	isbn = {978-0-521-20692-1 978-0-521-29858-2 978-0-511-56964-7},
	url = {https://www.cambridge.org/core/product/identifier/9780511569647/type/book},
	urldate = {2021-10-15},
	publisher = {Cambridge University Press},
	author = {Aitchison, J. and Dunsmore, I. R.},
	month = sep,
	year = {1975},
	doi = {10.1017/CBO9780511569647},
}

@book{shalizi_advanced_nodate,
	title = {Advanced {Data} {Analysis} from an {Elementary} {Point} of {View}},
	url = {https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Cambridge University Press},
	author = {Shalizi, Cosma Rohilla},
}

@book{maronna_robust_2019,
	address = {Hoboken, NJ},
	edition = {Second edition},
	series = {Wiley series in probability and statistics},
	title = {Robust statistics: theory and methods (with {R})},
	isbn = {978-1-119-21467-0 978-1-119-21466-3},
	shorttitle = {Robust statistics},
	language = {en},
	publisher = {Wiley},
	author = {Maronna, Ricardo A. and Martin, R. Douglas and Yohai, Víctor J. and Salibián-Barrera, Matías},
	year = {2019},
}

@book{boker_statistikubungen_2013,
	address = {Berlin, Heidelberg},
	title = {Statistikübungen für {Bachelor}- und {Masterstudenten}},
	isbn = {978-3-642-34787-0 978-3-642-34788-7},
	url = {http://link.springer.com/10.1007/978-3-642-34788-7},
	language = {de},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Böker, Fred and Sperlich, Stefan and Zucchini, Walter},
	year = {2013},
	doi = {10.1007/978-3-642-34788-7},
}

@book{fieguth_statistical_2011,
	address = {New York, NY},
	series = {Information {Science} and {Statistics}},
	title = {Statistical {Image} {Processing} and {Multidimensional} {Modeling}},
	isbn = {978-1-4419-7293-4 978-1-4419-7294-1},
	url = {https://uwaterloo.ca/statistical-image-processing/textbooks/statistical-image-processing-and-multidimensional-modeling},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Fieguth, Paul},
	year = {2011},
	doi = {10.1007/978-1-4419-7294-1},
}

@book{bortz_forschungsmethoden_2006,
	address = {Heidelberg},
	edition = {4., überarb. Aufl., [Nachdr.]},
	series = {Springer-{Lehrbuch} {Bachelor}, {Master}},
	title = {Forschungsmethoden und {Evaluation}: für {Human}- und {Sozialwissenschaftler} ; mit 87 {Tabellen}},
	isbn = {978-3-540-33305-0},
	shorttitle = {Forschungsmethoden und {Evaluation}},
	language = {de},
	publisher = {Springer},
	author = {Bortz, Jürgen and Döring, Nicola},
	year = {2006},
}

@book{lynn_methodology_2009,
	address = {Chichester, UK},
	series = {Wiley series in survey methodology},
	title = {Methodology of longitudinal surveys},
	isbn = {978-0-470-01871-2},
	language = {en},
	publisher = {John Wiley \& Sons},
	editor = {Lynn, Peter},
	year = {2009},
	note = {OCLC: ocn298612199},
}

@book{wolf_handbuch_2010,
	address = {Wiesbaden},
	edition = {1. Aufl},
	title = {Handbuch der sozialwissenschaftlichen {Datenanalyse}},
	isbn = {978-3-531-16339-0},
	language = {de},
	publisher = {VS Verlag für Sozialwissenschaften},
	editor = {Wolf, Christof and Best, Henning},
	year = {2010},
}

@book{urban_strukturgleichungsmodellierung_2014,
	address = {Wiesbaden},
	title = {Strukturgleichungsmodellierung},
	isbn = {978-3-658-01918-1 978-3-658-01919-8},
	url = {http://link.springer.com/10.1007/978-3-658-01919-8},
	language = {de},
	urldate = {2021-10-15},
	publisher = {Springer VS},
	author = {Urban, Dieter and Mayerl, Jochen},
	year = {2014},
	doi = {10.1007/978-3-658-01919-8},
}

@book{snijders_multilevel_2012,
	address = {Los Angeles},
	edition = {2nd ed},
	title = {Multilevel analysis: an introduction to basic and advanced multilevel modeling},
	isbn = {978-1-84920-200-8},
	shorttitle = {Multilevel analysis},
	publisher = {Sage},
	author = {Snijders, T. A. B. and Bosker, R. J.},
	year = {2012},
}

@book{snijders_multilevel_1999,
	address = {London ; Thousand Oaks, Calif},
	title = {Multilevel analysis: an introduction to basic and advanced multilevel modeling},
	isbn = {978-0-7619-5889-5 978-0-7619-5890-1},
	shorttitle = {Multilevel analysis},
	publisher = {Sage Publications},
	author = {Snijders, T. A. B. and Bosker, R. J.},
	year = {1999},
}

@book{kline_principles_2011,
	address = {New York},
	edition = {3rd ed},
	series = {Methodology in the social sciences},
	title = {Principles and practice of structural equation modeling},
	isbn = {978-1-60623-877-6 978-1-60623-876-9},
	abstract = {"This bestselling text provides a balance between the technical and practical aspects of structural equation modeling (SEM). Using clear and accessible language, Rex B. Kline covers core techniques, potential pitfalls, and applications across the behavioral and social sciences. Some more advanced topics are also covered, including estimation of interactive effects of latent variables and multilevel SEM. The companion Web page offers downloadable syntax, data, and output files for each detailed example for EQS, LISREL, and Mplus, allowing readers to view the results of the same analysis generated by three different computer tools."--pub. desc},
	language = {en},
	publisher = {Guilford Press},
	author = {Kline, Rex B.},
	year = {2011},
	note = {OCLC: ocn636917444},
}

@book{hox_multilevel_2010,
	address = {New York},
	edition = {2. ed},
	series = {Quantitative methodology series},
	title = {Multilevel analysis: techniques and applications},
	isbn = {978-1-84872-846-2 978-1-84872-845-5},
	shorttitle = {Multilevel analysis},
	abstract = {Introduction to multilevel analysis -- The basic two-level regression model -- Estimation and hypothesis testing in multilevel regression -- Some important methodological and statistical issues -- Analyzing longitudinal data -- The multilevel generalized linear model for dichotomous data and proportions -- The multilevel generalized linear model for categorical and count data -- Multilevel survival analysis -- Cross-classified multilevel models -- Multivariate multilevel regression models -- The multilevel approach to meta-analysis -- Sample sizes and power analysis in multilevel regression -- Advanced issues in estimation and testing -- Multilevel factor models -- Multilevel path models -- Latent curve models},
	language = {eng},
	publisher = {Routledge, Taylor \& Francis},
	author = {Hox, Joop J.},
	year = {2010},
}

@book{sarndal_estimation_2005,
	address = {Hoboken, NJ},
	title = {Estimation in surveys with nonresponse},
	isbn = {978-0-470-01133-1},
	language = {en},
	publisher = {Wiley},
	author = {Särndal, Carl-Erik and Lundström, Sixten},
	year = {2005},
}

@book{west_bayesian_1997,
	address = {New York},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {Bayesian forecasting and dynamic models},
	isbn = {978-0-387-94725-9},
	publisher = {Springer},
	author = {West, Mike and Harrison, Jeff},
	year = {1997},
}

@book{durbin_time_2012,
	address = {Oxford},
	edition = {2nd ed},
	series = {Oxford statistical science series},
	title = {Time series analysis by state space methods},
	isbn = {978-0-19-964117-8},
	number = {38},
	publisher = {Oxford University Press},
	author = {Durbin, J. and Koopman, S. J.},
	year = {2012},
}

@book{mayerl_antwortreaktionszeiten_2008,
	address = {Wiesbaden},
	edition = {1. Aufl},
	title = {Antwortreaktionszeiten in {Survey}-{Analysen}: {Messung}, {Auswertung} und {Anwendungen}},
	isbn = {978-3-531-16175-4},
	shorttitle = {Antwortreaktionszeiten in {Survey}-{Analysen}},
	language = {de},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Mayerl, Jochen and Urban, Dieter},
	year = {2008},
}

@book{maronna_robust_2006,
	address = {Chichester (GB)},
	series = {Wiley series in probability and statistics},
	title = {Robust statistics: theory and methods},
	isbn = {978-0-470-01092-1},
	shorttitle = {Robust statistics},
	language = {en},
	publisher = {Wiley},
	author = {Maronna, Ricardo A. and Martin, R. Douglas and Yohai, Víctor J.},
	year = {2006},
}

@book{bivand_applied_2013,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Applied {Spatial} {Data} {Analysis} with {R}},
	isbn = {978-1-4614-7617-7 978-1-4614-7618-4},
	url = {http://link.springer.com/10.1007/978-1-4614-7618-4},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Bivand, Roger S. and Pebesma, Edzer and Gómez-Rubio, Virgilio},
	year = {2013},
	doi = {10.1007/978-1-4614-7618-4},
}

@book{nagarajan_bayesian_2013,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Bayesian {Networks} in {R}},
	isbn = {978-1-4614-6445-7 978-1-4614-6446-4},
	url = {http://link.springer.com/10.1007/978-1-4614-6446-4},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Nagarajan, Radhakrishnan and Scutari, Marco and Lèbre, Sophie},
	year = {2013},
	doi = {10.1007/978-1-4614-6446-4},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic graphical models},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
}

@book{cunningham_causal_2021,
	address = {New Haven ; London},
	title = {Causal inference: the mixtape},
	isbn = {978-0-300-25168-5},
	shorttitle = {Causal inference},
	url = {https://mixtape.scunning.com/},
	abstract = {An accessible and contemporary introduction to the methods for determining cause and effect in the social sciences Causal inference encompasses the tools that allow social scientists to determine what causes what. Economists--who generally can't run controlled experiments to test and validate their hypotheses--apply these tools to observational data to make connections. In a messy world, causal inference is what helps establish the causes and effects of the actions being studied, whether the impact (or lack thereof) of increases in the minimum wage on employment, the effects of early childhood education on incarceration later in life, or the introduction of malaria nets in developing regions on economic growth. Scott Cunningham introduces students and practitioners to the methods necessary to arrive at meaningful answers to the questions of causation, using a range of modeling techniques and coding instructions for both the R and Stata programming languages. - -},
	publisher = {Yale University Press},
	author = {Cunningham, Scott},
	year = {2021},
	note = {OCLC: on1146568673},
}

@book{spirtes_causation_2000,
	address = {Cambridge, Mass},
	edition = {2nd ed},
	series = {Adaptive computation and machine learning},
	title = {Causation, prediction, and search},
	isbn = {978-0-262-19440-2},
	language = {en},
	publisher = {MIT Press},
	author = {Spirtes, Peter and Glymour, Clark N. and Scheines, Richard},
	year = {2000},
}

@book{robert_monte_2004,
	address = {New York, NY},
	edition = {2nd ed.},
	series = {Springer {Texts} in {Statistics}},
	title = {Monte {Carlo} {Statistical} {Methods}},
	isbn = {978-1-4419-1939-7},
	url = {http://link.springer.com/10.1007/978-1-4757-4145-2},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Robert, Christian P. and Casella, George},
	year = {2004},
	doi = {10.1007/978-1-4757-4145-2},
}

@article{pearl_causal_2009,
	title = {Causal inference in statistics: {An} overview},
	volume = {3},
	issn = {1935-7516},
	shorttitle = {Causal inference in statistics},
	url = {https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.full},
	doi = {10.1214/09-SS057},
	abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and uniﬁes other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the eﬀects of potential interventions, (also called “causal eﬀects” or “policy evaluation”) (2) queries about probabilities of counterfactuals, (including assessment of “regret,” “attribution” or “causes of eﬀects”) and (3) queries about direct and indirect eﬀects (also known as “mediation”). Finally, the paper deﬁnes the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
	language = {en},
	number = {none},
	urldate = {2021-10-15},
	journal = {Statistics Surveys},
	author = {Pearl, Judea},
	month = jan,
	year = {2009},
}

@misc{pearl_causes_nodate,
	title = {Causes and {Counterfactuals}: {Concepts}, {Principles} and {Tools}},
	language = {en},
	author = {Pearl, Judea and Bareinboim, Elias},
}

@book{cano_six_2012,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Six {Sigma} with {R}},
	isbn = {978-1-4614-3651-5 978-1-4614-3652-2},
	url = {http://link.springer.com/10.1007/978-1-4614-3652-2},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Cano, Emilio L. and Moguerza, Javier M. and Redchuk, Andrés},
	year = {2012},
	doi = {10.1007/978-1-4614-3652-2},
}

@book{bengtsson_old_2019,
	address = {Cham},
	series = {Demographic {Research} {Monographs}},
	title = {Old and {New} {Perspectives} on {Mortality} {Forecasting}},
	isbn = {978-3-030-05074-0 978-3-030-05075-7},
	url = {http://link.springer.com/10.1007/978-3-030-05075-7},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer International Publishing},
	editor = {Bengtsson, Tommy and Keilman, Nico},
	year = {2019},
	doi = {10.1007/978-3-030-05075-7},
}

@incollection{gentle_transforms_2012,
	address = {Berlin, Heidelberg},
	title = {Transforms in {Statistics} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_8},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vidakovic, Brani},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_8},
	pages = {203--242},
}

@incollection{gentle_grammar_2012,
	address = {Berlin, Heidelberg},
	title = {The {Grammar} of {Graphics} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_13},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wilkinson, Leland},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_13},
	pages = {375--414},
}

@incollection{gentle_support_2012,
	address = {Berlin, Heidelberg},
	title = {Support {Vector} {Machines} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_30},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rieck, Konrad and Sonnenburg, Sören and Mika, Sebastian and Schäfer, Christin and Laskov, Pavel and Tax, David and Müller, Klaus-Robert},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_30},
	pages = {883--926},
}

@incollection{gentle_stochastic_2012,
	address = {Berlin, Heidelberg},
	title = {Stochastic {Optimization} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_7},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Spall, James C.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_7},
	pages = {173--201},
}

@incollection{gentle_statistical_2012,
	address = {Berlin, Heidelberg},
	title = {Statistical {User} {Interfaces} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_14},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Klinke, Sigbert},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_14},
	pages = {415--434},
}

@incollection{gentle_statistical_2012-1,
	address = {Berlin, Heidelberg},
	title = {Statistical {Databases} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_10},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Boyens, Claus and Günther, Oliver and Lenz, Hans-J.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_10},
	pages = {273--297},
}

@incollection{gentle_statistical_2012-2,
	address = {Berlin, Heidelberg},
	title = {Statistical and {Computational} {Geometry} of {Biomolecular} {Structure} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_36},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vaisman, Iosif I.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_36},
	pages = {1095--1112},
}

@incollection{gentle_smoothing_2012,
	address = {Berlin, Heidelberg},
	title = {Smoothing: {Local} {Regression} {Techniques} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	shorttitle = {Smoothing},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_20},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Loader, Catherine},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_20},
	pages = {571--596},
}

@incollection{gentle_semiparametric_2012,
	address = {Berlin, Heidelberg},
	title = {Semiparametric {Models} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_21},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Horowitz, Joel L.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_21},
	pages = {597--617},
}

@incollection{gentle_saddlepoint_2012,
	address = {Berlin, Heidelberg},
	title = {Saddlepoint {Approximations}: {A} {Review} and {Some} {New} {Applications} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	shorttitle = {Saddlepoint {Approximations}},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_32},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Broda, Simon A. and Paolella, Marc S.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_32},
	pages = {953--983},
}

@incollection{gentle_robust_2012,
	address = {Berlin, Heidelberg},
	title = {Robust {Statistics} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_25},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Davies, Laurie and Gather, Ursula},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_25},
	pages = {711--749},
}

@incollection{gentle_recursive_2012,
	address = {Berlin, Heidelberg},
	title = {Recursive {Partitioning} and {Tree}-based {Methods} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_29},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zhang, Heping},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_29},
	pages = {853--882},
}

@incollection{gentle_random_2012,
	address = {Berlin, Heidelberg},
	title = {Random {Number} {Generation} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_3},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {L’Ecuyer, Pierre},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_3},
	pages = {35--71},
}

@incollection{gentle_parallel_2012,
	address = {Berlin, Heidelberg},
	title = {Parallel {Computing} {Techniques} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_9},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Nakano, Junji},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_9},
	pages = {243--271},
}

@incollection{gentle_object_2012,
	address = {Berlin, Heidelberg},
	title = {Object {Oriented} {Computing} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_15},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Virius, Miroslav},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_15},
	pages = {435--465},
}

@incollection{gentle_numerical_2012,
	address = {Berlin, Heidelberg},
	title = {Numerical {Linear} {Algebra} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_5},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Čížková, Lenka and Čížek, Pavel},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_5},
	pages = {105--137},
}

@incollection{gentle_network_2012,
	address = {Berlin, Heidelberg},
	title = {Network {Intrusion} {Detection} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_38},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Marchette, David J.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_38},
	pages = {1139--1165},
}

@incollection{gentle_multivariate_2012,
	address = {Berlin, Heidelberg},
	title = {Multivariate {Density} {Estimation} and {Visualization} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_19},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Scott, David W.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_19},
	pages = {549--569},
}

@incollection{gentle_model_2012,
	address = {Berlin, Heidelberg},
	title = {Model {Selection} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_16},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wang, Yuedong},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_16},
	pages = {469--497},
}

@incollection{gentle_learning_2012,
	address = {Berlin, Heidelberg},
	title = {Learning {Under} {Non}-stationarity: {Covariate} {Shift} {Adaptation} by {Importance} {Weighting} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	shorttitle = {Learning {Under} {Non}-stationarity},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_31},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sugiyama, Masashi},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_31},
	pages = {927--952},
}

@incollection{gentle_interactive_2012,
	address = {Berlin, Heidelberg},
	title = {Interactive and {Dynamic} {Graphics} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_12},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Symanzik, Jürgen},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_12},
	pages = {335--373},
}

@incollection{gentle_how_2012,
	address = {Berlin, Heidelberg},
	title = {How {Computational} {Statistics} {Became} the {Backbone} of {Modern} {Data} {Science} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_1},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_1},
	pages = {3--16},
}

@incollection{gentle_heavy-tailed_2012,
	address = {Berlin, Heidelberg},
	title = {Heavy-{Tailed} {Distributions} in {VaR} {Calculations} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_34},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Misiorek, Adam and Weron, Rafał},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_34},
	pages = {1025--1059},
}

@incollection{gentle_generalized_2012,
	address = {Berlin, Heidelberg},
	title = {Generalized {Linear} {Models} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_24},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Müller, Marlene},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_24},
	pages = {681--709},
}

@incollection{gentle_functional_2012,
	address = {Berlin, Heidelberg},
	title = {Functional {Magnetic} {Resonance} {Imaging} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_37},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Eddy, William F. and McNamee, Rebecca L.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_37},
	pages = {1113--1137},
}

@incollection{gentle_econometrics_2012,
	address = {Berlin, Heidelberg},
	title = {Econometrics [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_35},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bauwens, Luc and Rombouts, Jeroen V. K.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_35},
	pages = {1061--1094},
}

@incollection{gentle_discovering_2012,
	address = {Berlin, Heidelberg},
	title = {Discovering and {Visualizing} {Relations} in {High} {Dimensional} {Data} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_11},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Inselberg, Alfred},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_11},
	pages = {299--333},
}

@incollection{gentle_dimension_2012,
	address = {Berlin, Heidelberg},
	title = {Dimension {Reduction} {Methods} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_22},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mizuta, Masahiro},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_22},
	pages = {619--644},
}

@incollection{gentle_design_2012,
	address = {Berlin, Heidelberg},
	title = {Design and {Analysis} of {Monte} {Carlo} {Experiments} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_18},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kleijnen, Jack P. C.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_18},
	pages = {529--547},
}

@incollection{gentle_data_2012,
	address = {Berlin, Heidelberg},
	title = {Data and {Knowledge} {Mining} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_28},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wilhelm, Adalbert},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_28},
	pages = {825--852},
}

@incollection{gentle_computational_2012,
	address = {Berlin, Heidelberg},
	title = {Computational {Methods} in {Survival} {Analysis} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_27},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kamakura, Toshinari},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_27},
	pages = {807--824},
}

@incollection{gentle_bootstrap_2012,
	address = {Berlin, Heidelberg},
	title = {Bootstrap and {Resampling} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_17},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mammen, Enno and Nandi, Swagata},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_17},
	pages = {499--527},
}

@incollection{gentle_bayesian_2012,
	address = {Berlin, Heidelberg},
	title = {Bayesian {Computational} {Methods} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_26},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Robert, Christian P.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_26},
	pages = {751--805},
}

@incollection{gentle_bagging_2012,
	address = {Berlin, Heidelberg},
	title = {Bagging, {Boosting} and {Ensemble} {Methods}  [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_33},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bühlmann, Peter},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_33},
	pages = {985--1022},
}

@incollection{gentle_non_2012,
	address = {Berlin, Heidelberg},
	title = {({Non}) {Linear} {Regression} {Modeling}  [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_23},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Čı́žek, Pavel},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_23},
	pages = {645--680},
}

@incollection{gentle_em_2012,
	address = {Berlin, Heidelberg},
	title = {The {EM} {Algorithm} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_6},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ng, Shu Kay and Krishnan, Thriyambakam and McLachlan, Geoffrey J.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_6},
	pages = {139--172},
}

@incollection{gentle_markov_2012,
	address = {Berlin, Heidelberg},
	title = {Markov {Chain} {Monte} {Carlo} {Technology} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_4},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chib, Siddhartha},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_4},
	pages = {73--104},
}

@incollection{gentle_basic_2012,
	address = {Berlin, Heidelberg},
	title = {Basic {Computational} {Algorithms} [{Handbook} of {Computational} {Statistics}]},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3_2},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Handbook of {Computational} {Statistics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Monahan, John F.},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3_2},
	pages = {19--33},
}

@book{gentle_handbook_2012,
	address = {Berlin, Heidelberg},
	title = {Handbook of {Computational} {Statistics}},
	isbn = {978-3-642-21550-6 978-3-642-21551-3},
	url = {http://link.springer.com/10.1007/978-3-642-21551-3},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi},
	year = {2012},
	doi = {10.1007/978-3-642-21551-3},
}

@book{thomopoulos_essentials_2013,
	address = {New York, NY},
	title = {Essentials of {Monte} {Carlo} {Simulation}},
	isbn = {978-1-4614-6021-3 978-1-4614-6022-0},
	url = {http://link.springer.com/10.1007/978-1-4614-6022-0},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Thomopoulos, Nick T.},
	year = {2013},
	doi = {10.1007/978-1-4614-6022-0},
}

@book{bernardo_bayesian_1994,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Bayesian {Theory}},
	isbn = {978-0-470-31687-0 978-0-471-92416-6},
	url = {http://doi.wiley.com/10.1002/9780470316870},
	language = {en},
	urldate = {2021-10-15},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Bernardo, Jos M. and Smith, Adrian F. M.},
	month = may,
	year = {1994},
	doi = {10.1002/9780470316870},
}

@book{ohagan_bayesian_2004,
	address = {Chichester},
	edition = {sixth edition},
	series = {Kendall's advanced theory of statistics},
	title = {Bayesian inference},
	isbn = {978-0-470-68569-3},
	language = {eng},
	number = {Volume 2B},
	publisher = {Wiley \& Sons},
	author = {O'Hagan, Anthony and Forster, Jonathan and Kendall, Maurice G.},
	year = {2004},
}

@book{berger_statistical_1993,
	address = {New York},
	edition = {2. ed., 3. corr. print},
	series = {Springer series in statistics},
	title = {Statistical decision theory and {Bayesian} analysis},
	isbn = {978-3-540-96098-0 978-0-387-96098-2},
	language = {eng},
	publisher = {Springer},
	author = {Berger, James O.},
	year = {1993},
}

@book{good_estimation_2003,
	address = {Cambridge, MA, USA},
	title = {The {Estimation} {Of} {Probabilities}: {An} {Essay} on {Modern} {Bayesian} {Methods}},
	isbn = {978-0-262-57015-2},
	shorttitle = {The {Estimation} {Of} {Probabilities}},
	url = {https://mitpress.mit.edu/books/estimation-probabilities},
	abstract = {The problem of how to estimate probabilities has interested philosophers, statisticians, actuaries, and mathematicians for a long time. It is currently of interest for automatic recognition, medical diagnosis, and artificial intelligence in general. This monograph reviews existing methods, including those that are new or have not been written up in a connected manner.},
	language = {en},
	publisher = {MIT Press},
	author = {Good, Irving John},
	month = mar,
	year = {2003},
}

@book{gelman_data_2007,
	address = {Cambridge ; New York},
	series = {Analytical methods for social research},
	title = {Data analysis using regression and multilevel/hierarchical models},
	isbn = {978-0-521-86706-1 978-0-521-68689-1},
	url = {http://www.stat.columbia.edu/~gelman/arm/},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	year = {2007},
	note = {OCLC: ocm67375137},
}

@book{everitt_introduction_2011,
	address = {New York, NY},
	series = {Use {R}!},
	title = {An {Introduction} to {Applied} {Multivariate} {Analysis} with {R}},
	isbn = {978-1-4419-9649-7 978-1-4419-9650-3},
	url = {http://link.springer.com/10.1007/978-1-4419-9650-3},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Everitt, Brian and Hothorn, Torsten},
	year = {2011},
	doi = {10.1007/978-1-4419-9650-3},
}

@book{soetaert_solving_2012,
	address = {Berlin, Heidelberg},
	series = {Use {R}!},
	title = {Solving {Differential} {Equations} in {R}},
	isbn = {978-3-642-28069-6 978-3-642-28070-2},
	url = {http://link.springer.com/10.1007/978-3-642-28070-2},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Soetaert, Karline and Cash, Jeff and Mazzia, Francesca},
	year = {2012},
	doi = {10.1007/978-3-642-28070-2},
}

@book{ligges_programmieren_2008,
	address = {Berlin, Heidelberg},
	edition = {3rd ed.},
	title = {Programmieren mit {R}},
	isbn = {978-3-540-79997-9 978-3-540-79998-6},
	url = {http://link.springer.com/10.1007/978-3-540-79998-6},
	language = {de},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Ligges, Uwe},
	year = {2008},
	doi = {10.1007/978-3-540-79998-6},
}

@book{dasgupta_probability_2011,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Probability for {Statistics} and {Machine} {Learning}},
	isbn = {978-1-4419-9633-6 978-1-4419-9634-3},
	url = {http://link.springer.com/10.1007/978-1-4419-9634-3},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {DasGupta, Anirban},
	year = {2011},
	doi = {10.1007/978-1-4419-9634-3},
}

@book{lange_optimization_2013,
	address = {New York, NY},
	edition = {2nd ed.},
	series = {Springer {Texts} in {Statistics}},
	title = {Optimization},
	volume = {95},
	isbn = {978-1-4614-5837-1 978-1-4614-5838-8},
	url = {http://link.springer.com/10.1007/978-1-4614-5838-8},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Lange, Kenneth},
	year = {2013},
	doi = {10.1007/978-1-4614-5838-8},
}

@book{nolan_xml_2014,
	address = {New York, NY},
	series = {Use {R}!},
	title = {{XML} and {Web} {Technologies} for {Data} {Sciences} with {R}},
	isbn = {978-1-4614-7899-7 978-1-4614-7900-0},
	url = {http://link.springer.com/10.1007/978-1-4614-7900-0},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Nolan, Deborah and Temple Lang, Duncan},
	year = {2014},
	doi = {10.1007/978-1-4614-7900-0},
}

@book{munzert_automated_2014,
	address = {Chichester, UK},
	title = {Automated {Data} {Collection} with {R}: {A} {Practical} {Guide} to {Web} {Scraping} and {Text} {Mining}},
	isbn = {978-1-118-83473-2 978-1-118-83481-7},
	shorttitle = {Automated {Data} {Collection} with {R}},
	url = {http://doi.wiley.com/10.1002/9781118834732},
	language = {en},
	urldate = {2021-10-15},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Munzert, Simon and Rubba, Christian and Meißner, Peter and Nyhuis, Dominic},
	month = jul,
	year = {2014},
	doi = {10.1002/9781118834732},
}

@book{golyandina_singular_2013,
	address = {Berlin, Heidelberg},
	series = {{SpringerBriefs} in {Statistics}},
	title = {Singular {Spectrum} {Analysis} for {Time} {Series}},
	isbn = {978-3-642-34912-6 978-3-642-34913-3},
	url = {http://link.springer.com/10.1007/978-3-642-34913-3},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Golyandina, Nina and Zhigljavsky, Anatoly},
	year = {2013},
	doi = {10.1007/978-3-642-34913-3},
}

@book{shumway_time_2011,
	address = {New York, NY},
	edition = {3rd ed.},
	series = {Springer {Texts} in {Statistics}},
	title = {Time {Series} {Analysis} and {Its} {Applications}},
	isbn = {978-1-4419-7864-6 978-1-4419-7865-3},
	url = {http://link.springer.com/10.1007/978-1-4419-7865-3},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Shumway, Robert H. and Stoffer, David S.},
	year = {2011},
	doi = {10.1007/978-1-4419-7865-3},
}

@book{lehmann_theory_1998,
	address = {New York},
	edition = {2nd ed},
	series = {Springer texts in statistics},
	title = {Theory of point estimation},
	isbn = {978-0-387-98502-2},
	language = {en},
	publisher = {Springer},
	author = {Lehmann, E. L. and Casella, George},
	year = {1998},
}

@book{fahrmeir_regression_2009,
	address = {Berlin, Heidelberg},
	edition = {2},
	title = {Regression},
	isbn = {978-3-642-01836-7 978-3-642-01837-4},
	url = {http://link.springer.com/10.1007/978-3-642-01837-4},
	language = {de},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan},
	year = {2009},
	doi = {10.1007/978-3-642-01837-4},
}

@book{meintrup_stochastik_2005,
	address = {Berlin Heidelberg},
	series = {Statistik und ihre {Anwendungen}},
	title = {Stochastik: {Theorie} und {Anwendungen}},
	isbn = {978-3-540-21676-6},
	shorttitle = {Stochastik},
	language = {ger},
	publisher = {Springer},
	author = {Meintrup, David and Schäffler, Stefan},
	year = {2005},
}

@book{herzog_data_2007,
	address = {New York ; London},
	title = {Data quality and record linkage techniques},
	isbn = {978-0-387-69502-0},
	language = {en},
	publisher = {Springer},
	author = {Herzog, Thomas N. and Scheuren, Fritz and Winkler, William E.},
	year = {2007},
	note = {OCLC: ocn137313060},
}

@book{geisen_usability_2017,
	address = {Cambridge, MA},
	title = {Usability testing for survey research},
	isbn = {978-0-12-803656-3},
	language = {en},
	publisher = {Elsevier},
	author = {Geisen, Emily and Bergstrom, Jennifer Romano},
	year = {2017},
	note = {OCLC: ocn956959150},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
}

@book{siebertz_statistische_2010,
	address = {Berlin, Heidelberg},
	title = {Statistische {Versuchsplanung}},
	isbn = {978-3-642-05492-1 978-3-642-05493-8},
	url = {http://link.springer.com/10.1007/978-3-642-05493-8},
	language = {de},
	urldate = {2021-10-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Siebertz, Karl and Bebber, David van and Hochkirchen, Thomas},
	year = {2010},
	doi = {10.1007/978-3-642-05493-8},
}

@book{pronzato_design_2013,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Design of {Experiments} in {Nonlinear} {Models}},
	volume = {212},
	isbn = {978-1-4614-6362-7 978-1-4614-6363-4},
	url = {http://link.springer.com/10.1007/978-1-4614-6363-4},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer New York},
	author = {Pronzato, Luc and Pázman, Andrej},
	year = {2013},
	doi = {10.1007/978-1-4614-6363-4},
}

@book{hinkelmann_design_2012,
	address = {Hoboken, N.J},
	series = {Wiley series in probability and statistics},
	title = {Design and {Analysis} of {Experiments}, {Volume} 3: {Special} {Designs} and {Applications}},
	volume = {3},
	isbn = {978-0-470-53068-9},
	url = {https://www.wiley.com/en-us/Design+and+Analysis+of+Experiments%2C+Volume+3%3A+Special+Designs+and+Applications-p-9780470530689},
	abstract = {Design and Analysis of Experiments, Volume 3: Special Designs and Applications continues building upon the philosophical foundations of experimental design by providing important, modern applications of experimental design to the many fields that utilize them. The book also presents optimal and efficient designs for practice and covers key topics in current statistical research.},
	publisher = {Wiley-Interscience},
	editor = {Hinkelmann, Klaus},
	year = {2012},
	note = {OCLC: ocn123767305},
}

@book{hinkelmann_design_1994,
	address = {New York},
	edition = {Rev. ed.},
	series = {Wiley series in probability and mathematical statistics},
	title = {Design and {Analysis} of {Experiments}, {Volume} 2: {Advanced} {Experimental} {Design}},
	volume = {2},
	isbn = {978-0-471-55178-2 978-0-471-55177-5},
	abstract = {Design and Analysis of Experiments, Volume 2 provides more detail about aspects of error control and treatment design, with emphasis on their historical development and practical significance, and the connections between them.},
	publisher = {Wiley},
	author = {Hinkelmann, Klaus and Kempthorne, Oscar},
	year = {1994},
}

@book{hinkelmann_design_2008,
	address = {Hoboken, N.J},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Design and {Analysis} of {Experiments}, {Volume} 1: {Introduction} to {Experimental} {Design}},
	volume = {1},
	isbn = {978-0-471-72756-9},
	abstract = {Design and Analysis of Experiments, Volume 1, Second Edition provides a general introduction to the philosophy, theory, and practice of designing scientific comparative experiments and also details the intricacies that are often encountered throughout the design and analysis processes. With the addition of extensive numerical examples and expanded treatment of key concepts, this book further addresses the needs of practitioners and successfully provides a solid understanding of the relationship between the quality of experimental design and the validity of conclusions.},
	publisher = {Wiley-Interscience},
	editor = {Hinkelmann, Klaus and Kempthorne, Oscar},
	year = {2008},
	note = {OCLC: ocn123767305},
}

@book{biemer_introduction_2003,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Survey} {Methodology}},
	title = {Introduction to {Survey} {Quality}},
	isbn = {978-0-471-19375-3 978-0-471-45874-6},
	url = {http://doi.wiley.com/10.1002/0471458740},
	language = {en},
	urldate = {2021-10-15},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Biemer, Paul P. and Lyberg, Lars E.},
	month = feb,
	year = {2003},
	doi = {10.1002/0471458740},
}

@book{beigi_fundamentals_2011,
	address = {Boston, MA},
	title = {Fundamentals of {Speaker} {Recognition}},
	isbn = {978-0-387-77591-3 978-0-387-77592-0},
	url = {http://link.springer.com/10.1007/978-0-387-77592-0},
	language = {en},
	urldate = {2021-10-15},
	publisher = {Springer US},
	author = {Beigi, Homayoon},
	year = {2011},
	doi = {10.1007/978-0-387-77592-0},
}

@book{willis_cognitive_2005,
	address = {Thousand Oaks, Calif},
	title = {Cognitive interviewing: a tool for improving questionnaire design},
	isbn = {978-0-7619-2803-4 978-0-7619-2804-1},
	shorttitle = {Cognitive interviewing},
	publisher = {Sage Publications},
	author = {Willis, Gordon B.},
	year = {2005},
}

@book{hannappel_mikrosimulationen_2020,
	address = {Wiesbaden},
	title = {Mikrosimulationen: {Methodische} {Grundlagen} und ausgewählte {Anwendungsfelder}},
	isbn = {978-3-658-23701-1 978-3-658-23702-8},
	shorttitle = {Mikrosimulationen},
	url = {http://link.springer.com/10.1007/978-3-658-23702-8},
	language = {de},
	urldate = {2021-10-14},
	publisher = {Springer Fachmedien Wiesbaden},
	editor = {Hannappel, Marc and Kopp, Johannes},
	year = {2020},
	doi = {10.1007/978-3-658-23702-8},
}

@incollection{miller_analysis_2014,
	address = {Hoboken, NJ, USA},
	title = {Analysis - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch4},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Ryan, J. Michael},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch4},
	pages = {35--50},
}

@incollection{miller_analysis_2014-1,
	address = {Hoboken, NJ, USA},
	title = {Analysis {Software} for {Cognitive} {Interviewing} {Studies}: {Q}-{Notes} and {Q}-{Bank} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	shorttitle = {Analysis {Software} for {Cognitive} {Interviewing} {Studies}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch8},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Mezetin, Justin and Massey, Meredith},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch8},
	pages = {107--131},
}

@incollection{miller_assessing_2014,
	address = {Hoboken, NJ, USA},
	title = {Assessing {Translated} {Questions} via {Cognitive} {Interviewing} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch5},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Schoua-Glusberg, Alisú and Villar, Ana},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch5},
	pages = {51--67},
}

@incollection{miller_case_2014,
	address = {Hoboken, NJ, USA},
	title = {Case {Study}: {Evaluation} of a {Sexual} {Identity} {Question} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	shorttitle = {Case {Study}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch7},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Miller, Kristen and Ryan, J. Michael},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch7},
	pages = {85--106},
}

@incollection{miller_cognitive_2014,
	address = {Hoboken, NJ, USA},
	title = {Cognitive {Interviewing} in {Mixed} {Research} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch9},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Baena, Isabel Benitez and Padilla, José-Luis},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch9},
	pages = {133--152},
}

@incollection{miller_conclusion_2014,
	address = {Hoboken, NJ, USA},
	title = {Conclusion - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch10},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch10},
	pages = {153--161},
}

@incollection{miller_conveying_2014,
	address = {Hoboken, NJ, USA},
	title = {Conveying {Results} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch6},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Chepp, Valerie and Scanlon, Paul},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch6},
	pages = {69--84},
}

@incollection{miller_data_2014,
	address = {Hoboken, NJ, USA},
	title = {Data {Collection} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch3},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Willson, Stephanie and Miller, Kristen},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch3},
	pages = {15--33},
}

@incollection{miller_foundations_2014,
	address = {Hoboken, NJ, USA},
	title = {Foundations and {New} {Directions} - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch2},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Chepp, Valerie and Gray, Caroline},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch2},
	pages = {7--14},
}

@incollection{miller_introduction_2014,
	address = {Hoboken, NJ, USA},
	title = {Introduction - [{Cognitive} {Interviewing} {Methodology}]},
	isbn = {978-1-118-83886-0 978-1-118-58943-4},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118838860.ch1},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Cognitive {Interviewing} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Miller, Kristen},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	month = aug,
	year = {2014},
	doi = {10.1002/9781118838860.ch1},
	pages = {1--5},
}

@book{miller_cognitive_2014-1,
	address = {Hoboken, New Jersey},
	series = {Wiley series in survey methodology},
	title = {Cognitive interviewing methodology},
	isbn = {978-1-118-38354-4},
	abstract = {"Acknowledging the impact of sociological factors on the survey process, this book introduces a paradigm for the cognitive interview process. It introduces the interpretive approach to cognitive interviewing, presents the underlying theoretical foundations, and explores the issues relating it. The book also addresses the various aspects of data collection, analysis, and documentation. It is an ideal reference for survey researchers and practitioners in the social sciences who utilize these techniques in their everyday work and as a supplement for courses on survey methods at the upper-undergraduate and graduate levels"--},
	publisher = {John Wiley \& Sons, Inc},
	editor = {Miller, Kristen and Willson, Stephanie and Chepp, Valerie and Padilla, José-Luis},
	year = {2014},
}

@incollection{presser_case_2004,
	address = {Hoboken, NJ, USA},
	title = {The {Case} for {More} {Split}-{Sample} {Experiments} in {Developing} {Survey} {Instruments}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch9},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Fowler, Floyd Jackson},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch9},
	pages = {173--188},
}

@incollection{presser_vignettes_2004,
	address = {Hoboken, NJ, USA},
	title = {Vignettes and {Respondent} {Debriefing} for {Questionnaire} {Design} and {Evaluation}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch8},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Martin, Elizabeth},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch8},
	pages = {149--171},
}

@incollection{presser_evaluating_2004,
	address = {Hoboken, NJ, USA},
	title = {Evaluating {Survey} {Questions} by {Analyzing} {Patterns} of {Behavior} {Codes} and {Question}–{Answer} {Sequences}: {A} {Diagnostic} {Approach}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {Evaluating {Survey} {Questions} by {Analyzing} {Patterns} of {Behavior} {Codes} and {Question}–{Answer} {Sequences}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch6},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {van der Zouwen, Johannes and Smit, Johannes H.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch6},
	pages = {109--130},
}

@incollection{presser_different_2004,
	address = {Hoboken, NJ, USA},
	title = {Do {Different} {Cognitive} {Interview} {Techniques} {Produce} {Different} {Results}?},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch5},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {DeMaio, Theresa J. and Landreth, Ashley},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch5},
	pages = {89--108},
}

@incollection{presser_data_2004,
	address = {Hoboken, NJ, USA},
	title = {Data {Quality} in {Cognitive} {Interviews}: {The} {Case} of {Verbal} {Reports}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {Data {Quality} in {Cognitive} {Interviews}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch4},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Conrad, Frederick G. and Blair, Johnny},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch4},
	pages = {67--87},
}

@incollection{presser_dynamics_2004,
	address = {Hoboken, NJ, USA},
	title = {The {Dynamics} of {Cognitive} {Interviewing}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch3},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Beatty, Paul},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch3},
	pages = {45--66},
}

@incollection{presser_does_2004,
	address = {Hoboken, NJ, USA},
	title = {Does {Pretesting} {Make} a {Difference}? {An} {Experimental} {Test}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {Does {Pretesting} {Make} a {Difference}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch25},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Forsyth, Barbara and Rothgeb, Jennifer M. and Willis, Gordon B.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch25},
	pages = {525--546},
}

@incollection{presser_multiple_2004,
	address = {Hoboken, NJ, USA},
	title = {Multiple {Methods} for {Developing} and {Evaluating} a {Stated}-{Choice} {Questionnaire} to {Value} {Wetlands}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch24},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Kaplowitz, Michael D. and Lupi, Frank and Hoehn, John P.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch24},
	pages = {503--524},
}

@incollection{presser_multiple-method_2004,
	address = {Hoboken, NJ, USA},
	title = {A {Multiple}-{Method} {Approach} to {Improving} the {Clarity} of {Closely} {Related} {Concepts}: {Distinguishing} {Legal} and {Physical} {Custody} of {Children}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {A {Multiple}-{Method} {Approach} to {Improving} the {Clarity} of {Closely} {Related} {Concepts}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch23},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Schaeffer, Nora Cate and Dykema, Jennifer},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch23},
	pages = {475--502},
}

@incollection{presser_survey_2004,
	address = {Hoboken, NJ, USA},
	title = {Survey {Questionnaire} {Translation} and {Assessment}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch22},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Harkness, Janet and Pennell, Beth-Ellen and Schoua-Glusberg, Alisú},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch22},
	pages = {453--473},
}

@incollection{presser_developing_2004,
	address = {Hoboken, NJ, USA},
	title = {Developing and {Evaluating} {Cross}-{National} {Survey} {Instruments}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch21},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Smith, Tom W.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch21},
	pages = {431--452},
}

@incollection{presser_pretesting_2004,
	address = {Hoboken, NJ, USA},
	title = {Pretesting {Questionnaires} for {Children} and {Adolescents}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch20},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {de Leeuw, Edith and Borgers, Natacha and Smits, Astrid},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch20},
	pages = {409--429},
}

@incollection{presser_evolution_2004,
	address = {Hoboken, NJ, USA},
	title = {Evolution and {Adaptation} of {Questionnaire} {Development}, {Evaluation}, and {Testing} {Methods} for {Establishment} {Surveys}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch19},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Willimack, Diane K. and Lyberg, Lars and Martin, Jean and Japec, Lilli and Whitridge, Patricia},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch19},
	pages = {385--407},
}

@incollection{presser_development_2004,
	address = {Hoboken, NJ, USA},
	title = {Development and {Testing} of {Web} {Questionnaires}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch18},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Baker, Reginald P. and Crawford, Scott and Swinehart, Janice},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch18},
	pages = {361--384},
}

@incollection{presser_usability_2004,
	address = {Hoboken, NJ, USA},
	title = {Usability {Testing} to {Evaluate} {Computer}-{Assisted} {Instruments}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch17},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Hansen, Sue Ellen and Couper, Mick P.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch17},
	pages = {337--360},
}

@incollection{presser_methods_2004,
	address = {Hoboken, NJ, USA},
	title = {Methods for {Testing} and {Evaluating} {Computer}-{Assisted} {Questionnaires}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch16},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Tarnai, John and Moore, Danna L.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch16},
	pages = {319--335},
}

@incollection{presser_testing_2004,
	address = {Hoboken, NJ, USA},
	title = {Testing {Paper} {Self}-{Administered} {Questionnaires}: {Cognitive} {Interview} and {Field} {Test} {Comparisons}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {Testing {Paper} {Self}-{Administered} {Questionnaires}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch15},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Dillman, Don A. and Redline, Cleo D.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch15},
	pages = {299--317},
}

@incollection{presser_development_2004,
	address = {Hoboken, NJ, USA},
	title = {Development and {Improvement} of {Questionnaires} {Using} {Predictions} of {Reliability} and {Validity}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch14},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Saris, Willem E. and van der Veld, William and Gallhofer, Irmtraud},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch14},
	pages = {275--297},
}

@incollection{presser_item_2004,
	address = {Hoboken, NJ, USA},
	title = {Item {Response} {Theory} {Modeling} for {Questionnaire} {Evaluation}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch13},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Reeve, Bryce B. and Mâsse, Louise C.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch13},
	pages = {247--273},
}

@incollection{presser_modeling_2004,
	address = {Hoboken, NJ, USA},
	title = {Modeling {Measurement} {Error} to {Identify} {Flawed} {Questions}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch12},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Biemer, Paul},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch12},
	pages = {225--246},
}

@incollection{presser_experimental_2004,
	address = {Hoboken, NJ, USA},
	title = {Experimental {Design} {Considerations} for {Testing} and {Evaluating} {Questionnaires}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch11},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Tourangeau, Roger},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch11},
	pages = {209--224},
}

@incollection{presser_using_2004,
	address = {Hoboken, NJ, USA},
	title = {Using {Field} {Experiments} to {Improve} {Instrument} {Design}: {The} {SIPP} {Methods} {Panel} {Project}},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {Using {Field} {Experiments} to {Improve} {Instrument} {Design}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch10},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Moore, Jeffrey and Pascale, Joanne and Doyle, Pat and Chan, Anna and Griffiths, Julia Klein},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch10},
	pages = {189--207},
}

@incollection{presser_cognitive_2004,
	address = {Hoboken, NJ, USA},
	title = {Cognitive {Interviewing} {Revisited}: {A} {Useful} {Technique}, in {Theory}?},
	isbn = {978-0-471-45841-8 978-0-471-65472-8},
	shorttitle = {Cognitive {Interviewing} {Revisited}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471654728.ch2},
	language = {en},
	urldate = {2021-10-14},
	booktitle = {Wiley {Series} in {Survey} {Methodology}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Willis, Gordon B.},
	editor = {Presser, Stanley and Rothgeb, Jennifer M. and Couper, Mick P. and Lessler, Judith T. and Martin, Elizabeth and Martin, Jean and Singer, Eleanor},
	month = jun,
	year = {2004},
	doi = {10.1002/0471654728.ch2},
	pages = {23--43},
}

@book{saris_design_2014,
	address = {Hoboken, New Jersey},
	edition = {Second Edition},
	series = {Wiley series in survey methodology},
	title = {Design, evaluation, and analysis of questionnaires for survey research},
	isbn = {978-1-118-63461-5},
	publisher = {Wiley},
	author = {Saris, Willem E. and Gallhofer, Irmtraud N.},
	year = {2014},
}

@book{scherr_diskriminierung_2016,
	address = {Wiesbaden},
	edition = {2., überarbeitete Auflage},
	series = {Essentials},
	title = {Diskriminierung: wie {Unterschiede} und {Benachteiligungen} gesellschaftlich hergestellt werden},
	isbn = {978-3-658-10067-4 978-3-658-10066-7},
	shorttitle = {Diskriminierung},
	language = {ger},
	publisher = {Springer VS},
	author = {Scherr, Albert},
	year = {2016},
}

@book{murrell_r_2006,
	address = {Boca Raton, Fla.},
	series = {Computer science and data analysis series},
	title = {R graphics},
	isbn = {978-1-58488-486-6},
	url = {https://www.stat.auckland.ac.nz/~paul/RGraphics/rgraphics.html},
	language = {eng},
	publisher = {Chapman \& Hall/CRC},
	author = {Murrell, Paul},
	year = {2006},
}

@book{burns_r_2012,
	title = {The {R} inferno},
	isbn = {978-1-4710-4652-0},
	language = {English},
	publisher = {lulu.com},
	author = {Burns, Patrick},
	month = jan,
	year = {2012},
	note = {OCLC: 954273271},
}

@book{rossel_sozialstrukturanalyse_2009,
	address = {Wiesbaden},
	edition = {1. Aufl},
	series = {Hagener {Studientexte} zur {Soziologie}},
	title = {Sozialstrukturanalyse: eine kompakte {Einführung} ; [{Lehrbuch}]},
	isbn = {978-3-531-14997-4},
	shorttitle = {Sozialstrukturanalyse},
	language = {ger},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Rössel, Jörg},
	year = {2009},
}

@book{valliant_practical_2013,
	address = {New York},
	series = {Statistics for social and behavioral sciences},
	title = {Practical tools for designing and weighting survey samples},
	isbn = {978-1-4614-6448-8},
	publisher = {Springer},
	author = {Valliant, Richard and Dever, Jill A. and Kreuter, Frauke},
	year = {2013},
	note = {OCLC: ocn822974515},
}

@book{franz_arbeitsmarktokonomik_2013,
	address = {Berlin},
	edition = {8., aktualisierte und erg. Aufl},
	series = {Springer-{Lehrbuch}},
	title = {Arbeitsmarktökonomik},
	isbn = {978-3-642-36902-5},
	language = {ger},
	publisher = {Springer Gabler},
	author = {Franz, Wolfgang},
	year = {2013},
}

@book{feldman_text_2007,
	address = {Cambridge [etc.},
	title = {The text mining handbook advanced approaches in analyzing unstructured data},
	isbn = {978-0-511-33507-5},
	url = {https://wtlab.um.ac.ir/images/e-library/text_mining/The%20Text%20Mining%20HandBook.pdf},
	language = {English},
	urldate = {2021-10-14},
	publisher = {Cambridge University Press},
	author = {Feldman, Ronen and Sanger, James},
	year = {2007},
	note = {OCLC: 1080414704},
}

@book{agresti_categorical_2002,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Categorical data analysis},
	isbn = {978-0-471-36093-3},
	publisher = {Wiley-Interscience},
	author = {Agresti, Alan},
	year = {2002},
}

@book{agresti_introduction_2007,
	address = {Hoboken, NJ},
	edition = {2nd ed},
	series = {Wiley series in probability and mathematical statistics},
	title = {An introduction to categorical data analysis},
	isbn = {978-0-471-22618-5},
	publisher = {Wiley-Interscience},
	author = {Agresti, Alan},
	year = {2007},
	note = {OCLC: ocm71812851},
}

@book{aggarwal_mining_2012,
	address = {New York},
	title = {Mining text data},
	isbn = {978-1-4614-3223-4},
	language = {eng},
	publisher = {Springer},
	author = {Aggarwal, Charu C. and Zhai, ChengXiang},
	year = {2012},
}

@book{pruscha_statistical_2013,
	address = {Berlin ; New York},
	title = {Statistical analysis of climate series: analyzing, plotting, modeling, and predicting with {R}},
	isbn = {978-3-642-32083-5},
	shorttitle = {Statistical analysis of climate series},
	publisher = {Springer},
	author = {Pruscha, Helmut},
	year = {2013},
	note = {OCLC: ocn820522055},
}

@book{kundu_statistical_2012,
	address = {India},
	series = {{SpringerBriefs} in {Statistics}},
	title = {Statistical {Signal} {Processing}},
	isbn = {978-81-322-0627-9 978-81-322-0628-6},
	url = {http://link.springer.com/10.1007/978-81-322-0628-6},
	urldate = {2021-10-14},
	publisher = {Springer India},
	author = {Kundu, Debasis and Nandi, Swagata},
	year = {2012},
	doi = {10.1007/978-81-322-0628-6},
}

@book{kurzweil_physik_2008,
	address = {Wiesbaden},
	edition = {1. Aufl},
	series = {Studium {Technik}},
	title = {Physik {Formelsammlung}: für {Ingenieure} und {Naturwissenschaftler}},
	isbn = {978-3-8348-0251-4},
	shorttitle = {Physik {Formelsammlung}},
	language = {ger},
	publisher = {Vieweg},
	editor = {Kurzweil, Peter and Frenzel, Bernhard and Gebhard, Florian},
	year = {2008},
}

@book{hertel_theoretische_2007,
	address = {Berlin Heidelberg},
	series = {Springer-{Lehrbuch}},
	title = {Theoretische {Physik}: mit 6 {Tabellen}},
	isbn = {978-3-540-36644-7},
	shorttitle = {Theoretische {Physik}},
	abstract = {1. Überblick über die theoretische Physik. Das Lehrbuch ist gedacht für Studierende mit Physik als Nebenfach, für das Lehramt an Gymnasien, Bachelor sowie als 1. Einführung in die Master- oder Diplomstudiengänge Physik. Die Gebiete Mechanik, Elektrodynamik, Quantenmechanik und Thermodynamik werden zunächst in einführenden, dann in darauf aufbauenden Kapiteln behandelt. Der Autor hat über 30 Jahre als Professor für Theoretische Physik gewirkt. Breiter einsetzbar als die mehrbändigen Ausgaben zur theoretischen Physik, hier zuletzt E. Rebhah (ID 31/00 und 6/05). L. Landau (ID 6/05) sowie T. Fließbach (in dieser Nr.). Enthält keine Übungsaufgaben. Empfohlen als Ergänzung zu anderen Lehr- und Übungsbüchern der Physik. (3)},
	language = {ger},
	publisher = {Springer},
	author = {Hertel, Peter},
	year = {2007},
}

@book{dobrinski_physik_2010,
	address = {Wiesbaden},
	edition = {12., aktualisierte Aufl},
	series = {Studium},
	title = {Physik für {Ingenieure}},
	isbn = {978-3-8348-0580-5},
	language = {ger},
	publisher = {Vieweg + Teubner},
	author = {Dobrinski, Paul and Krakau, Gunter and Vogel, Anselm},
	year = {2010},
}

@book{hering_physik_2007,
	address = {Berlin Heidelberg New York},
	edition = {10., vollst. neu bearb. Aufl},
	series = {Springer-{Lehrbuch}},
	title = {Physik für {Ingenieure}: mit ... 116 {Tabellen}, [mit durchgerechneten {Lösungen} und neuem {Layout}]},
	isbn = {978-3-540-71855-0},
	shorttitle = {Physik für {Ingenieure}},
	language = {ger},
	publisher = {Springer},
	author = {Hering, Ekbert and Martin, Rolf and Stohrer, Martin},
	year = {2007},
}

@book{borcard_numerical_2011,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Numerical {Ecology} with {R}},
	isbn = {978-1-4419-7975-9 978-1-4419-7976-6},
	url = {http://link.springer.com/10.1007/978-1-4419-7976-6},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer New York},
	author = {Borcard, Daniel and Gillet, Francois and Legendre, Pierre},
	year = {2011},
	doi = {10.1007/978-1-4419-7976-6},
}

@book{knoblauch_modeling_2012,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Modeling {Psychophysical} {Data} in {R}},
	isbn = {978-1-4614-4475-6},
	url = {http://link.springer.com/10.1007/978-1-4614-4475-6},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer New York},
	author = {Knoblauch, Kenneth and Maloney, Laurence T.},
	year = {2012},
	doi = {10.1007/978-1-4614-4475-6},
}

@book{rautenberg_einfuhrung_2008,
	address = {Wiesbaden},
	edition = {3., überarb. Aufl},
	series = {Studium},
	title = {Einführung in die mathematische {Logik}: ein {Lehrbuch}},
	isbn = {978-3-8348-0578-2},
	shorttitle = {Einführung in die mathematische {Logik}},
	abstract = {Verlagstext: Dieses umfassende Lehrbuch wurde geschrieben für Studenten und Dozenten der Mathematik und Informatik, und wegen der ausführlichen Darstellung der Gödelschen Unvollständigkeitssätze auch für Fachstudenten der Philosophischen Logik},
	language = {ger},
	publisher = {Vieweg + Teubner},
	author = {Rautenberg, Wolfgang},
	year = {2008},
}

@book{weltner_mathematik_2012,
	address = {Berlin, Heidelberg},
	edition = {16. Aufl. 2012},
	series = {Springer-{Lehrbuch}},
	title = {Mathematik für {Physiker} und {Ingenieure} 2: {Basiswissen} für das {Grundstudium} - mit mehr als 900 {Aufgaben} und {Lösungen} online},
	isbn = {978-3-642-25519-9},
	shorttitle = {Mathematik für {Physiker} und {Ingenieure} 2},
	language = {ger},
	publisher = {Springer},
	author = {Weltner, Klaus},
	year = {2012},
}

@book{weltner_mathematik_2012-1,
	address = {Berlin, Heidelberg},
	edition = {17. Aufl. 2012},
	series = {Springer-{Lehrbuch}},
	title = {Mathematik für {Physiker} und {Ingenieure} 1: {Basiswissen} für das {Grundstudium} - mit mehr als 1400 {Aufgaben} und {Lösungen} online},
	isbn = {978-3-642-30085-1},
	shorttitle = {Mathematik für {Physiker} und {Ingenieure} 1},
	language = {ger},
	publisher = {Springer},
	author = {Weltner, Klaus},
	year = {2012},
}

@book{prechtl_vorlesungen_2008,
	address = {Wien New York},
	edition = {2., überarb. Aufl},
	title = {Vorlesungen über die {Grundlagen} der {Elektrotechnik}},
	isbn = {978-3-211-72455-2},
	language = {ger},
	publisher = {Springer},
	author = {Prechtl, Adalbert},
	year = {2008},
}

@book{kupfmuller_theoretische_2008,
	address = {Berlin Heidelberg},
	edition = {18. Aufl},
	series = {Springer-{Lehrbuch}},
	title = {Theoretische {Elektrotechnik}: eine {Einführung}},
	isbn = {978-3-540-78590-3},
	shorttitle = {Theoretische {Elektrotechnik}},
	language = {ger},
	publisher = {Springer},
	author = {Küpfmüller, Karl and Mathis, Wolfgang and Reibiger, Albrecht},
	year = {2008},
}

@book{plassmann_handbuch_2013,
	address = {Wiesbaden},
	edition = {6., neu bearb. Aufl},
	series = {Handbuch {Elektrotechnik}},
	title = {Handbuch {Elektrotechnik}: {Grundlagen} und {Anwendungen} für {Elektrotechniker}},
	isbn = {978-3-8348-2071-6},
	shorttitle = {Handbuch {Elektrotechnik}},
	abstract = {Dieses Handbuch stellt in systematischer Form alle wesentlichen Grundlagen der Elektrotechnik in der komprimierten Form eines Nachschlagewerkes zusammen. Es wurde für Studenten und Praktiker entwickelt. Für Spezialisten eines bestimmten Fachgebiets wird ein umfassender Einblick in Nachbargebiete geboten. Die didaktisch ausgezeichneten Darstellungen ermöglichen eine rasche Erarbeitung des umfangreichen Inhalts. Über 2000 Abbildungen und Tabellen, passgenau ausgewählte Formeln, Hinweise, Schaltpläne und Normen führen den Benutzer sicher durch die Elektrotechnik. In die 6. Auflage wurde die Regelungstechnik neu aufgenommen. Der Inhalt Mathematik - Physik - Werkstoffkunde - Elektrotechnik - Elektronik - Technische Kommunikation - Datentechnik - Automatisierungstechnik - Regelungstechnik - Messtechnik - Energietechnik - Nachrichtentechnik - Signal- und Systemtheorie Die Zielgruppe Studierende, Professoren und Dozenten an Universitäten, Hochschulen, Fachhochschulen und Fachschulen Technik Ingenieure und Praktiker der Elektrotechnik Die Herausgeber Prof. (em.) Dr.-Ing. Wilfried Plaßmannlehrte an derHochschule Hannover, Fakultät I, Elektro- und Informationstechnik. Univ.-Prof. Dr.-Ing. habil. Detlef Schulz leitet das Fachgebiet Elektrische Energiesysteme an der Helmut-Schmidt-Universität/Universität der Bundeswehr Hamburg. Die Autoren sind Fachleute aus Industrie, Forschung und Lehre},
	language = {ger},
	publisher = {Springer Vieweg},
	author = {Plassmann, Wilfried},
	year = {2013},
}

@book{schusler_digitale_2010,
	address = {Berlin Heidelberg},
	title = {Digitale {Signalverarbeitung}. 2: {Entwurf} diskreter {Systeme} / {Hans} {W}. {Schüßler}. {Bearb}. von {G}. {Dehner}},
	isbn = {978-3-642-01119-1 978-3-642-01118-4},
	shorttitle = {Digitale {Signalverarbeitung}. 2},
	language = {ger},
	publisher = {Springer},
	author = {Schüßler, Hans Wilhelm and Dehner, Günter},
	year = {2010},
}

@book{schusler_digitale_2008,
	address = {Berlin Heidelberg},
	edition = {5., neu bearb. u. erg. Aufl},
	title = {Digitale {Signalverarbeitung}. 1: {Analyse} diskreter {Signale} und {Systeme} / {Hans} {W}. {Schüßler}. {Bearb}. von {Günter} {Dehner}},
	isbn = {978-3-540-78251-3 978-3-540-78250-6},
	shorttitle = {Digitale {Signalverarbeitung}. 1},
	language = {ger},
	publisher = {Springer},
	author = {Schüßler, Hans Wilhelm and Dehner, Günter},
	year = {2008},
}

@book{christen_data_2012,
	address = {Berlin, Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Data {Matching}},
	isbn = {978-3-642-31163-5 978-3-642-31164-2},
	url = {http://link.springer.com/10.1007/978-3-642-31164-2},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer Berlin Heidelberg},
	author = {Christen, Peter},
	year = {2012},
	doi = {10.1007/978-3-642-31164-2},
}

@book{beyersmann_competing_2012,
	address = {New York, NY},
	series = {Use {R}!},
	title = {Competing {Risks} and {Multistate} {Models} with {R}},
	isbn = {978-1-4614-2035-4},
	url = {http://link.springer.com/10.1007/978-1-4614-2035-4},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer New York},
	author = {Beyersmann, Jan and Allignol, Arthur and Schumacher, Martin},
	year = {2012},
	doi = {10.1007/978-1-4614-2035-4},
}

@book{wehrens_chemometrics_2011,
	address = {Berlin, Heidelberg},
	series = {Use {R}!},
	title = {Chemometrics with {R}},
	isbn = {978-3-642-17840-5 978-3-642-17841-2},
	url = {http://link.springer.com/10.1007/978-3-642-17841-2},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer Berlin Heidelberg},
	author = {Wehrens, Ron},
	year = {2011},
	doi = {10.1007/978-3-642-17841-2},
}

@book{hernan_causal_2020,
	address = {Boca Raton},
	title = {Causal {Inference}: {What} {If}},
	url = {https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Chapman \& Hall/CRC},
	author = {Hernán, Miguel A and Robins, James M},
	month = dec,
	year = {2020},
}

@book{hilbe_astrostatistical_2013,
	address = {New York, NY},
	series = {Springer {Series} in {Astrostatistics}},
	title = {Astrostatistical {Challenges} for the {New} {Astronomy}},
	volume = {1},
	isbn = {978-1-4614-3507-5 978-1-4614-3508-2},
	url = {http://link.springer.com/10.1007/978-1-4614-3508-2},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer New York},
	editor = {Hilbe, Joseph M.},
	year = {2013},
	doi = {10.1007/978-1-4614-3508-2},
}

@book{van_den_boogaart_analyzing_2013,
	address = {Berlin, Heidelberg},
	series = {Use {R}!},
	title = {Analyzing {Compositional} {Data} with {R}},
	isbn = {978-3-642-36808-0 978-3-642-36809-7},
	url = {http://link.springer.com/10.1007/978-3-642-36809-7},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer Berlin Heidelberg},
	author = {van den Boogaart, K. Gerald and Tolosana-Delgado, Raimon},
	year = {2013},
	doi = {10.1007/978-3-642-36809-7},
}

@book{alwin_margins_2007,
	address = {Hoboken, N.J},
	series = {Wiley series in survey methodology},
	title = {Margins of error: a study of reliability in survey measurement},
	isbn = {978-0-470-08148-8},
	shorttitle = {Margins of error},
	publisher = {Wiley-Interscience},
	author = {Alwin, Duane F.},
	year = {2007},
	note = {OCLC: ocm77522516},
}

@book{di_ciaccio_advanced_2012,
	address = {Berlin, Heidelberg},
	title = {Advanced {Statistical} {Methods} for the {Analysis} of {Large} {Data}-{Sets}},
	isbn = {978-3-642-21036-5 978-3-642-21037-2},
	url = {http://link.springer.com/10.1007/978-3-642-21037-2},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer Berlin Heidelberg},
	editor = {Di Ciaccio, Agostino and Coli, Mauro and Angulo Ibanez, Jose Miguel},
	year = {2012},
	doi = {10.1007/978-3-642-21037-2},
}

@book{lovric_international_2011,
	address = {Berlin Heidelberg},
	title = {International {Encyclopedia} of {Statistical} {Science}},
	isbn = {978-3-642-04897-5},
	url = {https://www.springer.com/us/book/9783642048975},
	abstract = {The goal of this book is multidimensional: a) to help reviving Statistics education in many parts in the world where it is in crisis. For the first time authors from many developing countries have an opportunity to write together with the most prominent world authorities. The editor has spent several years searching for the most reputable statisticians all over the world. International contributors are either presidents of the local statistical societies, or head of the Statistics department at the main university, or the most distinguished statisticians in their countries. b) to enable any non-statistician to obtain quick and yet comprehensive and highly understandable view on certain statistical term, method or application c) to enable all the researchers, managers and practicioners to refresh their knowledge in Statistics, especially in certain controversial fields. d) to revive interest in statistics among students, since they will see its usefulness and relevance in almost all branches of Science.},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer-Verlag},
	editor = {Lovric, Miodrag},
	year = {2011},
}

@misc{noauthor_handbook_nodate,
	title = {Handbook of {Statistics} {\textbar} {All} {Handbook} {Volumes} {\textbar} {ScienceDirect}.com by {Elsevier}},
	url = {https://www.sciencedirect.com/handbook/handbook-of-statistics/volumes},
	urldate = {2021-10-13},
}

@book{sammut_encyclopedia_2017,
	address = {Boston, MA},
	title = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	isbn = {978-1-4899-7685-7},
	url = {http://link.springer.com/10.1007/978-1-4899-7687-1},
	language = {en},
	urldate = {2021-10-13},
	publisher = {Springer US},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2017},
	doi = {10.1007/978-1-4899-7687-1},
}

@book{helfrich_commons_2012,
	address = {Bielefeld},
	edition = {1. Auflage},
	title = {Commons: für eine neue {Politik} jenseits von {Markt} und {Staat}},
	isbn = {978-3-8376-2036-8},
	shorttitle = {Commons},
	publisher = {Transcript},
	editor = {Helfrich, Silke and Heinrich-Böll-Stiftung},
	year = {2012},
}

@techreport{the_committee_for_the_prize_in_economic_sciences_in_memory_of_alfred_nobel_answering_2021,
	title = {Answering causal questions using observational data},
	url = {https://www.nobelprize.org/uploads/2021/10/advanced-economicsciencesprize2021.pdf},
	language = {en},
	urldate = {2021-10-11},
	institution = {Royal Swedish Academy of Sciences},
	author = {The Committee for the Prize in Economic Sciences in Memory of Alfred Nobel},
	month = oct,
	year = {2021},
	keywords = {inspirational, review},
	pages = {48},
}

@book{noauthor_lonely_2019,
	edition = {18},
	series = {Lonely {Planet} {Travel} {Guide}},
	title = {Lonely {Planet} {India}},
	isbn = {978-1-78701-369-8},
	language = {English},
	publisher = {Lonely Planet},
	year = {2019},
	note = {OCLC: 1090529906},
}

@article{schierholz_occupation_2018,
	title = {Occupation coding during the interview},
	volume = {181},
	issn = {0964-1998, 1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssa.12297},
	doi = {10.1111/rssa.12297},
	abstract = {Currently, most surveys ask for occupation with open-ended questions. The verbal responses are coded afterwards, which is error prone and expensive. We present an alternative approach that allows occupation coding during the interview. Our new technique uses a supervised learning algorithm to predict candidate job categories. These suggestions are presented to the respondent, who in turn can choose the most appropriate occupation. 72.4\% of the respondents selected an occupation when the new instrument was tested in a telephone survey, entailing potential cost savings. To aid further improvements, we identify some factors for how to increase quality and to reduce interview duration.},
	language = {en},
	number = {2},
	urldate = {2021-10-11},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Schierholz, Malte and Gensicke, Miriam and Tschersich, Nikolai and Kreuter, Frauke},
	month = feb,
	year = {2018},
	pages = {379--407},
}

@article{schierholz_machine_2020,
	title = {Machine {Learning} for {Occupation} {Coding}—a {Comparison} {Study}},
	issn = {2325-0984, 2325-0992},
	url = {https://academic.oup.com/jssam/advance-article/doi/10.1093/jssam/smaa023/5952839},
	doi = {10.1093/jssam/smaa023},
	abstract = {Asking people about their occupation is common practice in surveys and censuses around the world. The answers are typically recorded in textual form and subsequently assigned (coded) to categories, which have been defined in official occupational classifications. While this coding step is often done manually, substituting it with more automated workflows has been a longstanding goal, promising reduced data-processing costs and accelerated publication of key statistics. Although numerous researchers have developed different algorithms for automated occupation coding, the algorithms have rarely been compared with each other or tested on different data sets. We fill this gap by comparing some of the most promising algorithms found in the literature and testing them on five data sets from Germany. The first two algorithms we test exemplify a common practice in which answers are coded automatically according to a predefined list of job titles. Statistical learning algorithms—that is, regularized multinomial regression, tree boosting, or algorithms developed specifically for occupation coding (algorithms three to six)—can improve upon algorithms one and two, but only if a sufficient number of training observations from previous surveys is available. The best results are obtained by merging the list of job titles with coded answers from previous surveys before using this combined training data for statistical learning (algorithm 7). However, the differences between the algorithms are often small compared to the large variation found across different data sets, which we ascribe to systematic differences in the way the data were coded in the first place. Such differences complicate the application of statistical learning, which risks perpetuating questionable coding decisions from the training data to the future.},
	language = {en},
	urldate = {2021-10-11},
	journal = {Journal of Survey Statistics and Methodology},
	author = {Schierholz, Malte and Schonlau, Matthias},
	month = nov,
	year = {2020},
	pages = {smaa023},
}

@article{massing_how_2019,
	title = {How {Standardized} is {Occupational} {Coding}? {A} {Comparison} of {Results} from {Different} {Coding} {Agencies} in {Germany}},
	volume = {35},
	issn = {2001-7367},
	shorttitle = {How {Standardized} is {Occupational} {Coding}?},
	url = {https://www.sciendo.com/article/10.2478/jos-2019-0008},
	doi = {10.2478/jos-2019-0008},
	abstract = {Abstract
            As occupational data play a crucial part in many social and economic analyses, information on the reliability of these data and, in particular on the role of coding agencies, is important. Based on our review of previous research, we develop four hypotheses, which we test using occupation-coded data from the German General Social Survey and the field test data from the German Programme for the International Assessment of Adult Competencies. Because the same data were coded by several agencies, their coding results could be directly compared. As the surveys used different instruments, and interviewer training differed, the effects of these factors could also be evaluated.
            Our main findings are: the percentage of uncodeable responses is low (1.8–4.9\%) but what is classified as “uncodeable” varies between coding agencies. Inter-agency coding reliability is relatively low κ ca. 0.5 at four-digit level, and codings sometimes differ systematically between agencies. The reliability of derived status scores is satisfactory (0.82–0.90). The previously reported negative relationship between answer length and coding reliability could be replicated and effects of interviewer training demonstrated. Finally, we discuss the importance of establishing common coding rules and present recommendations to overcome some of the problems in occupation coding.},
	language = {en},
	number = {1},
	urldate = {2021-10-11},
	journal = {Journal of Official Statistics},
	author = {Massing, Natascha and Wasmer, Martina and Wolf, Christof and Zuell, Cornelia},
	month = mar,
	year = {2019},
	pages = {167--187},
}

@book{international_labour_office_international_2012,
	address = {Geneva},
	title = {International {Standard} {Classification} of {Occupations}: {ISCO}-08},
	publisher = {International Labour Organization},
	author = {{International Labour Office}},
	year = {2012},
}

@inproceedings{westermark_automatic_2015,
	title = {Automatic {Coding} of {Occupation} {Using} {Spell} {Checking} and {Machine} {Learning}},
	booktitle = {30th {JOS} {Anniversary} {Conference}},
	author = {Westermark, Max and Franzen, Michael and Kraft, Karin},
	year = {2015},
}

@article{wenzowski_actr_1988,
	title = {{ACTR} - {A} {Generalized} {Automated} {Coding} {System}},
	volume = {14},
	number = {2},
	journal = {Survey Methodology},
	author = {Wenzowski, Michael},
	year = {1988},
	pages = {299--307},
}

@incollection{united_nations_statistical_commission_and_economic_commission_for_europe_statistical_1997,
	address = {New York},
	title = {Statistical {Data} {Editing} {Volume} {No}. 2},
	publisher = {United Nations},
	editor = {{United Nations Statistical Commission and Economic Commission for Europe}},
	year = {1997},
	note = {Section: 6},
}

@article{trappmann_pass_2013,
	title = {The {PASS} panel survey after six waves},
	volume = {46},
	issn = {1867-8343},
	url = {https://doi.org/10.1007/s12651-013-0150-1},
	doi = {10.1007/s12651-013-0150-1},
	abstract = {The aim of the Panel Study “Labour Market and Social Security” (PASS) is to provide a database which allows analysing the dynamics of welfare benefits receipt after the introduction of the Unemployment Benefits II in Germany in 2005. This entails the take up and ending of benefits receipt as well as the social situation of households and individuals receiving benefits, their subjective ways of coping with the situation and the contact to institutions providing the basic income support. PASS is set up as a household panel study with a sample of approx. 10,000 households interviewed in each wave. In addition to household interviews with the heads of the households about 15,000 interviews with individual household members aged 15 and older are carried out. This article provides an overview of the first six waves of PASS. It focuses on the survey's main goals, the questionnaire, the sampling and study design, the number of interviews, data access and methodological research on PASS. The article closes by describing the outlook for future developments.},
	number = {4},
	journal = {Journal for Labour Market Research},
	author = {Trappmann, Mark and Beste, Jonas and Bethmann, Arne and Müller, Gerrit},
	month = dec,
	year = {2013},
	pages = {275--281},
}

@techreport{tijdens_codebook_2010,
	address = {Amsterdam},
	type = {{AIAS} {Working} {Paper} 10-102},
	title = {Codebook and explanatory note on the {WageIndicator} dataset},
	institution = {Universiteit van Amsterdam},
	author = {Tijdens, Kea and Zijl, Sanne van and Hughie-Williams, Melanie and Kaveren, Maarten van and Steinmetz, Stephanie},
	year = {2010},
}

@techreport{thompson_creating_2014,
	address = {Suitland},
	type = {Background {Material} for a meeting of the {Federal} {Economic} {Statistics} {Advisory} {Committee}},
	title = {Creating an automated industry and occupation coding process for the {American} {Community} {Survey}},
	institution = {U.S. Census Bureau},
	author = {Thompson, Matthew and Kornbau, Michael E. and Vesely, Julie},
	year = {2014},
}

@inproceedings{takahashi_automatic_2014,
	title = {An {Automatic} {Coding} {System} with a {Three}-{Grade} {Confidence} {Level} {Corresponding} to the {National}/{International} {Occupation} and {Industry} {Standard}},
	isbn = {978-989-758-049-9},
	doi = {10.5220/0005131703690375},
	booktitle = {Proceedings of the {International} {Conference} on {Knowledge} {Engineering} and {Ontology} {Development} - {Volume} 1: {KEOD}, ({IC3K} 2014)},
	publisher = {SciTePress},
	author = {Takahashi, Kazuko and Taki, Hirofumi and Tanabe, Shunsuke and Li, Wei},
	year = {2014},
	note = {Backup Publisher: INSTICC},
	pages = {369--375},
}

@inproceedings{takahashi_automatic_2005,
	address = {Berlin},
	title = {Automatic {Occupation} {Coding} with {Combination} of {Machine} {Learning} and {Hand}-{Crafted} {Rules}},
	isbn = {978-3-540-31935-1},
	abstract = {We apply a machine learning method to the occupation coding, which is a task to categorize the answers to open-ended questions regarding the respondent's occupation. Specifically, we use Support Vector Machines (SVMs) and their combination with hand-crafted rules. Conducting the occupation coding manually is expensive and sometimes leads to inconsistent coding results when the coders are not experts of the occupation coding. For this reason, a rule-based automatic method has been developed and used. However, its categorization performance is not satisfiable. Therefore, we adopt SVMs, which show high performance in various fields, and compare it with the rule-based method. We also investigate effective combination methods of SVMs and the rule-based method. In our methods, the output of the rule-based method is used as features for SVMs. We empirically show that SVMs outperform the rule-based method in the occupation coding and that the combination of the two methods yields even better accuracy.},
	booktitle = {{PAKDD} 2005. {LNCS}, vol. 3518},
	publisher = {Springer},
	author = {Takahashi, Kazuko and Takamura, Hiroya and Okumura, Manabu},
	editor = {Ho, Tu Bao and Cheung, David and Liu, Huan},
	year = {2005},
	pages = {269--279},
}

@book{statistisches_bundesamt_demographische_2016,
	address = {Wiesbaden},
	title = {Demographische {Standards}},
	publisher = {Statistisches Bundesamt},
	author = {{Statistisches Bundesamt}},
	month = dec,
	year = {2016},
}

@incollection{speizer_automated_1998,
	address = {New York},
	title = {Automated {Coding} of {Survey} {Data}},
	isbn = {978-0-471-17848-4},
	booktitle = {Computer {Assisted} {Survey} {Information} {Collection}},
	publisher = {Wiley},
	author = {Speizer, Howard and Buckley, Paul},
	editor = {Couper, Mick P. and Baker, Reginald P. and Bethlehem, Jelke and Clark, Cynthia Z. F. and Martin, Jean and Nicholls II, William L. and O'Reilly, James M.},
	year = {1998},
	pages = {pp. 223--243},
}

@article{scism_car_2016,
	title = {Car {Insurance} {Firms} {Could} {Be} {Banned} {From} {Asking} {What} {You} {Do} for a {Living}},
	url = {https://www.wsj.com/articles/car-insurance-firms-could-be-banned-from-asking-what-you-do-for-a-living-1479308820},
	urldate = {2018-10-02},
	journal = {The Wall Street Journal},
	author = {Scism, Leslie},
	year = {2016},
}

@techreport{schierholz_hilfsklassifikation_2018,
	address = {Nuremberg},
	type = {{IAB}-{Discussion} {Paper} 13/2018},
	title = {Eine {Hilfsklassifikation} mit {Tätigkeitsbeschreibungen} für {Zwecke} der {Berufs}{\textbackslash}-kodierung * {Leitgedanken} und {Dokumentation}},
	institution = {Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Schierholz, Malte and Brenner, Lorraine and Cohausz, Lea and Damminger, Lisa and Fast, Lisa and Hörig, Ann-Kathrin and Huber, Anna-Lena and Ludwig, Theresa and Petry, Annabell and Tschischka, Laura},
	year = {2018},
}

@techreport{schierholz_automating_2014,
	address = {Nuremberg},
	type = {{FDZ}-{Methodenreport} 10/2014},
	title = {Automating {Survey} {Coding} for {Occupation}},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Schierholz, Malte},
	year = {2014},
}

@article{russ_computer-based_2016,
	title = {Computer-based coding of free-text job descriptions to efficiently identify occupations in epidemiological studies},
	volume = {73},
	issn = {1351-0711},
	url = {https://oem.bmj.com/content/73/6/417},
	doi = {10.1136/oemed-2015-103152},
	abstract = {Background Mapping job titles to standardised occupation classification (SOC) codes is an important step in identifying occupational risk factors in epidemiological studies. Because manual coding is time-consuming and has moderate reliability, we developed an algorithm called SOCcer (Standardized Occupation Coding for Computer-assisted Epidemiologic Research) to assign SOC-2010 codes based on free-text job description components.Methods Job title and task-based classifiers were developed by comparing job descriptions to multiple sources linking job and task descriptions to SOC codes. An industry-based classifier was developed based on the SOC prevalence within an industry. These classifiers were used in a logistic model trained using 14 983 jobs with expert-assigned SOC codes to obtain empirical weights for an algorithm that scored each SOC/job description. We assigned the highest scoring SOC code to each job. SOCcer was validated in 2 occupational data sources by comparing SOC codes obtained fromSOCcer to expert assigned SOC codes and lead exposure estimates obtained by linking SOC codes to a job-exposure matrix.Results For 11 991 case–control study jobs, SOCcer-assigned codes agreed with 44.5\% and 76.3\% of manually assigned codes at the 6-digit and 2-digit level, respectively. Agreement increased with the score, providing a mechanism to identify assignments needing review. Good agreement was observed between lead estimates based on SOCcer and manual SOC assignments (κ 0.6–0.8). Poorer performance was observed for inspection job descriptions, which included abbreviations and worksite-specific terminology.Conclusions Although some manual coding will remain necessary, using SOCcer may improve the efficiency of incorporating occupation into large-scale epidemiological studies.},
	number = {6},
	journal = {Occupational and Environmental Medicine},
	author = {Russ, Daniel E and Ho, Kwan-Yuet and Colt, Joanne S and Armenti, Karla R and Baris, Dalsu and Chow, Wong-Ho and Davis, Faith and Johnson, Alison and Purdue, Mark P and Karagas, Margaret R and Schwartz, Kendra and Schwenn, Molly and Silverman, Debra T and Johnson, Calvin A and Friesen, Melissa C},
	year = {2016},
	note = {Publisher: BMJ Publishing Group Ltd
\_eprint: https://oem.bmj.com/content/73/6/417.full.pdf},
	pages = {417--424},
}

@techreport{rohrbach-schmidt_bibbbaua_2013,
	address = {Bonn},
	type = {{BIBB}-{FDZ} {Data} and {Methodological} {Reports} {Nr}. 1/2013. {Version} 4.1},
	title = {{BIBB}/{BAuA} {Employment} {Survey} 2012},
	institution = {Federal Institute for Vocational Education and Training},
	author = {Rohrbach-Schmidt, Daniela and Hall, Anja},
	year = {2013},
}

@incollection{riviere_automated_1997,
	address = {New York},
	title = {Automated {Coding} - {Foreword}},
	booktitle = {Statistical {Data} {Editing} {Volume} {No}. 2},
	publisher = {United Nations},
	author = {Riviere, Pascal},
	editor = {{United Nations Statistical Commission and Economic Commission for Europe}},
	year = {1997},
}

@incollection{potts_basic_2012,
	address = {Chichester},
	title = {Basic concepts},
	isbn = {978-1-119-96000-3},
	url = {http://dx.doi.org/10.1002/9781119960003.ch2},
	booktitle = {Forecast {Verification}: {A} {Practitioner}'s {Guide} to {Atmospheric} {Science}, 2nd {Edtion}},
	publisher = {Wiley},
	author = {Potts, Jacqueline M.},
	editor = {Jollife, Ian T. and Stephenson, David B.},
	year = {2012},
	doi = {10.1002/9781119960003.ch2},
	pages = {11--29},
}

@article{pers_validation_2009,
	title = {The {Validation} and {Assessment} of {Machine} {Learning}: {A} {Game} of {Prediction} from {High}-{Dimensional} {Data}},
	volume = {4},
	url = {https://doi.org/10.1371/journal.pone.0006287},
	doi = {10.1371/journal.pone.0006287},
	abstract = {In applied statistics, tools from machine learning are popular for analyzing complex and high-dimensional data. However, few theoretical results are available that could guide to the appropriate machine learning tool in a new application. Initial development of an overall strategy thus often implies that multiple methods are tested and compared on the same set of data. This is particularly difficult in situations that are prone to over-fitting where the number of subjects is low compared to the number of potential predictors. The article presents a game which provides some grounds for conducting a fair model comparison. Each player selects a modeling strategy for predicting individual response from potential predictors. A strictly proper scoring rule, bootstrap cross-validation, and a set of rules are used to make the results obtained with different strategies comparable. To illustrate the ideas, the game is applied to data from the Nugenob Study where the aim is to predict the fat oxidation capacity based on conventional factors and high-dimensional metabolomics data. Three players have chosen to use support vector machines, LASSO, and random forests, respectively.},
	number = {8},
	journal = {PLOS ONE},
	author = {Pers, Tune H. and Albrechtsen, Anders and Holst, Claus and Sørensen, Thorkild I. A. and Gerds, Thomas A.},
	month = aug,
	year = {2009},
	note = {Publisher: Public Library of Science},
	pages = {1--8},
}

@article{oreagon_computer-assigned_1972,
	title = {Computer-assigned codes from verbal responses},
	volume = {15},
	doi = {10.1145/361405.361419},
	number = {6},
	journal = {Commun. ACM},
	author = {O'Reagon, Robert T.},
	year = {1972},
	note = {Place: New York, NY, USA
Publisher: ACM},
	pages = {455--459},
}

@book{ohagan_kendall_2004,
	address = {London},
	title = {Kendall' {Advanced} {Theory} of {Statistics}, {Volume} {2B}: {Bayesian} {Inference}, 2nd {Edition}},
	isbn = {9780340807520},
	publisher = {Arnold},
	author = {O'Hagan, Anthony and Forster, Jonathan},
	year = {2004},
}

@article{nelson_future_2018,
	title = {The {Future} of {Coding}: {A} {Comparison} of {Hand}-{Coding} and {Three} {Types} of {Computer}-{Assisted} {Text} {Analysis} {Methods}},
	volume = {Online First},
	url = {https://doi.org/10.1177/0049124118769114},
	doi = {10.1177/0049124118769114},
	abstract = {Advances in computer science and computational linguistics have yielded new, and faster, computational approaches to structuring and analyzing textual data. These approaches perform well on tasks like information extraction, but their ability to identify complex, socially constructed, and unsettled theoretical concepts—a central goal of sociological content analysis—has not been tested. To fill this gap, we compare the results produced by three common computer-assisted approaches—dictionary, supervised machine learning (SML), and unsupervised machine learning—to those produced through a rigorous hand-coding analysis of inequality in the news (N = 1,253 articles). Although we find that SML methods perform best in replicating hand-coded results, we document and clarify the strengths and weaknesses of each approach, including how they can complement one another. We argue that content analysts in the social sciences would do well to keep all these approaches in their toolkit, deploying them purposefully according to the task at hand.},
	journal = {Sociological Methods \& Research},
	author = {Nelson, Laura K. and Burk, Derek and Knudsen, Marcel and McCall, Leslie},
	year = {2018},
	note = {\_eprint: https://doi.org/10.1177/0049124118769114},
	pages = {0049124118769114},
}

@mastersthesis{nahoomi_automatically_2018,
	address = {Guelph},
	title = {Automatically {Coding} {Occupation} {Titles} to a {Standard} {Occupation} {Classification}},
	url = {http://hdl.handle.net/10214/14251},
	school = {University of Guelph},
	author = {Nahoomi, Negin},
	month = sep,
	year = {2018},
}

@article{murphy_verification_1967,
	title = {Verification of {Probabilistic} {Predictions}: {A} {Brief} {Review}},
	volume = {6},
	url = {https://doi.org/10.1175/1520-0450(1967)006<0748:VOPPAB>2.0.CO;2},
	doi = {10.1175/1520-0450(1967)006<0748:VOPPAB>2.0.CO;2},
	abstract = {Abstract The evaluation process is considered in some detail with particular reference to probabilistic predictions. The process consists of several ordered steps at each of which elements (of the process) are identified. Consideration of the purposes leads to the identification of two distinct forms of evaluation: operational evaluation concerned with the value of predictions to the user and empirical evaluation, or verification, concerned with the perfection of predictions, i.e., the association between predictions and observations. Attributes, i.e., desirable properties, of predictions are defined with reference to these purposes, and a number of measures of the attributes for empirical evaluation are considered. An artificial example of comparative verification in which different measures appear to yield contradictory results is used to demonstrate the importance of, and need for, a careful analysis of the evaluation process.},
	number = {5},
	journal = {Journal of Applied Meteorology},
	author = {Murphy, Allan H. and Epstein, Edward S.},
	year = {1967},
	note = {\_eprint: https://doi.org/10.1175/1520-0450(1967)006{\textless}0748:VOPPAB{\textgreater}2.0.CO;2},
	pages = {748--755},
}

@incollection{munz_string_2016,
	address = {Wiesbaden},
	title = {String {Coding} in a {Generic} {Framework}},
	isbn = {978-3-658-11994-2},
	url = {https://doi.org/10.1007/978-3-658-11994-2\_39},
	abstract = {For many questions in the social sciences that are supposed be answered with survey data, reliable and detailed information about occupations is crucial. As classifications for occupations are very extensive and complex, it is not feasible to simply present a full scheme to the respondent. To overcome this issue, an open string is queried from the respondent and later converted to an appropriate entry in a chosen classification. This task can be handled using a generic coding framework, which is illustrated in this article. The raw material with the strings-to-code (reported occupations) and covariate information has to be prepared and delivered to the process itself. The selected coding scheme has to meet several requirements, such as discriminatory power, completeness, and adequacy. The NEPS's coding framework can be adapted to a larger set of variables: The interface for exporting content-to-code from the NEPS dataset files is used beyond the coding of occupational information. Every NEPS survey developer who is urged to classify his or her string variable(s) is provided with spreadsheets ready for the related workflow. When finished, the NEPS Data Center re-imports these spreadsheets into the dataset. Several further mechanisms have been integrated into this process to ensure high data quality.},
	booktitle = {Methodological {Issues} of {Longitudinal} {Surveys}: {The} {Example} of the {National} {Educational} {Panel} {Study}},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Munz, Manuel and Wenzig, Knut and Bela, Daniel},
	editor = {Blossfeld, Hans-Peter and von Maurice, Jutta and Bayer, Michael and Skopek, Jan},
	year = {2016},
	doi = {10.1007/978-3-658-11994-2_39},
	pages = {709--726},
}

@inproceedings{measure_automated_2014,
	title = {Automated {Coding} of {Worker} {Injury} {Narratives}},
	booktitle = {Proceedings of the {Government} {Statistics} {Section}: {American} {Statistical} {Association}},
	author = {Measure, Alexander},
	year = {2014},
	pages = {2124--2133},
}

@article{mauz_new_2017,
	title = {New data for action. {Data} collection for {KiGGS} {Wave} 2 has been completed},
	volume = {2},
	issn = {2511-2708},
	doi = {10.17886/RKI-GBE-2017-105},
	number = {S3},
	journal = {Journal of Health Monitoring},
	author = {Mauz, Elvira and Gößwald, Antje and Kamtsiuris, Panagiotis and Hoffmann, Robert and Lange, Michael and Schenck, Ursula von and Allen, Jennifer and Butschalowsky, Hans and Frank, Laura and Hölling, Heike and Houben, Robin and Krause, Laura and Kuhnert, Ronny and Lange, Cornelia and Stephan, Müters and Neuhauser, Hannelore and Christina, Poethko-Müller and Richter, Almut and Schaffrath Rosario, Angelika and Schaarschmidt, Jörg and Schlack, Robin and Schlaud, Martin and Schmich, Patrick and Gina, Schöne and Wetzstein, Matthias and Ziese, Thomas and Kurth, Bärbel-Maria},
	year = {2017},
	pages = {2--27},
}

@article{mannetje_use_2003,
	title = {The {Use} of {Occupation} and {Industry} {Classifications} in {General} {Population} {Studies}},
	volume = {32},
	doi = {10.1093/ije/dyg080},
	abstract = {Occupation and industry classifications are used in epidemiological studies to classify study subjects according to their job and subsequently to study risk by job, to infer social class indicators, or to infer exposure to specific agents through job-exposure matrices. However, documentation on methodological aspects concerning the use of occupation and industry classifications is sparse within epidemiology. This paper reviews the diverse applications of occupation and industry classifications in population-based epidemiological studies. The different classifications in use are discussed, and criteria are given for choosing a classification in an epidemiological study. Finally, the reliability of coding for occupation and industry is reviewed. A further standardization of the use of occupation and industry classifications in epidemiology is recommended, in order to facilitate future comparisons between studies and fully exploit their possibilities, especially when occupational exposures are to be inferred.},
	number = {3},
	journal = {International Journal of Epidemiology},
	author = {Mannetje, Andrea 't and Kromhout, Hans},
	year = {2003},
	note = {\_eprint: http://ije.oxfordjournals.org/content/32/3/419.full.pdf+html},
	pages = {419--428},
}

@inproceedings{lyberg_automated_1983,
	title = {Automated {Coding} at {Statistics} {Sweden}},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	author = {Lyberg, Lars and Andersson, Ronnie},
	year = {1983},
	pages = {41--50},
}

@incollection{li_learning_2017,
	address = {Boston},
	title = {Learning to {Rank}},
	isbn = {978-1-4899-7687-1},
	url = {https://doi.org/10.1007/978-1-4899-7687-1\_893},
	abstract = {Many tasks in information retrieval, natural language processing, and data mining are essentially ranking problems. These include document retrieval, expert search, question answering, collaborative filtering, and keyphrase extraction. Learning to rank is a subarea of machine learning, studying methodologies and theories for automatically constructing a model from data for a ranking problem (Liu T-Y, Found Trends Inf Retr 3(3):225–331, 2009; Li H, Synth Lect Hum Lang Technol 4(1):1–113, 2011a; Li H, IEICE Trans Inf Syst 94-D(10):1854–1862, 2011b). Learning to rank is usually formalized as a supervised learning task, while unsupervised learning and semi-supervised learning formulations are also possible. In learning, training data consisting of sets of objects as well as the total or partial orders of the objects in each set is given, and a ranking model is learned using the data. In prediction, a new set of objects is given, and a ranking list of the objects is created using the ranking model. Learning to rank has been intensively studied in the past decade and many methods of learning to rank have been proposed. Popular methods include Ranking SVM, IR SVM, AdaRank, LambdaRank, and LambdaMART. The methods can be categorized into the pointwise, pairwise, and listwise approaches according to the loss functions which they use. It is known that learning-to-rank methods, such as LambdaMART, are being employed in a number of commercial web search engines. In this entry, we describe the formulation as well as several methods of learning to rank. Without loss of generality, we take document retrieval as example.},
	booktitle = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	publisher = {Springer},
	author = {Li, Hang},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2017},
	doi = {10.1007/978-1-4899-7687-1_893},
	pages = {729--734},
}

@article{lange_kiggs_2018,
	title = {{KiGGS} {Wave} 2 longitudinal component – data collection design and developments in the numbers of participants in the {KiGGS} cohort},
	volume = {3},
	issn = {2511-2708},
	doi = {10.17886/RKI-GBE-2018-035},
	number = {1},
	journal = {Journal of Health Monitoring},
	author = {Lange, Michael and Hoffmann, Robert and Mauz, Elvira and Houben, Robin and Gößwald, Antje and Schaffrath Rosario, Angelika and Kurth, Bärbel-Maria},
	year = {2018},
	pages = {92--107},
}

@article{lange_implementation_2017,
	title = {Implementation of the {European} health interview survey ({EHIS}) into the {German} health update ({GEDA})},
	volume = {75},
	issn = {2049-3258},
	url = {https://doi.org/10.1186/s13690-017-0208-6},
	doi = {10.1186/s13690-017-0208-6},
	abstract = {This methodological paper describes the integration of the `European Health Interview Survey wave 2' (EHIS 2) into the `German Health Update' 2014/2015 (GEDA 2014/2015-EHIS).},
	number = {1},
	journal = {Archives of Public Health},
	author = {Lange, C. and Finger, J.D. and Allen, J. and Born, S. and Hoebel, J. and Kuhnert, R. and Müters, S. and Thelen, J. and Schmich, P. and Varga, M. and von der Lippe, E. and Wetzstein, M. and Ziese, T.},
	month = sep,
	year = {2017},
	pages = {40},
}

@article{knaus_methods_1987,
	title = {Methods and {Problems} in {Coding} {Natural} {Language} {Survey} {Data}},
	volume = {3},
	number = {1},
	journal = {Journal of Official Statistics},
	author = {Knaus, Rodger},
	year = {1987},
	pages = {45--67},
}

@incollection{klingemann_computerunterstutzte_1984,
	address = {Frankfurt/Main},
	title = {Computerunterstützte {Inhaltsanalyse} als {Instrument} zur {Vercodung} offener {Fragen} in der {Umfrageforschung}},
	isbn = {3-593-33254-X},
	booktitle = {Computerunterstüzte {Inhaltsanalyse} in der empirischen {Sozialforschung}},
	publisher = {Campus Verlag},
	author = {Klingemann, Hans-Dieter and Schönbach, Klaus},
	editor = {Klingemann, Hans-Dieter},
	year = {1984},
	pages = {227--278},
}

@inproceedings{javed_carotene_2015,
	address = {Redwood City},
	title = {Carotene: {A} {Job} {Title} {Classification} {System} for the {Online} {Recruitment} {Domain}},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Big} {Data}},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	author = {Javed, Faizan and Luo, Qinlong and McNair, Matt and Jacob, Ferosh and Zhao, Meng and Kang, Tae Seung},
	month = aug,
	year = {2015},
	pages = {pp. 286--293},
}

@techreport{ikudo_occupational_2018,
	address = {Cambridge, MA},
	type = {Working {Paper} 24951},
	title = {Occupational {Classifications}: {A} {Machine} {Learning} {Approach}},
	institution = {National Bureau of Economic Research},
	author = {Ikudo, Akina and Lane, Julia and Staudt, Joseph and Weinberg, Bruce},
	month = aug,
	year = {2018},
}

@article{hoffmann_kiggs_2018,
	title = {{KiGGS} {Wave} 2 cross-sectional study – participant acquisition, response rates and representativeness},
	volume = {3},
	issn = {2511-2708},
	doi = {10.17886/RKI-GBE-2018-032},
	number = {1},
	journal = {Journal of Health Monitoring},
	author = {Hoffmann, Robert and Lange, Michael and Butschalowsky, Hans and Houben, Robin and Schmich, Patrick and Allen, Jennifer and Kuhnert, Ronny and Schaffrath Rosario, Angelika and Gößwald, Antje},
	year = {2018},
	pages = {78--91},
}

@article{hill_multilevel_1998,
	title = {Multilevel {Modeling} of {Educational} {Data} {With} {Cross}-{Classification} and {Missing} {Identification} for {Units}},
	volume = {23},
	number = {2},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hill, Peter W. and Goldstein, Harvey},
	year = {1998},
	pages = {117--128},
}

@book{hastie_statistical_2015,
	address = {Boca Raton},
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}},
	isbn = {978-1-4987-1216-3},
	publisher = {Chapman and Hall/CRC Press},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
}

@book{hastie_elements_2009,
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84858-7},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
}

@techreport{hartmann_vercodung_2012,
	address = {Munich},
	title = {Die {Vercodung} der offenen {Angaben} zur beruflichen {Tätigkeit} nach der {Klassifikation} der {Berufe} 2010 ({KldB} 2010) und nach der {International} {Standard} {Classification} of {Occupations} 2008 ({ISCO08})},
	url = {https://metadaten.bibb.de/download/684},
	institution = {TNS Infratest Sozialforschung},
	author = {Hartmann, Josef and Tschersich, Nikolai and Schütz, Gerd},
	month = dec,
	year = {2012},
}

@techreport{hall_bibbbaua_2015,
	address = {Bonn},
	type = {Research {Data} {Center} at {BIBB} (ed., data access)},
	title = {{BIBB}/{BAuA} {Employment} {Survey} of the {Working} {Population} on {Qualification} and {Working} {Conditions} in {Germany} 2012. vt\_1.0, sv\_2.0},
	url = {https://doi.org/10.7803/501.12.1.4.10},
	institution = {Federal Institute for Vocational Education and Training},
	author = {Hall, Anja and Siefer, Anke and Tiemann, Michael},
	year = {2015},
}

@article{gweon_three_2017,
	title = {Three {Methods} for {Occupation} {Coding} {Based} on {Statistical} {Learning}},
	volume = {33},
	number = {1},
	journal = {Journal of Official Statistics},
	author = {Gweon, Hyukjun and Schonlau, Matthias and Kaczmirek, Lars and Blohm, Michael and Steiner, Stefan},
	year = {2017},
	pages = {101--122},
}

@book{goldstein_multilevel_2011,
	address = {Chichester},
	title = {Multilevel {Statistical} {Models}, {Fourth} {Edition}},
	isbn = {978-0-470-74865-7},
	publisher = {Wiley},
	author = {Goldstein, Harvey},
	year = {2011},
}

@techreport{gillman_automated_1994,
	address = {Suitland},
	type = {Statistical {Research} {Report} {Series} 94/04},
	title = {Automated {Coding} {Research} at the {Census} {Bureau}},
	url = {https://www.census.gov/srd/papers/pdf/rr94-4.pdf},
	institution = {United States Census Bureau},
	author = {Gillman, Daniel W. and Appel, Martin V.},
	year = {1994},
}

@article{geis_stand_2000,
	title = {Stand der {Berufsvercodung}},
	volume = {24},
	number = {47},
	journal = {ZUMA-Nachrichten},
	author = {Geis, Alfons and Hoffmeyer-Zlotnik, Jürgen H.P.},
	month = nov,
	year = {2000},
	pages = {103--128},
}

@article{friedman_greedy_2001,
	title = {Greedy function approximation: {A} gradient boosting machine},
	volume = {29},
	url = {https://doi.org/10.1214/aos/1013203451},
	doi = {10.1214/aos/1013203451},
	number = {5},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome},
	month = oct,
	year = {2001},
	note = {Publisher: The Institute of Mathematical Statistics},
	pages = {1189--1232},
}

@techreport{federal_employment_agency_gesamtberufsliste_2019,
	address = {Nuremberg},
	type = {Gesamtberufsliste\_der\_BA.xlsx},
	title = {Gesamtberufsliste der {Bundesagentur} für {Arbeit} ({Stand}: 03.01.2019)},
	institution = {Bundesagentur für Arbeit},
	author = {{Federal Employment Agency}},
	year = {2019},
}

@book{federal_employment_agency_klassifikation_2011,
	address = {Nuremberg},
	title = {Klassifikation der {Berufe} 2010},
	publisher = {Bundesagentur für Arbeit},
	author = {{Federal Employment Agency}},
	month = mar,
	year = {2011},
}

@techreport{elias_cascot_2014,
	address = {University of Warwick, Coventry},
	type = {User {Guide}},
	title = {{CASCOT} {International} version 5},
	url = {http://www2.warwick.ac.uk/fac/soc/ier/software/cascot/internat/},
	urldate = {2017-09-18},
	institution = {Institute for Employment Research},
	author = {Elias, Peter and Birch, Margaret and Ellison, Ritva},
	year = {2014},
}

@techreport{elias_occupational_1997,
	type = {{OECD} {Labour} {Market} and {Social} {Policy} {Occasional} {Papers}},
	title = {Occupational {Classification} ({ISCO}-88): {Concepts}, {Methods}, {Reliability}, {Validity} and {Cross}-{National} {Comparability}},
	shorttitle = {Occupational {Classification} ({ISCO}-88)},
	url = {https:///doi.org/10.1787/304441717388},
	language = {en},
	number = {20},
	urldate = {2021-10-11},
	author = {Elias, Peter},
	month = jan,
	year = {1997},
	doi = {10.1787/304441717388},
	note = {Series: OECD Labour Market and Social Policy Occasional Papers
Volume: 20},
}

@article{dawid_statistical_1984,
	title = {Statistical {Theory}: {The} {Prequential} {Approach} (with discussion)},
	volume = {147},
	issn = {00359238},
	url = {http://www.jstor.org/stable/2981683},
	abstract = {The prequential approach is founded on the premises that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Dawid, Alexander Philip},
	year = {1984},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {278--292},
}

@article{conrad_classifying_2016,
	title = {Classifying {Open}-{Ended} {Reports}: {Factors} {Affecting} the {Reliability} of {Occupation} {Codes}},
	volume = {32},
	number = {1},
	journal = {Journal of Official Statistics},
	author = {Conrad, Frederick G. and Couper, Mick P. and Sakshaug, Joseph W.},
	year = {2016},
	pages = {75--92},
}

@article{chen_error_1993,
	title = {Error {Control} of {Automated} {Industry} and {Occupation} {Coding}},
	volume = {9},
	issn = {0282-423X},
	number = {4},
	journal = {Journal of Official Statistics},
	author = {Chen, Bor-Chung and Creecy, Robert H. and Appel, Martin V.},
	year = {1993},
	pages = {729--745},
}

@article{browne_multiple_2001,
	title = {Multiple {Membership} {Multiple} {Classification} ({MMMC}) {Models}},
	volume = {1},
	number = {2},
	journal = {Statistical Modeling},
	author = {Browne, William J. and Goldstein, Harvey and Rasbash, Jon},
	year = {2001},
	pages = {103--124},
}

@article{brocker_increasing_2007,
	title = {Increasing the {Reliability} of {Reliability} {Diagrams}},
	volume = {22},
	url = {https://doi.org/10.1175/WAF993.1},
	doi = {10.1175/WAF993.1},
	abstract = {Abstract The reliability diagram is a common diagnostic graph used to summarize and evaluate probabilistic forecasts. Its strengths lie in the ease with which it is produced and the transparency of its definition. While visually appealing, major long-noted shortcomings lie in the difficulty of interpreting the graph visually; for the most part, ambiguities arise from variations in the distributions of forecast probabilities and from various binning procedures. A resampling method for assigning consistency bars to the observed frequencies is introduced that allows for immediate visual evaluation as to just how likely the observed relative frequencies are under the assumption that the predicted probabilities are reliable. Further, an alternative presentation of the same information on probability paper eases quantitative evaluation and comparison. Both presentations can easily be employed for any method of binning.},
	number = {3},
	journal = {Weather and Forecasting},
	author = {Bröcker, Jochen and Smith, Leonard A.},
	year = {2007},
	note = {\_eprint: https://doi.org/10.1175/WAF993.1},
	pages = {651--661},
}

@inproceedings{bethmann_automatic_2014,
	title = {Automatic {Coding} of {Occupations}},
	url = {https://www.statcan.gc.ca/eng/conferences/symposium2014/program/14291-eng.pdf},
	booktitle = {Proceedings of {Statistics} {Canada} {Symposium} 2014},
	publisher = {Statistics Canada},
	author = {Bethmann, Arne and Schierholz, Malte and Wenzig, Knut and Zielonka, Markus},
	year = {2014},
	note = {event-place: Gatineau, Quebec, Canada},
}

@techreport{berg_codebuch_2017,
	address = {Nuremberg},
	type = {{FDZ}-{Datenreport} 07/2017},
	title = {Codebuch und {Dokumentation} des {Panel} '{Arbeitsmarkt} und soziale {Sicherung}' ({PASS}) * {Band} {I}: {Datenreport} {Welle} 10},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Berg, Marco and Cramer, Ralph and Dickmann, Christian and Gilberg, Reiner and Jesske, Birgit and Kleudgen, Martin and Beste, Jonas and Dummert, Sandra and Frodermann, Corinna and Fuchs, Benjamin and Schwarz, Stefan and Trappmann, Mark and Trenkle, Simon},
	year = {2017},
}

@techreport{antoni_arbeiten_2010,
	address = {Nuremberg},
	type = {{FDZ}-{Methodenreport} 05/2010},
	title = {Arbeiten und {Lernen} im {Wandel} * {Teil} 1: Überblick über die {Studie}},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {Antoni, Manfred and Drasch, Katrin and Kleinert, Corinna and Matthes, Britta and Ruland, Michael and Trahms, Annette},
	month = may,
	year = {2010},
}

@unpublished{albrecht_occupation_2017,
	title = {Occupation {Coding} in the {German} {Health} {Update} ({GEDA}-{Study} 2014/15)},
	url = {\url{https://www.europeansurveyresearch.org/conference/programme2017?sess=4\#630}},
	author = {Albrecht, Stefan and Schmich, Patrick and Varga, Maike},
	year = {2017},
}

@book{bouchet-valat_snowballc_2018,
	title = {{SnowballC}: {Snowball} {Stemmers} {Based} on the {C} 'libstemmer' {UTF}-8 {Library}},
	url = {https://CRAN.R-project.org/package=SnowballC},
	author = {Bouchet-Valat, Milan},
	year = {2018},
}

@book{r_core_team_r_2016,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2016},
}

@book{feinerer_tm_2018,
	title = {tm: {Text} {Mining} {Package}},
	url = {https://CRAN.R-project.org/package=tm},
	author = {Feinerer, Ingo and Hornik, Kurt},
	year = {2018},
}

@book{r_core_team_r_2018,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2018},
}

@techreport{antoni_working_2010,
	address = {Nuremberg},
	type = {{FDZ}-{Methodenreport} 05/2010 (en)},
	title = {Working and {Learning} in a {Changing} {World} * {Part} 1: {Overview} of the {Study}},
	institution = {Research Data Centre (FDZ) at the Institute for Employment Research (IAB)},
	author = {Antoni, Manfred and Drasch, Katrin and Kleinert, Corinna and Matthes, Britta and Ruland, Michael and Trahms, Annette},
	month = may,
	year = {2010},
}

@techreport{geis_handbuch_2011,
	address = {Mannheim},
	type = {Coding {Documentation}},
	title = {Handbuch für die {Berufsvercodung}},
	url = {https://www.gesis.org/fileadmin/upload/dienstleistung/tools_standards/handbuch_der_berufscodierung_110304.pdf},
	institution = {GESIS – Leibniz-Institut für Sozialwissenschaften},
	author = {Geis, Alfons},
	month = mar,
	year = {2011},
}

@misc{dowle_datatable_2012,
	title = {data.table: {Extension} of data.frame for {Fast} {Indexing}, {Fast} {Ordered} {Joins}, {Fast} {Assignment}, {Fast} {Grouping} and {List} {Columns}},
	url = {https://cran.r-project.org/package=data.table},
	author = {Dowle, Matt and Short, T and Lianoglou, S},
	year = {2012},
}

@book{tourangeau_psychology_2000,
	address = {Cambridge},
	title = {The {Psychology} of {Survey} {Response}},
	isbn = {978-0-521-57246-0},
	publisher = {Cambridge University Press},
	author = {Tourangeau, Roger and Rips, Lance J. and Rasinski, Kenneth},
	year = {2000},
}

@techreport{willms_historische_1983,
	address = {Mannheim},
	type = {{VASMA}-{Projekt}, {Arbeitspapier} {Nr}. 30},
	title = {Historische {Berufsforschung} mit amtlicher {Statistik}. {Rekonstruktion} der {Entwicklung} der {Berufsstatistik} in {Deutschland} und {Entwurf} einer {Klassifikation} vergleichbarer {Berufsfelder}, 1925–1980},
	author = {Willms, Angelika},
	year = {1983},
}

@book{weber_wirtschaft_1980,
	address = {Tübingen},
	title = {Wirtschaft und {Gesellschaft}. {Grundriß} der verstehenden {Soziologie}},
	volume = {5},
	publisher = {Mohr},
	author = {Weber, Max},
	year = {1980},
	note = {Studienausgabe, 5. rev. Auflage besorgt von Johannes Winckelmann},
}

@techreport{tijdens_measuring_2010,
	address = {Amterdam},
	title = {Measuring occupations in web-surveys. {The} {WISCO} database of occupations},
	institution = {Amsterdam Institute for Advanced labour Studies},
	author = {Tijdens, Kea},
	editor = {University of Amsterdam},
	year = {2010},
	note = {Type: Studies. Working Paper No. 10/86},
}

@incollection{stoos_systematik_1979,
	address = {Königstein im Taunus},
	title = {Systematik der {Berufe} und der beruflichen {Tätigkeiten}},
	booktitle = {Sozialstrukturanalysen mit {Umfragedaten}},
	publisher = {Athenäum-Verlag},
	author = {Stooß, Friedemann and Saterdag, Hermann},
	editor = {Pappi, Franz Urban (Hg},
	year = {1979},
	pages = {41--57},
}

@incollection{stoos_systematik_1977,
	address = {Göttingen},
	title = {Die {Systematik} der {Berufe} und der beruflichen {Tätigkeiten}},
	booktitle = {Handbuch der {Berufspsychologie}},
	publisher = {Verlag für Psychologie Hogrefe},
	author = {Stooß, Friedemann and Seifert, Karl Heinz},
	year = {1977},
	pages = {69--98},
}

@article{sperling_zur_1961,
	title = {Zur {Theorie} und {Methode} der {Berufsklassifizierung}},
	volume = {81},
	journal = {Schmollers Jahrbuch für Gesetzgebung, Verwaltung und Volkswirtschaft},
	author = {Sperling, Hans},
	year = {1961},
	pages = {705--720},
}

@book{sutherland_wealth_1998,
	address = {New York},
	title = {Wealth of {Nations} – {A} {Selected} {Edition}},
	publisher = {Oxford University Press},
	author = {Smith, Adam},
	editor = {Sutherland, Kathryn},
	year = {1998},
}

@incollection{schonbach_probleme_1979,
	address = {Königstein im Taunus},
	title = {Probleme der {Verschlüsselung} von {Berufstätigkeiten}},
	booktitle = {Sozialstrukturanalysen mit {Umfragedaten}},
	publisher = {Athenäum-Verlag},
	author = {Schönbach, Klaus},
	editor = {Pappi, Franz Urban (Hg},
	year = {1979},
	pages = {41--57},
}

@article{prigge_codierung_2014,
	title = {Codierung der {Tätigkeitsangaben} im {Basiskollektiv} der {Gutenberg}-{Gesundheitsstudie} unter {Anwendung} der {Klassifikation} der {Berufe} {KldB} 2010. {Darstellung} des {Vorgehens} und der {Datenqualität}},
	volume = {68},
	number = {3},
	journal = {Zeitschrift für Arbeitswissenschaft},
	author = {Prigge, Michaela and Köhr, Martha and Pfeiffer, Norbert and Blettner, Maria and Beutel, Manfred and Wild, Philipp and Münzel, Thomas and Blankenberg, Stefan and Seidler, Andreas and Letzel, Stephan and Latza, Ute and Liebers, Falk},
	year = {2014},
	pages = {153--162},
}

@incollection{pollak_comparative_2010,
	title = {The comparative measurement of supervisory status},
	booktitle = {Social {Class} in {Europe}. {An} introduction to the {European} {Socio}-economic {Classification}},
	publisher = {Routledge},
	author = {Pollak, Reinhard and Bauer, Gerrit and Müller, Walter and Weiss, Felix and Wirth, Heike},
	collaborator = {Rose, David and Harrison, Eric},
	year = {2010},
}

@techreport{muller_implementation_2014,
	address = {Nürnberg},
	title = {The implementation of the {German} {Classification} of {Occupations} 2010 in the {IAB} {Job} {Vacancy} {Survey}},
	institution = {Institut für Arbeitsmarkt und Berufsforschung},
	author = {Müller, Anne},
	year = {2014},
}

@book{meier_handbuch_2003,
	title = {Handbuch zur {Berufsdatenbank}},
	url = {https://www.bfs.admin.ch/bfsstatic/dam/assets/337799/master},
	urldate = {2017-05-29},
	author = {Meier, Urs},
	editor = {Bundesamt für Statistik},
	year = {2003},
}

@book{meerwarth_nationalokonomie_1925,
	address = {Berlin},
	series = {Handbuch der {Wirtschaftsund} {Sozialwissenschaften} in {Einzelbänden}},
	title = {Nationalökonomie und {Statistik}. {Eine} {Einführung} in die empirische {Nationalökonomie}.},
	number = {7},
	publisher = {Walter de Gruyter},
	author = {Meerwarth, Rudolf},
	year = {1925},
}

@article{international_labour_office_systems_1923,
	title = {Systems of {Classification} of {Industries} and {Occupations}},
	volume = {1},
	journal = {Studies and Reports, Series N},
	author = {International Labour Office},
	year = {1923},
	note = {Publisher: International Labour Office},
}

@book{international_labour_office_international_1958,
	address = {Geneva},
	title = {International {Standard} {Classification} of {Occupations}},
	author = {International Labour Office},
	year = {1958},
}

@article{furst_zur_1929,
	title = {Zur {Methode} der deutschen {Berufsstatistik}},
	volume = {19},
	journal = {Allgemeines Statistisches Archiv},
	author = {Fürst, Gerhard},
	year = {1929},
	pages = {1--29},
}

@techreport{embury_constructing_1997,
	address = {Geneva},
	type = {Working {Paper} {No}. 95/2},
	title = {Constructing a map of the world of work. {How} to develop the structure and contents of a national standard classification of occupations},
	institution = {Bureau of Statistics, International Labour Office},
	author = {Embury, Brian},
	year = {1997},
}

@article{dostal_beruf_1998,
	title = {Beruf – {Auflösungstendenzen} und erneute {Konsolidierun}},
	volume = {31},
	number = {3},
	journal = {Mitteilungen aus der Arbeitsmarkt und Berufsforschung},
	author = {Dostal, Werner and Stooß, Friedemann and Troll, Lothar},
	year = {1998},
	pages = {438--460},
}

@incollection{demszky_von_der_hagen_beruf_2010,
	address = {Wiesbaden},
	title = {Beruf und {Profession}},
	booktitle = {Handbuch {Arbeitssoziologie}},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Demszky von der Hagen, Alma and Voß, G. Günter},
	editor = {Böhle, Fritz and Voß, G. Günter and Wachtler, Günther},
	year = {2010},
	pages = {751--803},
}

@article{damelang_institutionelle_2015,
	title = {Institutionelle {Eigenschaften} von {Berufen} und ihr {Einfluss} auf berufliche {Mobilität} in {Deutschland}},
	volume = {135},
	number = {3},
	journal = {Schmollers Jahrbuch},
	author = {Damelang, Andreas and Schulz, Florian and Vicari, Basha},
	year = {2015},
	pages = {307--333},
}

@book{statistisches_bundesamt_demographische_2016,
	address = {Wiesbaden},
	title = {Demographische {Standards}. {Ausgabe} 2016},
	volume = {Band 17 der Reihe Statistik und Wissenschaft},
	author = {Statistisches Bundesamt},
	year = {2016},
}

@book{statistisches_bundesamt_klassifizierung_1961,
	address = {Stuttgart},
	title = {Klassifizierung der {Berufe}},
	publisher = {Kohlhammer},
	author = {Statistisches Bundesamt},
	year = {1961},
}

@book{bundesagentur_fur_arbeit_klassifikation_2011,
	address = {Nuremberg},
	title = {Klassifikation der {Berufe} 2010},
	publisher = {Bundesagentur für Arbeit},
	author = {{Bundesagentur für Arbeit}},
	month = mar,
	year = {2011},
}

@misc{bundesagentur_fur_arbeit_tatigkeitsschlussel-online_2017,
	title = {Tätigkeitsschlüssel-{Online}},
	url = {http://bns-ts.arbeitsagentur.de/},
	urldate = {2017-09-18},
	journal = {Tätigkeitsschlüssel-Online},
	author = {{Bundesagentur für Arbeit}},
	year = {2017},
}

@book{watson_sociology_2012,
	address = {London},
	edition = {6th edition},
	title = {Sociology, work and organization},
	publisher = {Routledge},
	author = {Watson, Tony James},
	year = {2012},
}

@article{schierholz_hilfsklassifikation_2018,
	title = {Eine {Hilfsklassifikation} mit {Tätigkeitsbeschreibungen} für {Zwecke} der {Berufskodierung}},
	volume = {12},
	copyright = {All rights reserved},
	issn = {1863-8155, 1863-8163},
	url = {http://link.springer.com/10.1007/s11943-018-0231-2},
	doi = {10.1007/s11943-018-0231-2},
	abstract = {Occupational classiﬁcations are structured by the type of work that employees perform. Consequently, German surveys ask employees about the work they perform in order to collect information about occupation. Although the question is tailored to this classiﬁcation principle, category deﬁnitions describing the work are infrequently used for coding. Instead, coding is more indirect as coders often select job titles from a separate coding index. Since many job titles are imprecise and do not sufﬁciently describe the work actually performed, incorrect assignments may occur.},
	language = {de},
	number = {3-4},
	urldate = {2021-10-11},
	journal = {AStA Wirtschafts- und Sozialstatistisches Archiv},
	author = {Schierholz, Malte},
	month = dec,
	year = {2018},
	pages = {285--298},
}

@incollection{krosnick_question_2010,
	address = {Bingley},
	title = {Question and {Questionnaire} {Design}},
	isbn = {978-1-84855-224-1},
	booktitle = {Handbook of {Survey} {Research}},
	publisher = {Emerald Group},
	author = {Krosnick, Jon and Presser, Stanley},
	editor = {Marsden, Peter V. and Wright, James D.},
	year = {2010},
	pages = {pp. 263--313},
}

@book{ripley_rodbc_2013,
	title = {{RODBC}: {ODBC} {Database} {Access}},
	url = {https://CRAN.R-project.org/package=RODBC},
	author = {Ripley, Brian and Lapsley, Michael},
	year = {2013},
}

@book{wickham_stringr_2015,
	title = {stringr: {Simple}, {Consistent} {Wrappers} for {Common} {String} {Operations}},
	url = {https://CRAN.R-project.org/package=stringr},
	author = {Wickham, Hadley},
	year = {2015},
}

@article{feinerer_text_2008,
	title = {Text mining infrastructure in {R}},
	volume = {25},
	issn = {1548-7660},
	doi = {10.18637/jss.v025.i05},
	abstract = {During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count-based analysis methods, text clustering, text classification and string kernels.},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Feinerer, Ingo and Hornik, Kurt and Meyer, David},
	year = {2008},
	pages = {1--54},
}

@book{r_core_team_foreign_2014,
	title = {foreign: {Read} {Data} {Stored} by {Minitab}, {S}, {SAS}, {SPSS}, {Stata}, {Systat}, {Weka}, {dBase}, ...},
	url = {https://CRAN.R-project.org/package=foreign},
	author = {{R Core Team}},
	year = {2014},
}

@book{urbanek_simon_rserve_2013,
	title = {Rserve: {Binary} {R} server},
	url = {http://CRAN.R-project.org/package=Rserve},
	publisher = {Urbanek, Simon},
	author = {{Urbanek, Simon}},
	year = {2013},
}

@book{nipo_software_nipo_2014,
	address = {Amsterdam},
	title = {{NIPO} {Fieldwork} {System}},
	publisher = {NIPO Software},
	author = {{NIPO Software}},
	year = {2014},
}

@article{leblanc_combining_1996,
	title = {Combining {Estimates} in {Regression} and {Classification}},
	volume = {91},
	issn = {01621459},
	abstract = {We consider the problem of how to combine a collection of general regression fit vectors to obtain a better predictive model. The individual fits may be from subset linear regression, ridge regression, or something more complex like a neural network. We develop a general framework for this problem and examine a cross-validation-based proposal called "model mix" or "stacking" in this context. We also derive combination methods based on the bootstrap and analytic methods and compare them in examples. Finally, we apply these ideas to classification problems where the estimated combination weights can yield insight into the structure of the problem.},
	number = {436},
	journal = {J. Am. Statist. Ass.},
	author = {LeBlanc, Michael and Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1641--1650},
}

@book{oracle_corporation_mysql_2014,
	address = {Redwood City},
	title = {{MySQL}},
	publisher = {Oracle Corporation},
	author = {{Oracle Corporation}},
	year = {2014},
}

@book{r_core_team_r_2012,
	address = {Vienna},
	title = {R: {A} language and environment for statistical computing},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2012},
}

@incollection{schaeffer_interviewers_2010,
	address = {Bingley},
	title = {Interviewers and {Interviewing}},
	isbn = {978-1-84855-224-1},
	booktitle = {Handbook of {Survey} {Research}},
	publisher = {Emerald Group},
	author = {Schaeffer, Nora Cate and Dykema, Jennifer and Maynard, Douglas W.},
	editor = {Marsden, Peter V. and Wright, James D.},
	year = {2010},
	pages = {pp. 437--470},
}

@article{hoffmeyer-zlotnik_computerunterstutzte_2006,
	title = {Computerunterstützte {Vercodung} der {International} {Standard} {Classification} of {Occupations} ({ISCO}-88): {Vorstellen} eines {Instruments}},
	volume = {30},
	number = {58},
	journal = {ZUMA-Nachrichten},
	author = {Hoffmeyer-Zlotnik, Jürgen H.P. and Hess, Doris and Geis, Alfons J.},
	month = may,
	year = {2006},
	pages = {101--113},
}

@techreport{tijdens_reviewing_2014,
	address = {Amsterdam},
	type = {{AIAS} {Working} {Paper} 149},
	title = {Reviewing the measurement and comparison of occupations across {Europe}},
	institution = {Universiteit van Amsterdam},
	author = {Tijdens, Kea},
	year = {2014},
}

@inproceedings{bradburn_respondent_1978,
	title = {Respondent {Burden}},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	author = {Bradburn, Norman M.},
	year = {1978},
	pages = {35--40},
}

@book{schnell_survey-interviews_2012,
	address = {Wiesbaden},
	title = {Survey-{Interviews}. {Standardisierte} {Befragungen} in den {Sozialwissenschaften}},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Schnell, Rainer},
	year = {2012},
}

@inproceedings{bobbitt_coding_1993,
	title = {Coding {Major} {Field} of {Study}},
	booktitle = {Proceedings of the {Survey} {Research} {Methods} {Section}: {American} {Statistical} {Association}},
	author = {Bobbitt, Larry G. and Carroll, C. Dennis},
	year = {1993},
	pages = {177--182},
}

@inproceedings{svensson_quality_2012,
	title = {Quality control of coding of survey responses at {Statistics} {Sweden}},
	booktitle = {Proceedings of the {European} {Conference} on {Quality} in {Official} {Statistics} - {Q2012}},
	author = {Svensson, Jörgen},
	year = {2012},
}

@inproceedings{hacking_computer_2006,
	title = {Computer {Assisted} {Coding} by {Interviewers}},
	booktitle = {Proceedings of the {IBUC} 2006 10th {International} {Blaise} {Users} {Conference}},
	author = {Hacking, Wim and Michiels, John and Janssen-Jansen, Saskia},
	year = {2006},
	pages = {283--296},
}

@article{biemer_continuous_1994,
	title = {Continuous {Quality} {Improvement} for {Survey} {Operations}: {Some} {General} {Principles} and {Applications}},
	volume = {10},
	number = {3},
	journal = {Journal of Official Statistics},
	author = {Biemer, Paul and Caspar, Rachel},
	year = {1994},
	pages = {307--326},
}

@book{statistisches_bundesamt_demographische_2010,
	address = {Wiesbaden},
	title = {Demographische {Standards}},
	publisher = {Statistisches Bundesamt},
	author = {{Statistisches Bundesamt}},
	month = dec,
	year = {2010},
}

@book{fowler_standardized_1990,
	address = {Newbury Park},
	title = {Standardized {Survey} {Interviewing}: {Minimizing} {Interviewer}-{Related} {Error}},
	isbn = {9780803930925},
	publisher = {SAGE Publications},
	author = {Fowler, Floyd J. and Mangione, Thomas W.},
	year = {1990},
}

@book{hoffmeyer-zlotnik_harmonisierung_2012,
	address = {Wiesbaden},
	title = {Harmonisierung demographischer und sozio-ökonomischer {Variablen}: {Instrumente} für die international vergleichende {Surveyforschung}},
	isbn = {3-531-18374-5},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Hoffmeyer-Zlotnik, Jürgen H.P. and Warner, Uwe},
	year = {2012},
}

@article{belloni_measuring_2016,
	title = {Measuring and {Detecting} {Errors} in {Occupational} {Coding}: an {Analysis} of {SHARE} {Data}},
	volume = {32},
	number = {4},
	journal = {Journal of Official Statistics},
	author = {Belloni, Michele and Brugiavini, Agar and Meschi, Elena and Tijdens, Kea},
	year = {2016},
	pages = {917--945},
}

@techreport{office_for_national_statistics_quality_2003,
	address = {Titchfield},
	type = {Census 2001 {Review} and {Evaluation} {Report}},
	title = {Quality of {Data} {Capture} and {Coding}: {Evaluation} {Report}},
	institution = {Office for National Statistics},
	author = {{Office for National Statistics}},
	month = sep,
	year = {2003},
}

@book{united_nations__international_labour_office_measuring_2010,
	address = {New York},
	title = {Measuring the {Economically} {Active} in {Population} {Censuses}: {A} {Handbook}},
	publisher = {United Nations \& International Labour Office},
	author = {{United Nations \& International Labour Office}},
	year = {2010},
}

@book{bundesagentur_fur_arbeit_klassifikation_2011-1,
	address = {Nuremberg},
	title = {Klassifikation der {Berufe} 2010. {Band} 2: {Definitorischer} und beschreibender {Teil}},
	publisher = {Bundesagentur für Arbeit},
	author = {{Bundesagentur für Arbeit}},
	month = mar,
	year = {2011},
}

@book{bundesagentur_fur_arbeit_klassifikation_2011-2,
	address = {Nuremberg},
	title = {Klassifikation der {Berufe} 2010. {Band} 1: {Systematischer} und alphabetischer {Teil} mit {Erläuterungen}},
	publisher = {Bundesagentur für Arbeit},
	author = {{Bundesagentur für Arbeit}},
	month = mar,
	year = {2011},
}

@techreport{bundesagentur_fur_arbeit_b_swtxt_2013,
	address = {Nuremberg},
	type = {Index of {Search} {Words}},
	title = {B\_SW.txt},
	institution = {Bundesagentur für Arbeit},
	author = {{Bundesagentur für Arbeit}},
	year = {2013},
}

@techreport{hoffmann_what_1995,
	address = {Geneva},
	type = {{STAT} {Working} {Papers} {No}. 95-1},
	title = {What kind of work do you do? {Data} collection and processing strategies when measuring “occupation” for statistical surveys and administrative records},
	institution = {Bureau of Statistics, International Labour Office},
	author = {Hoffmann, Eivind and Elias, Peter and Embury, Brian and Thomas, Roger},
	month = feb,
	year = {1995},
}

@techreport{vom_berge_sample_2013,
	address = {Nuremberg},
	type = {{FDZ}-{Datenreport} 01/2013},
	title = {Sample of integrated labour market biographies ({SIAB}) 1975-2010},
	institution = {Forschungsdatenzentrum der Bundesagentur für Arbeit im Institut für Arbeitsmarkt- und Berufsforschung},
	author = {vom Berge, Philipp and König, Marion and Seth, Stefan},
	year = {2013},
}
